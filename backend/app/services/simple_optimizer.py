"""
Ù…Ø­Ø³Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¨Ø³Ø· - Ù†Ø³Ø®Ø© ØªØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡
Simple Text Optimizer - Working Version
"""

import re
from typing import Dict, Any
from loguru import logger


class SimpleOptimizer:
    """Ù…Ø­Ø³Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¨Ø³Ø· ÙˆØ§Ù„ÙØ¹Ø§Ù„"""
    
    def __init__(self):
        logger.info("âœ… ØªÙ… ØªÙƒÙˆÙŠÙ† Ù…Ø­Ø³Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¨Ø³Ø·")
    
    def optimize_text(self, text: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø© ÙˆÙØ¹Ø§Ù„Ø©"""
        
        if not text or not text.strip():
            return {
                "optimized_text": "",
                "original_length": 0,
                "optimized_length": 0,
                "reduction_percentage": 0
            }
        
        original_length = len(text)
        logger.info(f"ğŸ”§ Ø¨Ø¯Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ: {original_length} Ø­Ø±Ù")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©
        cleaned_text = self._remove_noise_simple(text)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø©
        important_parts = self._extract_important_parts(cleaned_text)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù†
        max_chars = max_tokens * 4  # ØªÙ‚Ø¯ÙŠØ±: 1 ØªÙˆÙƒÙ† = 4 Ø£Ø­Ø±Ù Ø¹Ø±Ø¨ÙŠØ©
        optimized_text = self._build_optimized_simple(important_parts, max_chars)
        
        optimized_length = len(optimized_text)
        reduction_percentage = ((original_length - optimized_length) / original_length * 100) if original_length > 0 else 0
        
        logger.info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ø³ÙŠÙ†: {original_length} -> {optimized_length} Ø­Ø±Ù ({reduction_percentage:.1f}% ØªÙ‚Ù„ÙŠÙ„)")
        
        return {
            "optimized_text": optimized_text,
            "original_length": original_length,
            "optimized_length": optimized_length,
            "reduction_percentage": round(reduction_percentage, 1)
        }
    
    def _remove_noise_simple(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©"""
        
        # Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø­Ø°ÙÙ‡Ø§
        noise_patterns = [
            r'Scene Overview.*?(?=\n\n|\n[A-Z]|\nâ€¢|\n*)',
            r'Technical Details.*?(?=\n\n|\n[A-Z]|\nâ€¢|\n*)',
            r'Spatial Relationships.*?(?=\n\n|\n[A-Z]|\nâ€¢|\n*)',
            r'Analysis.*?(?=\n\n|\n[A-Z]|\nâ€¢|\n*)',
            r'Summary.*?(?=\n\n|\n[A-Z]|\nâ€¢|\n*)',
            r'â€¢.*?(?=\n)',
            r'The image.*?\.(?=\n)',
            r'Arabic text.*?\.(?=\n)',
            r'No visible.*?\.(?=\n)'
        ]
        
        cleaned = text
        for pattern in noise_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # ØªÙ†Ø¸ÙŠÙ Ø¹Ø§Ù…
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        cleaned = re.sub(r' +', ' ', cleaned)        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        
        return cleaned.strip()
    
    def _extract_important_parts(self, text: str) -> list:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø©"""
        
        important_parts = []
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            if self._is_important_line(line):
                important_parts.append(line)
        
        return important_parts
    
    def _is_important_line(self, line: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø·Ø± Ù…Ù‡Ù…"""
        
        # Ø³Ø·Ø± Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹
        if len(line) < 5:
            return False
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        important_keywords = [
            'Ù‚Ø§Ø¹Ø¯Ø©', 'Ù…Ø¨Ø¯Ø£', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù†Ø¸Ø±ÙŠØ©', 'ØªØ¹Ø±ÙŠÙ',
            'Ø¨Ø§Ø³ÙƒØ§Ù„', 'Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ', 'Ø¶ØºØ·', 'Ø³Ø§Ø¦Ù„',
            'ØªØ·Ø¨ÙŠÙ‚Ø§Øª', 'Ù…Ø«Ø§Ù„', 'Ù…Ù„Ø§Ø­Ø¸Ø©', 'Ù…Ù‡Ù…'
        ]
        
        line_lower = line.lower()
        
        # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù‡Ù…Ø©
        if any(keyword in line_lower for keyword in important_keywords):
            return True
        
        # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø·ÙˆÙŠÙ„
        if len(line) > 15 and re.search(r'[\u0600-\u06FF]', line):
            return True
        
        # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ Ø§Ù„Ø´Ø±Ø·Ø© (Ù‚ÙˆØ§Ø¦Ù…)
        if line.startswith('-') or line.startswith('*'):
            return True
        
        return False
    
    def _build_optimized_simple(self, parts: list, max_chars: int) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù† Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©"""
        
        if not parts:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙŠØ§Øª Ù…Ù‡Ù…Ø©"
        
        optimized_parts = []
        current_length = 0
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: Ø§Ù„Ø£Ù‡Ù… Ø£ÙˆÙ„Ø§Ù‹
        sorted_parts = self._sort_by_importance(parts)
        
        for part in sorted_parts:
            if current_length + len(part) + 5 < max_chars:  # Ù‡Ø§Ù…Ø´ Ø£Ù…Ø§Ù†
                optimized_parts.append(part)
                current_length += len(part) + 1  # +1 Ù„Ù„Ø³Ø·Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯
            else:
                break
        
        return '\n'.join(optimized_parts)
    
    def _sort_by_importance(self, parts: list) -> list:
        """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©"""
        
        def importance_score(line):
            score = 0
            line_lower = line.lower()
            
            # Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆÙ…Ø¨Ø§Ø¯Ø¦ (Ø§Ù„Ø£Ù‡Ù…)
            if any(keyword in line_lower for keyword in ['Ù‚Ø§Ø¹Ø¯Ø©', 'Ù…Ø¨Ø¯Ø£', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù†Ø¸Ø±ÙŠØ©']):
                score += 100
            
            # ØªØ¹Ø±ÙŠÙØ§Øª
            if any(keyword in line_lower for keyword in ['ØªØ¹Ø±ÙŠÙ', 'Ù‡Ùˆ', 'Ù‡ÙŠ']):
                score += 80
            
            # ØªØ·Ø¨ÙŠÙ‚Ø§Øª
            if any(keyword in line_lower for keyword in ['ØªØ·Ø¨ÙŠÙ‚Ø§Øª', 'Ø§Ø³ØªØ®Ø¯Ø§Ù…']):
                score += 60
            
            # Ù…Ù„Ø§Ø­Ø¸Ø§Øª
            if any(keyword in line_lower for keyword in ['Ù…Ù„Ø§Ø­Ø¸Ø©', 'Ù…Ù‡Ù…']):
                score += 40
            
            # Ø·ÙˆÙ„ Ø§Ù„Ø®Ø·
            score += min(len(line) // 10, 20)
            
            return score
        
        return sorted(parts, key=importance_score, reverse=True)


# Ø¥Ù†Ø´Ø§Ø¡ instance ÙˆØ§Ø­Ø¯
simple_optimizer = SimpleOptimizer() 