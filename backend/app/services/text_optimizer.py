"""
Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙƒÙŠ
Smart Text Optimization System

Ø§Ù„Ù‡Ø¯Ù: ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„ØªÙˆÙƒÙ†Ø² 70%+ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ¯Ø©
"""

import re
from typing import List, Dict, Any, Tuple
from loguru import logger


class TextOptimizer:
    """Ù…Ø­Ø³Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙƒÙŠ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ØªÙˆÙƒÙ†Ø²"""
    
    def __init__(self):
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        self.educational_keywords = {
            'scientific_terms': [
                'Ù‚Ø§Ø¹Ø¯Ø©', 'Ù…Ø¨Ø¯Ø£', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù†Ø¸Ø±ÙŠØ©', 'ØªØ¹Ø±ÙŠÙ', 'Ù…Ø¹Ø§Ø¯Ù„Ø©',
                'Ø¨Ø§Ø³ÙƒØ§Ù„', 'Ù†ÙŠÙˆØªÙ†', 'Ø£Ø±Ø®Ù…ÙŠØ¯Ø³', 'Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ†',
                'Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ', 'ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ', 'Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ', 'Ø­Ø±Ø§Ø±ÙŠ',
                'Ø¶ØºØ·', 'Ù‚ÙˆØ©', 'Ø·Ø§Ù‚Ø©', 'Ø³Ø±Ø¹Ø©', 'ØªØ³Ø§Ø±Ø¹', 'ÙƒØªÙ„Ø©'
            ],
            'educational_elements': [
                'Ø³Ø¤Ø§Ù„', 'Ù…Ø«Ø§Ù„', 'ØªÙ…Ø±ÙŠÙ†', 'Ù†Ø´Ø§Ø·', 'ØªØ·Ø¨ÙŠÙ‚', 'Ù…Ù„Ø§Ø­Ø¸Ø©',
                'ØªØ¹Ø±ÙŠÙ', 'Ø´Ø±Ø­', 'ØªÙˆØ¶ÙŠØ­', 'Ø®Ù„Ø§ØµØ©', 'Ù…Ù„Ø®Øµ'
            ],
            'mathematical_terms': [
                'Ù…Ø¹Ø§Ø¯Ù„Ø©', 'Ø­Ø³Ø§Ø¨', 'Ø±Ù‚Ù…', 'Ù†Ø³Ø¨Ø©', 'Ù†ØªÙŠØ¬Ø©', 'Ø­Ù„',
                'Ù…ØªØºÙŠØ±', 'Ø«Ø§Ø¨Øª', 'Ø¯Ø§Ù„Ø©', 'Ø±Ø³Ù…', 'Ø¬Ø¯ÙˆÙ„', 'Ø¥Ø­ØµØ§Ø¦ÙŠØ©'
            ]
        }
        
        # Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø­Ø°ÙÙ‡Ø§ (ÙˆØµÙ LandingAI Ø§Ù„Ù…ÙØµÙ„)
        self.noise_patterns = [
            r'Scene Overview.*?(?=\n\n|\n[A-Z]|\nâ€¢)',
            r'Technical Details.*?(?=\n\n|\n[A-Z]|\nâ€¢)',
            r'Spatial Relationships.*?(?=\n\n|\n[A-Z]|\nâ€¢)',
            r'Analysis.*?(?=\n\n|\n[A-Z]|\nâ€¢)',
            r'photo:.*?(?=\n\n|\n[A-Z])',
            r'illustration:.*?(?=\n\n|\n[A-Z])',
            r'figure:.*?(?=\n\n|\n[A-Z])',
            r'Summary.*?(?=\n\n|\n[A-Z])',
            r'from page \d+.*?\)',
            r'with ID [a-f0-9-]+',
            r'No visible scale.*?\.',
            r'Arabic text.*?translation:.*?\)',
            r'The image.*?educational.*?\.',
            r'Circular.*?number.*?\.',
            r'Main subject.*?background\.',
        ]
        
        logger.info("âœ… ØªÙ… ØªÙƒÙˆÙŠÙ† Ù…Ø­Ø³Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙƒÙŠ")
    
    def optimize_for_ai_analysis(self, text: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        
        if not text or not text.strip():
            return {
                "optimized_text": "",
                "original_length": 0,
                "optimized_length": 0,
                "reduction_percentage": 0,
                "extracted_elements": {}
            }
        
        original_length = len(text)
        logger.info(f"ðŸ”§ Ø¨Ø¯Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ: {original_length} Ø­Ø±Ù")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªÙ†Ø¸ÙŠÙ Ø£ÙˆÙ„ÙŠ
        cleaned_text = self._remove_noise(text)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù‡Ù…Ø©
        extracted_elements = self._extract_educational_elements(cleaned_text)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù†
        optimized_text = self._build_optimized_text(extracted_elements, max_tokens)
        
        if optimized_text is None:
            optimized_text = ""
        
        optimized_length = len(optimized_text)
        reduction_percentage = ((original_length - optimized_length) / original_length * 100) if original_length > 0 else 0
        
        logger.info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ø³ÙŠÙ†: {original_length} -> {optimized_length} Ø­Ø±Ù ({reduction_percentage:.1f}% ØªÙ‚Ù„ÙŠÙ„)")
        
        return {
            "optimized_text": optimized_text,
            "original_length": original_length,
            "optimized_length": optimized_length,
            "reduction_percentage": round(reduction_percentage, 1),
            "extracted_elements": extracted_elements
        }
    
    def _remove_noise(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©"""
        
        cleaned = text
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        for pattern in self.noise_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        cleaned = re.sub(r' +', ' ', cleaned)
        
        return cleaned.strip()
    
    def _extract_educational_elements(self, text: str) -> Dict[str, List[str]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©"""
        
        elements = {
            "definitions": [],
            "laws_and_principles": [],
            "examples": [],
            "questions": [],
            "explanations": [],
            "applications": [],
            "notes": [],
            "other_important": []
        }
        
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø·Ø± Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            line_type = self._classify_line(line)
            
            if line_type in elements:
                elements[line_type].append(line)
            elif self._is_important_content(line):
                elements["other_important"].append(line)
        
        return elements
    
    def _classify_line(self, line: str) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø·Ø± Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        
        line_lower = line.lower()
        
        # ØªØ¹Ø±ÙŠÙØ§Øª ÙˆÙ‚ÙˆØ§Ù†ÙŠÙ†
        if any(keyword in line_lower for keyword in ['Ù‚Ø§Ø¹Ø¯Ø©', 'Ù…Ø¨Ø¯Ø£', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù†Ø¸Ø±ÙŠØ©']):
            return "laws_and_principles"
        
        if any(keyword in line_lower for keyword in ['ØªØ¹Ø±ÙŠÙ', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'ÙŠÙØ¹Ø±Ù']):
            return "definitions"
        
        # Ø£Ø³Ø¦Ù„Ø©
        if line.endswith('ØŸ') or any(keyword in line_lower for keyword in ['Ø³Ø¤Ø§Ù„', 'Ø§Ø³Ø£Ù„', 'Ù…Ø§ Ù‡Ùˆ', 'ÙƒÙŠÙ', 'Ù„Ù…Ø§Ø°Ø§']):
            return "questions"
        
        # Ø£Ù…Ø«Ù„Ø© ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª
        if any(keyword in line_lower for keyword in ['Ù…Ø«Ø§Ù„', 'ØªØ·Ø¨ÙŠÙ‚', 'Ø§Ø³ØªØ®Ø¯Ø§Ù…', 'ÙŠÙØ³ØªØ®Ø¯Ù…']):
            return "examples"
        
        if any(keyword in line_lower for keyword in ['ØªØ·Ø¨ÙŠÙ‚Ø§Øª', 'Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª']):
            return "applications"
        
        # Ù…Ù„Ø§Ø­Ø¸Ø§Øª
        if any(keyword in line_lower for keyword in ['Ù…Ù„Ø§Ø­Ø¸Ø©', 'Ø§Ù†ØªØ¨Ù‡', 'Ù…Ù‡Ù…', 'ØªØ°ÙƒØ±']):
            return "notes"
        
        # Ø´Ø±Ø­ ÙˆØªÙˆØ¶ÙŠØ­
        if any(keyword in line_lower for keyword in ['Ø´Ø±Ø­', 'ØªÙˆØ¶ÙŠØ­', 'Ø¹Ù†Ø¯Ù…Ø§', 'Ø¥Ø°Ø§', 'Ù„Ø£Ù†']):
            return "explanations"
        
        return "other_important"
    
    def _is_important_content(self, line: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø·Ø± Ù…Ù‡Ù…"""
        
        # Ø³Ø·Ø± Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ -> ØºÙŠØ± Ù…Ù‡Ù…
        if len(line) < 10:
            return False
        
        # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù‡Ù…Ø©
        for category in self.educational_keywords.values():
            if any(keyword in line for keyword in category):
                return True
        
        # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª
        if re.search(r'\d+|=|%|\+|\-|\*|/', line):
            return True
        
        # Ø³Ø·Ø± Ø·ÙˆÙŠÙ„ Ù†Ø³Ø¨ÙŠØ§Ù‹ Ù…Ø¹ Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ
        if len(line) > 20 and re.search(r'[\u0600-\u06FF]', line):
            return True
        
        return False
    
    def _build_optimized_text(self, elements: Dict[str, List[str]], max_tokens: int) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù† Ø¨ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"""
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        priority_order = [
            "laws_and_principles",
            "definitions", 
            "explanations",
            "examples",
            "applications",
            "questions",
            "notes",
            "other_important"
        ]
        
        optimized_parts = []
        current_length = 0
        max_chars = max_tokens * 4  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ: 1 ØªÙˆÙƒÙ† = 4 Ø£Ø­Ø±Ù Ø¹Ø±Ø¨ÙŠØ©
        
        for category in priority_order:
            if category not in elements:
                continue
            
            category_items = elements[category]
            if not category_items:
                continue
            
            # Ø¥Ø¶Ø§ÙØ© Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø³Ù… Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            section_header = self._get_section_header(category)
            if section_header is None:
                section_header = ""
            
            for item in category_items:
                if item is None or not item:
                    continue
                estimated_addition = len(item) + len(section_header) + 10  # Ù‡Ø§Ù…Ø´ Ø£Ù…Ø§Ù†
                
                if current_length + estimated_addition > max_chars:
                    break
                
                if section_header and section_header not in optimized_parts:
                    optimized_parts.append(section_header)
                    current_length += len(section_header)
                    section_header = None  # Ù„Ø§ Ù†ÙƒØ±Ø±Ù‡Ø§
                
                optimized_parts.append(item)
                current_length += len(item)
            
            if current_length >= max_chars:
                break
        
        result = '\n'.join(optimized_parts)
        return result if result else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙŠØ§Øª Ù…Ù‡Ù…Ø©"
    
    def _get_section_header(self, category: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø³Ù…"""
        
        headers = {
            "laws_and_principles": "## Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…Ø¨Ø§Ø¯Ø¦:",
            "definitions": "## Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª:",
            "explanations": "## Ø§Ù„Ø´Ø±Ø­:",
            "examples": "## Ø§Ù„Ø£Ù…Ø«Ù„Ø©:",
            "applications": "## Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª:",
            "questions": "## Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:",
            "notes": "## Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©:",
            "other_important": "## Ù…Ø­ØªÙˆÙ‰ Ø¥Ø¶Ø§ÙÙŠ:"
        }
        
        return headers.get(category, "")
    
    def compare_optimized_texts(self, text1: str, text2: str) -> Dict[str, Any]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ù†ØµÙŠÙ† Ù…Ø­Ø³Ù†ÙŠÙ†"""
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØµÙŠÙ†
        opt1 = self.optimize_for_ai_analysis(text1)
        opt2 = self.optimize_for_ai_analysis(text2)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        elements_comparison = self._compare_elements(
            opt1["extracted_elements"], 
            opt2["extracted_elements"]
        )
        
        return {
            "text1_optimization": opt1,
            "text2_optimization": opt2,
            "elements_comparison": elements_comparison,
            "total_reduction": (opt1["reduction_percentage"] + opt2["reduction_percentage"]) / 2,
            "tokens_saved_estimate": (opt1["original_length"] + opt2["original_length"] - 
                                    opt1["optimized_length"] - opt2["optimized_length"]) // 4
        }
    
    def _compare_elements(self, elements1: Dict, elements2: Dict) -> Dict[str, Any]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©"""
        
        comparison = {}
        
        all_categories = set(elements1.keys()) | set(elements2.keys())
        
        for category in all_categories:
            items1 = set(elements1.get(category, []))
            items2 = set(elements2.get(category, []))
            
            common = items1 & items2
            only_in_1 = items1 - items2
            only_in_2 = items2 - items1
            
            comparison[category] = {
                "common_count": len(common),
                "unique_to_text1": len(only_in_1),
                "unique_to_text2": len(only_in_2),
                "similarity_percentage": len(common) / max(len(items1 | items2), 1) * 100
            }
        
        return comparison


# Ø¥Ù†Ø´Ø§Ø¡ instance ÙˆØ§Ø­Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©
text_optimizer = TextOptimizer() 