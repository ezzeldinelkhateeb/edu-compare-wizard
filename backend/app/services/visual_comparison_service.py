"""
Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„ØµÙˆØ± Ø§Ù„Ù…Ø­Ø³Ù†Ø©
Enhanced Visual Image Comparison Service using SSIM + pHash + CLIP + Structural Analysis
"""

import os
import cv2
import numpy as np
from typing import Dict, Any, Tuple, Optional, List
from datetime import datetime
import tempfile
from pathlib import Path
import json

from skimage.metrics import structural_similarity as ssim
import imagehash
from PIL import Image, ImageEnhance, ImageOps
from pydantic import BaseModel, Field
from loguru import logger

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù€ sentence_transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ sentence_transformers")
except ImportError:
    logger.warning("âš ï¸ sentence_transformers ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø³ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„ CLIP")
    SentenceTransformer = None
    SENTENCE_TRANSFORMERS_AVAILABLE = False

from app.core.config import get_settings

settings = get_settings()


class VisualComparisonResult(BaseModel):
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
    similarity_score: float = Field(..., ge=0, le=100, description="Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©")
    ssim_score: float = Field(..., ge=0, le=1, description="Ù†ØªÙŠØ¬Ø© SSIM")
    phash_score: float = Field(..., ge=0, le=1, description="Ù†ØªÙŠØ¬Ø© pHash")
    clip_score: Optional[float] = Field(None, ge=0, le=1, description="Ù†ØªÙŠØ¬Ø© CLIP")
    
    # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ø­Ø³Ù†Ø©
    histogram_correlation: float = Field(default=0.0, description="Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…")
    feature_matching_score: float = Field(default=0.0, description="ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª")
    edge_similarity: float = Field(default=0.0, description="ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù")
    
    # ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ÙŠ
    layout_similarity: float = Field(default=0.0, description="ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØªØ®Ø·ÙŠØ·")
    text_region_overlap: float = Field(default=0.0, description="ØªØ¯Ø§Ø®Ù„ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Øµ")
    
    # Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    weights_used: Dict[str, float] = Field(default_factory=dict)
    
    # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    processing_time: float
    old_image_path: str
    new_image_path: str
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµÙˆØ±
    old_image_size: Tuple[int, int] = Field(default=(0, 0))
    new_image_size: Tuple[int, int] = Field(default=(0, 0))
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ù…Ø­Ø³Ù†
    difference_detected: bool = False
    difference_map_path: Optional[str] = None
    major_changes_detected: bool = False
    
    # Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØºÙŠÙŠØ±
    changed_regions: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    mean_squared_error: float = 0.0
    peak_signal_noise_ratio: float = 0.0
    
    # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰
    content_type_detected: str = "educational"
    probable_content_match: bool = False
    
    # Ø±Ø³Ø§Ø¦Ù„
    analysis_summary: str = ""
    recommendations: str = ""
    confidence_notes: str = ""


class EnhancedVisualComparisonService:
    """Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØµÙˆØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"""
    
    def __init__(self):
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ
        self.ssim_weight = float(os.getenv("VISUAL_COMPARISON_SSIM_WEIGHT", "0.25"))
        self.phash_weight = float(os.getenv("VISUAL_COMPARISON_PHASH_WEIGHT", "0.15"))
        self.clip_weight = float(os.getenv("VISUAL_COMPARISON_CLIP_WEIGHT", "0.25"))
        self.histogram_weight = float(os.getenv("VISUAL_COMPARISON_HISTOGRAM_WEIGHT", "0.10"))
        self.feature_weight = float(os.getenv("VISUAL_COMPARISON_FEATURE_WEIGHT", "0.15"))
        self.edge_weight = float(os.getenv("VISUAL_COMPARISON_EDGE_WEIGHT", "0.10"))
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† ØªØ³Ø§ÙˆÙŠ 1.0
        total_weight = (self.ssim_weight + self.phash_weight + self.clip_weight + 
                       self.histogram_weight + self.feature_weight + self.edge_weight)
        if abs(total_weight - 1.0) > 0.01:
            logger.warning(f"âš ï¸ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† {total_weight:.2f} â‰  1.0ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ·Ø¨ÙŠØ¹")
            self.ssim_weight /= total_weight
            self.phash_weight /= total_weight
            self.clip_weight /= total_weight
            self.histogram_weight /= total_weight
            self.feature_weight /= total_weight
            self.edge_weight /= total_weight
        
        # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ (Ø£Ù‚Ù„ Ø­Ø³Ø§Ø³ÙŠØ©)
        self.similarity_threshold = float(os.getenv("VISUAL_COMPARISON_THRESHOLD", "0.75"))
        self.high_similarity_threshold = float(os.getenv("HIGH_SIMILARITY_THRESHOLD", "0.90"))
        
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP
        self.clip_model = None
        self.clip_available = False
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                self.clip_model = SentenceTransformer('clip-ViT-B-32')
                self.clip_available = True
                logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP Ø¨Ù†Ø¬Ø§Ø­")
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ CLIP: {e}")
                self.clip_available = False
        else:
            logger.info("â„¹ï¸ CLIP ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø³ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„Ù‡")
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† CLIP Ù…ØªØ§Ø­Ø§Ù‹
        if not self.clip_available:
            remaining_weight = 1.0 - self.clip_weight
            factor = 1.0 / remaining_weight
            self.ssim_weight *= factor
            self.phash_weight *= factor
            self.histogram_weight *= factor
            self.feature_weight *= factor
            self.edge_weight *= factor
            self.clip_weight = 0.0
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ SIFT Ù„Ù„Ù…ÙŠØ²Ø§Øª
        try:
            self.sift = cv2.SIFT_create()
            self.feature_matching_available = True
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ SIFT Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª")
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ SIFT: {e}")
            self.feature_matching_available = False
        
        logger.info(f"ğŸ”§ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©:")
        logger.info(f"   SSIM: {self.ssim_weight:.2f}")
        logger.info(f"   pHash: {self.phash_weight:.2f}")
        logger.info(f"   CLIP: {self.clip_weight:.2f}")
        logger.info(f"   Histogram: {self.histogram_weight:.2f}")
        logger.info(f"   Features: {self.feature_weight:.2f}")
        logger.info(f"   Edges: {self.edge_weight:.2f}")

    async def compare_images(
        self, 
        old_image_path: str, 
        new_image_path: str,
        output_dir: Optional[str] = None
    ) -> VisualComparisonResult:
        """
        Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØµÙˆØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        Enhanced comparison for educational images
        """
        start_time = datetime.now()
        
        logger.info(f"ğŸ–¼ï¸ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©: {Path(old_image_path).name} vs {Path(new_image_path).name}")
        
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª
            if not os.path.exists(old_image_path) or not os.path.exists(new_image_path):
                raise FileNotFoundError("Ø£Ø­Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
            old_image = self._load_and_preprocess_image(old_image_path)
            new_image = self._load_and_preprocess_image(new_image_path)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ©
            old_pil = Image.open(old_image_path)
            new_pil = Image.open(new_image_path)
            old_size = old_pil.size
            new_size = new_pil.size
            
            # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content_type = self._detect_content_type(old_image, new_image)
            
            # 1. Ø­Ø³Ø§Ø¨ SSIM
            ssim_score = self._calculate_ssim(old_image, new_image)
            logger.debug(f"ğŸ“Š SSIM Score: {ssim_score:.3f}")
            
            # 2. Ø­Ø³Ø§Ø¨ pHash
            phash_score = self._calculate_phash_similarity(old_image_path, new_image_path)
            logger.debug(f"ğŸ“Š pHash Score: {phash_score:.3f}")
            
            # 3. Ø­Ø³Ø§Ø¨ CLIP (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
            clip_score = None
            if self.clip_available:
                clip_score = self._calculate_clip_similarity(old_image_path, new_image_path)
                logger.debug(f"ğŸ“Š CLIP Score: {clip_score:.3f}")
            
            # 4. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
            histogram_correlation = self._calculate_histogram_correlation(old_image, new_image)
            logger.debug(f"ğŸ“Š Histogram Correlation: {histogram_correlation:.3f}")
            
            # 5. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª
            feature_matching_score = 0.0
            if self.feature_matching_available:
                feature_matching_score = self._calculate_feature_matching(old_image, new_image)
                logger.debug(f"ğŸ“Š Feature Matching: {feature_matching_score:.3f}")
            
            # 6. ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù
            edge_similarity = self._calculate_edge_similarity(old_image, new_image)
            logger.debug(f"ğŸ“Š Edge Similarity: {edge_similarity:.3f}")
            
            # 7. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
            overall_similarity = self._calculate_enhanced_weighted_similarity(
                ssim_score, phash_score, clip_score, 
                histogram_correlation, feature_matching_score, edge_similarity
            )
            
            # 8. ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ÙŠ Ù…Ø­Ø³Ù†
            layout_similarity = self._analyze_layout_similarity(old_image, new_image)
            text_region_overlap = self._analyze_text_regions(old_image, new_image)
            
            # 9. ÙƒØ´Ù Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…ØªØºÙŠØ±Ø©
            changed_regions = self._detect_changed_regions(old_image, new_image)
            
            # 10. Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
            difference_map_path = None
            if output_dir:
                difference_map_path = self._create_enhanced_difference_map(
                    old_image, new_image, output_dir, 
                    Path(old_image_path).stem, Path(new_image_path).stem,
                    changed_regions
                )
            
            # 11. Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
            mse = self._calculate_mse(old_image, new_image)
            psnr = self._calculate_psnr(mse)
            
            # 12. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
            difference_detected = overall_similarity < (self.similarity_threshold * 100)
            major_changes = overall_similarity < 60  # Ø¹ØªØ¨Ø© Ø£Ù‚Ù„ Ù„Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
            probable_content_match = overall_similarity > (self.high_similarity_threshold * 100)
            
            # 13. ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ø³Ù† Ù…Ø¹ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ø«Ù‚Ø©
            analysis_summary = self._generate_enhanced_analysis_summary(
                overall_similarity, ssim_score, phash_score, clip_score, 
                histogram_correlation, feature_matching_score, edge_similarity,
                difference_detected, content_type
            )
            
            recommendations = self._generate_enhanced_recommendations(
                overall_similarity, major_changes, difference_detected, 
                probable_content_match, content_type
            )
            
            confidence_notes = self._generate_confidence_notes(
                ssim_score, phash_score, feature_matching_score, overall_similarity
            )
            
            # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = VisualComparisonResult(
                similarity_score=round(overall_similarity, 2),
                ssim_score=round(ssim_score, 3),
                phash_score=round(phash_score, 3),
                clip_score=round(clip_score, 3) if clip_score else None,
                histogram_correlation=round(histogram_correlation, 3),
                feature_matching_score=round(feature_matching_score, 3),
                edge_similarity=round(edge_similarity, 3),
                layout_similarity=round(layout_similarity, 3),
                text_region_overlap=round(text_region_overlap, 3),
                weights_used={
                    "ssim": self.ssim_weight,
                    "phash": self.phash_weight,
                    "clip": self.clip_weight,
                    "histogram": self.histogram_weight,
                    "features": self.feature_weight,
                    "edges": self.edge_weight
                },
                processing_time=processing_time,
                old_image_path=old_image_path,
                new_image_path=new_image_path,
                old_image_size=old_size,
                new_image_size=new_size,
                difference_detected=difference_detected,
                difference_map_path=difference_map_path,
                major_changes_detected=major_changes,
                changed_regions=changed_regions,
                mean_squared_error=mse,
                peak_signal_noise_ratio=psnr,
                content_type_detected=content_type,
                probable_content_match=probable_content_match,
                analysis_summary=analysis_summary,
                recommendations=recommendations,
                confidence_notes=confidence_notes
            )
            
            logger.info(f"âœ… Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©: {overall_similarity:.1f}% ØªØ·Ø§Ø¨Ù‚ ÙÙŠ {processing_time:.2f}s")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©: {e}")
            
            return VisualComparisonResult(
                similarity_score=0.0,
                ssim_score=0.0,
                phash_score=0.0,
                processing_time=processing_time,
                old_image_path=old_image_path,
                new_image_path=new_image_path,
                analysis_summary=f"ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: {str(e)}",
                recommendations="ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©",
                confidence_notes="Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø®Ø·Ø£"
            )
    
    def _load_and_preprocess_image(self, image_path: str) -> np.ndarray:
        """ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©"""
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©: {image_path}")
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù…Ø§Ø¯ÙŠ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¬ÙˆØ¯Ø©
        gray = cv2.GaussianBlur(gray, (3, 3), 0)
        
        return gray
    
    def _calculate_ssim(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ SSIM Ø¨ÙŠÙ† ØµÙˆØ±ØªÙŠÙ†"""
        try:
            # ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù„ØªØªØ·Ø§Ø¨Ù‚
            if img1.shape != img2.shape:
                target_shape = (min(img1.shape[1], img2.shape[1]), 
                               min(img1.shape[0], img2.shape[0]))
                img1 = cv2.resize(img1, target_shape)
                img2 = cv2.resize(img2, target_shape)
            
            # Ø­Ø³Ø§Ø¨ SSIM
            score, _ = ssim(img1, img2, full=True)
            return max(0.0, min(1.0, score))  # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ÙŠÙ† 0 Ùˆ 1
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ SSIM: {e}")
            return 0.0
    
    def _calculate_phash_similarity(self, img1_path: str, img2_path: str) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ pHash Ø¨ÙŠÙ† ØµÙˆØ±ØªÙŠÙ†"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PIL
            img1 = Image.open(img1_path)
            img2 = Image.open(img2_path)
            
            # Ø­Ø³Ø§Ø¨ pHash
            hash1 = imagehash.phash(img1)
            hash2 = imagehash.phash(img2)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
            distance = hash1 - hash2
            max_distance = 64  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù…Ø³Ø§ÙØ© pHash
            similarity = 1.0 - (distance / max_distance)
            
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ pHash: {e}")
            return 0.0
    
    def _calculate_clip_similarity(self, img1_path: str, img2_path: str) -> Optional[float]:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ CLIP Ø¨ÙŠÙ† ØµÙˆØ±ØªÙŠÙ†"""
        if not self.clip_available:
            return None
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
            img1 = Image.open(img1_path).convert('RGB')
            img2 = Image.open(img2_path).convert('RGB')
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ embeddings
            embeddings = self.clip_model.encode([img1, img2])
            
            # Ø­Ø³Ø§Ø¨ cosine similarity
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ CLIP: {e}")
            return None
    
    def _calculate_histogram_correlation(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
            hist1 = cv2.calcHist([img1], [0], None, [256], [0, 256])
            hist2 = cv2.calcHist([img2], [0], None, [256], [0, 256])
            
            # Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
            correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
            return max(0.0, min(1.0, correlation))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…: {e}")
            return 0.0
    
    def _calculate_feature_matching(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ÙˆÙ„Ø§Øª SIFT
            kp1, des1 = self.sift.detectAndCompute(img1, None)
            kp2, des2 = self.sift.detectAndCompute(img2, None)
            
            # Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
            matches = bf.match(des1, des2)
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
            if len(matches) > 0:
                distances = [m.distance for m in matches]
                feature_matching_score = 1.0 - (np.mean(distances) / 256.0)
            else:
                feature_matching_score = 0.0
            
            return max(0.0, min(1.0, feature_matching_score))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª: {e}")
            return 0.0
    
    def _calculate_edge_similarity(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù…Ø§Ø¯ÙŠ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if len(img1.shape) == 3:
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            else:
                gray1 = img1.copy()
                
            if len(img2.shape) == 3:
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            else:
                gray2 = img2.copy()
            
            # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø¬Ø§Ù…
            h1, w1 = gray1.shape
            h2, w2 = gray2.shape
            
            if (h1, w1) != (h2, w2):
                # ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£ÙˆÙ„Ù‰
                gray2 = cv2.resize(gray2, (w1, h1))
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ÙˆÙ„Ø§Øª Ø­ÙˆØ§Ù
            edges1 = cv2.Canny(gray1, 100, 200)
            edges2 = cv2.Canny(gray2, 100, 200)
            
            # Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SSIM Ù„Ù„Ø­ÙˆØ§Ù
            from skimage.metrics import structural_similarity as ssim
            similarity = ssim(edges1, edges2)
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…ÙˆØ¬Ø¨Ø© Ø¨ÙŠÙ† 0 Ùˆ 1
            return max(0.0, min(1.0, (similarity + 1) / 2))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù: {e}")
            return 0.0
    
    def _analyze_layout_similarity(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØªØ®Ø·ÙŠØ·"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯ (Ù…Ø¹ ÙØ­Øµ Ø§Ù„Ù‚Ù†ÙˆØ§Øª)
            if len(img1.shape) == 3:
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            else:
                gray1 = img1.copy()
                
            if len(img2.shape) == 3:
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            else:
                gray2 = img2.copy()
            
            # Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØªØ®Ø·ÙŠØ·
            similarity = cv2.matchShapes(gray1, gray2, cv2.CONTOURS_MATCH_I2, 0)
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØªØ®Ø·ÙŠØ·: {e}")
            return 0.0
    
    def _analyze_text_regions(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¯Ø§Ø®Ù„ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Øµ"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯ (Ù…Ø¹ ÙØ­Øµ Ø§Ù„Ù‚Ù†ÙˆØ§Øª)
            if len(img1.shape) == 3:
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            else:
                gray1 = img1.copy()
                
            if len(img2.shape) == 3:
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            else:
                gray2 = img2.copy()
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø«Ù†Ø§Ø¦ÙŠ
            _, thresh1 = cv2.threshold(gray1, 127, 255, cv2.THRESH_BINARY)
            _, thresh2 = cv2.threshold(gray2, 127, 255, cv2.THRESH_BINARY)
            
            # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø¬Ø§Ù…
            h1, w1 = thresh1.shape
            h2, w2 = thresh2.shape
            
            if (h1, w1) != (h2, w2):
                thresh2 = cv2.resize(thresh2, (w1, h1))
            
            # Ø­Ø³Ø§Ø¨ ØªØ¯Ø§Ø®Ù„ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Øµ
            intersection = cv2.bitwise_and(thresh1, thresh2)
            union = cv2.bitwise_or(thresh1, thresh2)
            
            intersection_count = cv2.countNonZero(intersection)
            union_count = cv2.countNonZero(union)
            
            if union_count == 0:
                overlap = 1.0  # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù†Ø§Ø·Ù‚ Ù†Øµ ÙÙŠ Ø£ÙŠ Ù…Ù† Ø§Ù„ØµÙˆØ±ØªÙŠÙ†
            else:
                overlap = intersection_count / union_count
            
            return max(0.0, min(1.0, overlap))
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ ØªØ¯Ø§Ø®Ù„ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù†Øµ: {e}")
            return 0.0
    
    def _calculate_weighted_similarity(
        self, 
        ssim_score: float, 
        phash_score: float, 
        clip_score: Optional[float]
    ) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø¨Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        
        total_score = (
            ssim_score * self.ssim_weight +
            phash_score * self.phash_weight
        )
        
        if clip_score is not None and self.clip_weight > 0:
            total_score += clip_score * self.clip_weight
        
        return total_score * 100  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©
    
    def _calculate_enhanced_weighted_similarity(
        self, 
        ssim_score: float, 
        phash_score: float, 
        clip_score: Optional[float],
        histogram_correlation: float,
        feature_matching_score: float,
        edge_similarity: float
    ) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
        
        total_score = (
            ssim_score * self.ssim_weight +
            phash_score * self.phash_weight +
            histogram_correlation * self.histogram_weight +
            feature_matching_score * self.feature_weight +
            edge_similarity * self.edge_weight
        )
        
        if clip_score is not None and self.clip_weight > 0:
            total_score += clip_score * self.clip_weight
        
        return total_score * 100  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©
    
    def _create_difference_map(
        self, 
        img1: np.ndarray, 
        img2: np.ndarray, 
        output_dir: str,
        name1: str, 
        name2: str
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª"""
        try:
            # ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù„ØªØªØ·Ø§Ø¨Ù‚
            if img1.shape != img2.shape:
                target_shape = (min(img1.shape[1], img2.shape[1]), 
                               min(img1.shape[0], img2.shape[0]))
                img1 = cv2.resize(img1, target_shape)
                img2 = cv2.resize(img2, target_shape)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ù…Ø·Ù„Ù‚
            diff = cv2.absdiff(img1, img2)
            
            # ØªØ·Ø¨ÙŠÙ‚ threshold Ù„Ø¥Ø¨Ø±Ø§Ø² Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª
            _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ù…Ù„ÙˆÙ†Ø© Ù„Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª
            diff_colored = cv2.applyColorMap(thresh, cv2.COLORMAP_JET)
            
            # Ø­ÙØ¸ Ø§Ù„Ø®Ø±ÙŠØ·Ø©
            os.makedirs(output_dir, exist_ok=True)
            diff_path = os.path.join(output_dir, f"diff_{name1}_vs_{name2}.png")
            cv2.imwrite(diff_path, diff_colored)
            
            return diff_path
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª: {e}")
            return None
    
    def _calculate_mse(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ Mean Squared Error"""
        try:
            if img1.shape != img2.shape:
                target_shape = (min(img1.shape[1], img2.shape[1]), 
                               min(img1.shape[0], img2.shape[0]))
                img1 = cv2.resize(img1, target_shape)
                img2 = cv2.resize(img2, target_shape)
            
            mse = np.mean((img1.astype(float) - img2.astype(float)) ** 2)
            return float(mse)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ MSE: {e}")
            return 0.0
    
    def _calculate_psnr(self, mse: float) -> float:
        """Ø­Ø³Ø§Ø¨ Peak Signal-to-Noise Ratio"""
        if mse == 0:
            return float('inf')
        
        max_pixel_value = 255.0
        psnr = 20 * np.log10(max_pixel_value / np.sqrt(mse))
        return float(psnr)
    
    def _generate_analysis_summary(
        self, 
        overall_similarity: float,
        ssim_score: float,
        phash_score: float,
        clip_score: Optional[float],
        difference_detected: bool
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        
        summary_parts = [
            f"Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {overall_similarity:.1f}%",
            f"Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ (SSIM): {ssim_score:.3f}",
            f"Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¨ØµØ±ÙŠ (pHash): {phash_score:.3f}"
        ]
        
        if clip_score is not None:
            summary_parts.append(f"Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (CLIP): {clip_score:.3f}")
        
        if difference_detected:
            summary_parts.append("ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ù…Ù„Ø­ÙˆØ¸Ø© Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ±ØªÙŠÙ†")
        else:
            summary_parts.append("Ø§Ù„ØµÙˆØ±ØªØ§Ù† Ù…ØªØ´Ø§Ø¨Ù‡ØªØ§Ù† Ø¥Ù„Ù‰ Ø­Ø¯ ÙƒØ¨ÙŠØ±")
        
        return ". ".join(summary_parts)
    
    def _generate_recommendations(
        self, 
        overall_similarity: float,
        major_changes: bool,
        difference_detected: bool
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        
        if major_changes:
            return "ØªÙ… Ø§ÙƒØªØ´Ø§Ù ØªØºÙŠÙŠØ±Ø§Øª ÙƒØ¨ÙŠØ±Ø©. ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"
        elif difference_detected:
            return "ØªÙ… Ø§ÙƒØªØ´Ø§Ù ØªØºÙŠÙŠØ±Ø§Øª Ø·ÙÙŠÙØ©. ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„ØªØ­Ø¯ÙŠØ«"
        else:
            return "Ø§Ù„ØµÙˆØ±ØªØ§Ù† Ù…ØªØ´Ø§Ø¨Ù‡ØªØ§Ù† Ø¬Ø¯Ø§Ù‹. Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø§Ø¬Ø© Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª ÙƒØ¨ÙŠØ±Ø©"
    
    async def health_check(self) -> Dict[str, Any]:
        """ÙØ­Øµ ØµØ­Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©"""
        try:
            return {
                "status": "healthy",
                "message": "Visual Comparison Service Ø¬Ø§Ù‡Ø²",
                "clip_available": self.clip_available,
                "weights": {
                    "ssim": self.ssim_weight,
                    "phash": self.phash_weight,
                    "clip": self.clip_weight
                },
                "similarity_threshold": self.similarity_threshold
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "message": f"Ø®Ø·Ø£ ÙÙŠ Visual Comparison Service: {str(e)}",
                "error": str(e)
            }

    def _detect_content_type(self, img1: np.ndarray, img2: np.ndarray) -> str:
        """ÙƒØ´Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø¨ÙŠØ¶
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape) == 3 else img1
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape) == 3 else img2
            
            white_ratio1 = np.sum(gray1 > 200) / gray1.size
            white_ratio2 = np.sum(gray2 > 200) / gray2.size
            
            if white_ratio1 > 0.7 and white_ratio2 > 0.7:
                return "educational_text"
            elif white_ratio1 > 0.4 and white_ratio2 > 0.4:
                return "mixed_content"
            else:
                return "visual_content"
                
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {e}")
            return "unknown"
    
    def _detect_changed_regions(self, img1: np.ndarray, img2: np.ndarray) -> List[Dict[str, Any]]:
        """ÙƒØ´Ù Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…ØªØºÙŠØ±Ø©"""
        try:
            # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø¬Ø§Ù… Ø£ÙˆÙ„Ø§Ù‹
            h1, w1 = img1.shape[:2]
            h2, w2 = img2.shape[:2]
            
            if (h1, w1) != (h2, w2):
                img2_resized = cv2.resize(img2, (w1, h1))
            else:
                img2_resized = img2.copy()
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ù†ÙˆØ§Øª
            if len(img1.shape) != len(img2_resized.shape):
                if len(img1.shape) == 3 and len(img2_resized.shape) == 2:
                    img2_resized = cv2.cvtColor(img2_resized, cv2.COLOR_GRAY2BGR)
                elif len(img1.shape) == 2 and len(img2_resized.shape) == 3:
                    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù
            diff = cv2.absdiff(img1, img2_resized)
            gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY) if len(diff.shape) == 3 else diff
            
            # ØªØ­Ø¯ÙŠØ¯ Ø¹ØªØ¨Ø© Ù„Ù„ØªØºÙŠÙŠØ±
            _, thresh = cv2.threshold(gray_diff, 30, 255, cv2.THRESH_BINARY)
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…ØªØºÙŠØ±Ø©
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            changed_regions = []
            for i, contour in enumerate(contours):
                if cv2.contourArea(contour) > 100:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
                    x, y, w, h = cv2.boundingRect(contour)
                    area = cv2.contourArea(contour)
                    changed_regions.append({
                        "id": f"region_{i}",
                        "x": int(x),
                        "y": int(y),
                        "width": int(w),
                        "height": int(h),
                        "area": int(area),
                        "center": {"x": int(x + w/2), "y": int(y + h/2)}
                    })
            
            return changed_regions
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…ØªØºÙŠØ±Ø©: {e}")
            return []
    
    def _create_enhanced_difference_map(
        self, 
        img1: np.ndarray, 
        img2: np.ndarray, 
        output_dir: str,
        name1: str, 
        name2: str,
        changed_regions: List[Dict[str, Any]]
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ù…Ø­Ø³Ù†Ø©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            diff_filename = f"difference_map_{name1}_vs_{name2}_{timestamp}.png"
            diff_path = os.path.join(output_dir, diff_filename)
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£Ø­Ø¬Ø§Ù…
            h1, w1 = img1.shape[:2]
            h2, w2 = img2.shape[:2]
            
            if (h1, w1) != (h2, w2):
                # ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£ÙˆÙ„Ù‰
                img2_resized = cv2.resize(img2, (w1, h1))
            else:
                img2_resized = img2.copy()
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ù†ÙˆØ§Øª
            if len(img1.shape) != len(img2_resized.shape):
                if len(img1.shape) == 3 and len(img2_resized.shape) == 2:
                    img2_resized = cv2.cvtColor(img2_resized, cv2.COLOR_GRAY2BGR)
                elif len(img1.shape) == 2 and len(img2_resized.shape) == 3:
                    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù
            diff = cv2.absdiff(img1, img2_resized)
            
            # ØªØ­Ø³ÙŠÙ† Ø¹Ø±Ø¶ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª
            diff_enhanced = cv2.convertScaleAbs(diff, alpha=2.0, beta=30)
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø±ÙØ§Øª Ù„Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…ØªØºÙŠØ±Ø©
            diff_with_regions = diff_enhanced.copy()
            for region in changed_regions:
                x, y, w, h = region["x"], region["y"], region["width"], region["height"]
                cv2.rectangle(diff_with_regions, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(diff_with_regions, f"R{region['id'][-1]}", 
                           (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
            cv2.imwrite(diff_path, diff_with_regions)
            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª: {diff_path}")
            
            return diff_path
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª: {e}")
            return ""
    
    def _generate_enhanced_analysis_summary(
        self, 
        overall_similarity: float,
        ssim_score: float,
        phash_score: float,
        clip_score: Optional[float],
        histogram_correlation: float,
        feature_matching_score: float,
        edge_similarity: float,
        difference_detected: bool,
        content_type: str
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ø³Ù†"""
        
        analysis_parts = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        if overall_similarity >= 95:
            analysis_parts.append("ğŸ¯ Ø§Ù„ØµÙˆØ± Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ø¨Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹")
        elif overall_similarity >= 85:
            analysis_parts.append("âœ… Ø§Ù„ØµÙˆØ± Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¨Ø¯Ø±Ø¬Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ø¹ Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø·ÙÙŠÙØ©")
        elif overall_similarity >= 70:
            analysis_parts.append("âš ï¸ Ø§Ù„ØµÙˆØ± Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¨Ø¯Ø±Ø¬Ø© Ù…ØªÙˆØ³Ø·Ø© Ù…Ø¹ ÙˆØ¬ÙˆØ¯ Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ù…Ù„Ø­ÙˆØ¸Ø©")
        elif overall_similarity >= 50:
            analysis_parts.append("ğŸ” Ø§Ù„ØµÙˆØ± Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯Ø±Ø¬Ø© ÙƒØ¨ÙŠØ±Ø© Ù…Ø¹ Ø¨Ø¹Ø¶ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡")
        else:
            analysis_parts.append("âŒ Ø§Ù„ØµÙˆØ± Ù…Ø®ØªÙ„ÙØ© ØªÙ…Ø§Ù…Ø§Ù‹")
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ
        metrics_details = []
        metrics_details.append(f"SSIM: {ssim_score:.3f}")
        metrics_details.append(f"pHash: {phash_score:.3f}")
        if clip_score is not None:
            metrics_details.append(f"CLIP: {clip_score:.3f}")
        metrics_details.append(f"Histogram: {histogram_correlation:.3f}")
        metrics_details.append(f"Features: {feature_matching_score:.3f}")
        metrics_details.append(f"Edges: {edge_similarity:.3f}")
        
        analysis_parts.append(f"ğŸ“Š Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©: {' | '.join(metrics_details)}")
        
        # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_analysis = {
            "educational_text": "ğŸ“š Ù…Ø­ØªÙˆÙ‰ ØªØ¹Ù„ÙŠÙ…ÙŠ Ù†ØµÙŠ",
            "mixed_content": "ğŸ“ Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªÙ„Ø· (Ù†Øµ ÙˆØµÙˆØ±)",
            "visual_content": "ğŸ–¼ï¸ Ù…Ø­ØªÙˆÙ‰ Ø¨ØµØ±ÙŠ",
            "unknown": "â“ Ù†ÙˆØ¹ Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        }
        
        analysis_parts.append(f"ğŸ” Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {content_analysis.get(content_type, content_type)}")
        
        return " â€¢ ".join(analysis_parts)
    
    def _generate_enhanced_recommendations(
        self, 
        overall_similarity: float,
        major_changes: bool,
        difference_detected: bool,
        probable_content_match: bool,
        content_type: str
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ù…Ø­Ø³Ù†Ø©"""
        
        recommendations = []
        
        if probable_content_match:
            recommendations.append("âœ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ·Ø§Ø¨Ù‚ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ - ÙŠÙ…ÙƒÙ† Ø§Ø¹ØªØ¨Ø§Ø±Ù‡Ù…Ø§ Ù†ÙØ³ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
            recommendations.append("ğŸ“‹ ÙŠÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ØªØºÙŠÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©")
        elif overall_similarity >= 75:
            recommendations.append("ğŸ” Ù‡Ù†Ø§Ùƒ ØªØ´Ø§Ø¨Ù‡ ÙƒØ¨ÙŠØ± Ù…Ø¹ ÙˆØ¬ÙˆØ¯ Ø§Ø®ØªÙ„Ø§ÙØ§Øª")
            recommendations.append("ğŸ“ ÙŠÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙØµÙŠÙ„ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø·Ø¨ÙŠØ¹Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª")
            if content_type == "educational_text":
                recommendations.append("ğŸ“š Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ØªØ­Ø¯ÙŠØ«Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ")
        elif major_changes:
            recommendations.append("âš ï¸ Ù‡Ù†Ø§Ùƒ Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ±ØªÙŠÙ†")
            recommendations.append("ğŸ”„ ÙŠÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
            recommendations.append("ğŸ“Š Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ù…ÙØµÙ„Ø©")
        else:
            recommendations.append("âŒ Ø§Ù„ØµÙˆØ± Ù…Ø®ØªÙ„ÙØ© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±")
            recommendations.append("ğŸ†• ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯ Ø£Ùˆ ØªØºÙŠÙŠØ±Ø§Øª Ø¬Ø°Ø±ÙŠØ©")
        
        return " â€¢ ".join(recommendations)
    
    def _generate_confidence_notes(
        self, 
        ssim_score: float,
        phash_score: float,
        feature_matching_score: float,
        overall_similarity: float
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ø«Ù‚Ø©"""
        
        confidence_notes = []
        
        # ØªØ­Ù„ÙŠÙ„ SSIM
        if ssim_score >= 0.9:
            confidence_notes.append("ğŸ¯ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ")
        elif ssim_score >= 0.7:
            confidence_notes.append("âœ… Ø«Ù‚Ø© Ø¬ÙŠØ¯Ø© ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ")
        elif ssim_score >= 0.5:
            confidence_notes.append("âš ï¸ Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ")
        else:
            confidence_notes.append("âŒ Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ")
        
        # ØªØ­Ù„ÙŠÙ„ pHash
        if phash_score >= 0.9:
            confidence_notes.append("ğŸ” ØªØ·Ø§Ø¨Ù‚ Ù‚ÙˆÙŠ ÙÙŠ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©")
        elif phash_score >= 0.7:
            confidence_notes.append("ğŸ‘ ØªØ·Ø§Ø¨Ù‚ Ø¬ÙŠØ¯ ÙÙŠ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©")
        else:
            confidence_notes.append("ğŸ” ØªØ·Ø§Ø¨Ù‚ Ø¶Ø¹ÙŠÙ ÙÙŠ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if feature_matching_score >= 0.8:
            confidence_notes.append("ğŸ² ØªØ·Ø§Ø¨Ù‚ Ù…Ù…ØªØ§Ø² ÙÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©")
        elif feature_matching_score >= 0.5:
            confidence_notes.append("ğŸ“Š ØªØ·Ø§Ø¨Ù‚ Ù…ØªÙˆØ³Ø· ÙÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©")
        else:
            confidence_notes.append("ğŸ“‰ ØªØ·Ø§Ø¨Ù‚ Ø¶Ø¹ÙŠÙ ÙÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©")
        
        # ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        if overall_similarity >= 90:
            confidence_notes.append("ğŸ’¯ Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        elif overall_similarity >= 75:
            confidence_notes.append("âœ… Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚Ø© Ø¬ÙŠØ¯ ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        elif overall_similarity >= 50:
            confidence_notes.append("âš ï¸ Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø· ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        else:
            confidence_notes.append("â“ Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶ - ÙŠÙ†ØµØ­ Ø¨Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©")
        
        return " â€¢ ".join(confidence_notes)


# Ø¥Ù†Ø´Ø§Ø¡ instance ÙˆØ§Ø­Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©
visual_comparison_service = EnhancedVisualComparisonService()
enhanced_visual_comparison_service = visual_comparison_service  # Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ 