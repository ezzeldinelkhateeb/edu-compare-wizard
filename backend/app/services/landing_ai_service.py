"""
ุฎุฏูุฉ LandingAI Agentic Document Extraction
LandingAI Agentic Document Extraction Service
"""

import os
import json
import asyncio
from typing import Dict, Any, List, Optional, Tuple, ContextManager
from datetime import datetime
from pathlib import Path
import tempfile
import shutil
import logging
import time
import requests
import signal
import platform
from contextlib import contextmanager
import random

# ุฅุถุงูุฉ ุงุณุชูุฑุงุฏ Tesseract OCR
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import numpy as np

try:
    import agentic_doc
    from agentic_doc.parse import parse
    # ุงุณุชูุฑุงุฏ ุงุฎุชูุงุฑู ููููููุงุช ุงูุฃุฎุฑู
    try:
        from agentic_doc.utils import viz_parsed_document
    except ImportError:
        viz_parsed_document = None
    try:
        from agentic_doc.config import VisualizationConfig
    except ImportError:
        VisualizationConfig = None
    # ChunkType ูุฏ ูุง ูููู ูุชุงุญ ูู ุจุนุถ ุงูุฅุตุฏุงุฑุงุช
    try:
        from agentic_doc.types import ChunkType
    except ImportError:
        ChunkType = None
    HAS_ADVANCED_FEATURES = True
    LANDINGAI_AVAILABLE = True
except ImportError as e:
    # Fallback for basic functionality
    HAS_ADVANCED_FEATURES = False
    viz_parsed_document = None
    VisualizationConfig = None
    ChunkType = None
    LANDINGAI_AVAILABLE = False
    print(f"โ Failed to import agentic_doc: {e}")
from pydantic import BaseModel, Field
from loguru import logger

from app.core.config import get_settings

settings = get_settings()

# API key should be set in environment variables
VISION_AGENT_API_KEY = settings.VISION_AGENT_API_KEY

# Setup logging
logger = logging.getLogger(__name__)

# Context manager ูููููุฉ ุงูุฒูููุฉ - ูุฎุตุต ูู Windows
import platform
from contextlib import contextmanager

@contextmanager  
def timeout_context(seconds):
    """Context manager ูููููุฉ ุงูุฒูููุฉ - ูุชูุงูู ูุน Windows ู Linux"""
    if platform.system() == "Windows":
        # ูู Windows ูุนุชูุฏ ุนูู ุงูููู ุงูุฏุงุฎููุฉ ููููุชุจุงุช
        try:
            yield
        except Exception as e:
            raise e
    else:
        # ูู Linux/Unix ูุณุชุฎุฏู signal
        def timeout_handler(signum, frame):
            raise TimeoutError(f"ุงูุนูููุฉ ุงุณุชุบุฑูุช ุฃูุซุฑ ูู {seconds} ุซุงููุฉ")
        
        # ุญูุธ ุงููุนุงูุฌ ุงูุณุงุจู
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(seconds)
        
        try:
            yield
        finally:
            # ุฅุนุงุฏุฉ ุชุนููู ุงูููุจู
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)

# ุฅุถุงูุฉ ุฏุงูุฉ ูุบููุฉ ููู parse ูุน ูุญุงููุงุช ูุชุนุฏุฏุฉ
def parse_with_retry(file_path, max_retries=7, base_timeout=120, backoff_factor=1.8):
    """
    ุฏุงูุฉ ูุบููุฉ ุญูู agentic_doc.parse ูุน ุฅุนุงุฏุฉ ุงููุญุงููุฉ ูุงููููุฉ ุงููุชุฒุงูุฏุฉ
    
    Args:
        file_path: ูุณุงุฑ ุงูููู ููุชุญููู
        max_retries: ุงูุญุฏ ุงูุฃูุตู ูุนุฏุฏ ุงููุญุงููุงุช
        base_timeout: ุงููููุฉ ุงูุฃุณุงุณูุฉ ุจุงูุซูุงูู
        backoff_factor: ุนุงูู ุงูุชุฑุงุฌุน ูููููุฉ ุจูู ุงููุญุงููุงุช
        
    Returns:
        ูุชูุฌุฉ ุงูุชุญููู ูู agentic_doc.parse
    """
    
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            # ุญุณุงุจ ุงููููุฉ ุงูุญุงููุฉ ูุน ุฅุถุงูุฉ ุนุดูุงุฆูุฉ ูุชุฌูุจ thundering herd
            current_timeout = int(base_timeout * (backoff_factor ** attempt))
            jitter = random.uniform(0.1, 0.3) * current_timeout
            timeout_with_jitter = current_timeout + jitter
            
            logger.info(f"๐ ูุญุงููุฉ {attempt + 1}/{max_retries} - ูููุฉ: {timeout_with_jitter:.1f}s")
            
            # ุชูููุฐ ุงูุชุญููู ูุน ุงููููุฉ
            with timeout_context(int(timeout_with_jitter)):
                result = parse(file_path)
                
                # ุงูุชุญูู ูู ุตุญุฉ ุงููุชูุฌุฉ
                if result and len(result) > 0:
                    logger.info(f"โ ูุฌุญ ุงูุชุญููู ูู ุงููุญุงููุฉ {attempt + 1}")
                    return result
                else:
                    raise Exception("ูุชูุฌุฉ ูุงุฑุบุฉ ูู agentic_doc.parse")
                    
        except TimeoutError as e:
            last_exception = e
            logger.warning(f"โฑ๏ธ ุงูุชูุช ูููุฉ ุงููุญุงููุฉ {attempt + 1}: {e}")
            
        except Exception as e:
            last_exception = e
            error_msg = str(e).lower()
            
            # ุชุตููู ุงูุฃุฎุทุงุก
            if any(keyword in error_msg for keyword in ['server disconnected', 'connection', 'timeout', 'network']):
                logger.warning(f"๐ ุฎุทุฃ ูู ุงูุงุชุตุงู ูู ุงููุญุงููุฉ {attempt + 1}: {e}")
            elif any(keyword in error_msg for keyword in ['down', 'invalid', 'unavailable']):
                logger.warning(f"๐ซ ุฎุทุฃ ูู ุงูุฎุงุฏู ูู ุงููุญุงููุฉ {attempt + 1}: {e}")
            elif any(keyword in error_msg for keyword in ['rate limit', 'quota', 'limit exceeded']):
                logger.warning(f"โก ุชุฌุงูุฒ ุงูุญุฏ ุงููุณููุญ ูู ุงููุญุงููุฉ {attempt + 1}: {e}")
                # ุงูุชุธุงุฑ ุฃุทูู ูู ุญุงูุฉ rate limiting
                time.sleep(30 + (attempt * 10))
            else:
                logger.warning(f"โ ุฎุทุฃ ุบูุฑ ูุชููุน ูู ุงููุญุงููุฉ {attempt + 1}: {e}")
        
        # ุงูุชุธุงุฑ ูุจู ุงููุญุงููุฉ ุงูุชุงููุฉ (ุฅูุง ูู ุงููุญุงููุฉ ุงูุฃุฎูุฑุฉ)
        if attempt < max_retries - 1:
            wait_time = min(30, 5 * (2 ** attempt)) + random.uniform(1, 3)
            logger.info(f"โณ ุงูุชุธุงุฑ {wait_time:.1f}s ูุจู ุงููุญุงููุฉ ุงูุชุงููุฉ...")
            time.sleep(wait_time)
    
    # ุฅุฐุง ูุดูุช ุฌููุน ุงููุญุงููุงุช
    logger.error(f"โ ูุดู ูู ุฌููุน ุงููุญุงููุงุช ({max_retries})")
    raise last_exception or Exception("ูุดู ูู ุฌููุน ูุญุงููุงุช ุงูุชุญููู")

class CurriculumAnalysis(BaseModel):
    """ูููุฐุฌ ุชุญููู ุงููููุฌ ุงูุชุนูููู"""
    subject: str = Field(description="ุงูููุถูุน ุฃู ุงููุงุฏุฉ ุงูุฏุฑุงุณูุฉ")
    grade_level: str = Field(description="ุงููุณุชูู ุงูุฏุฑุงุณู ุฃู ุงูุตู")
    chapter_title: str = Field(description="ุนููุงู ุงููุตู ุฃู ุงููุญุฏุฉ")
    learning_objectives: List[str] = Field(description="ุงูุฃูุฏุงู ุงูุชุนููููุฉ", default_factory=list)
    topics: List[str] = Field(description="ุงูููุงุถูุน ุงููุบุทุงุฉ", default_factory=list)
    key_concepts: List[str] = Field(description="ุงูููุงููู ุงูุฃุณุงุณูุฉ", default_factory=list)
    assessment_methods: List[str] = Field(description="ุทุฑู ุงูุชูููู", default_factory=list)
    exercises_count: int = Field(description="ุนุฏุฏ ุงูุชูุงุฑูู", default=0)
    difficulty_level: str = Field(description="ูุณุชูู ุงูุตุนูุจุฉ", default="ูุชูุณุท")
    page_count: int = Field(description="ุนุฏุฏ ุงูุตูุญุงุช", default=0)
    language: str = Field(description="ูุบุฉ ุงููุญุชูู", default="ุงูุนุฑุจูุฉ")
    content_type: str = Field(description="ููุน ุงููุญุชูู", default="ูุธุฑู")


class LandingAIExtractionResult(BaseModel):
    """ูุชูุฌุฉ ุงุณุชุฎุฑุงุฌ LandingAI"""
    file_path: str
    file_name: str
    processing_time: float
    success: bool
    error_message: Optional[str] = None
    
    # ูุญุชูู ูุณุชุฎุฑุฌ
    markdown_content: str = ""
    structured_analysis: Optional[CurriculumAnalysis] = None
    
    # ุชูุงุตูู ุงูุงุณุชุฎุฑุงุฌ
    total_chunks: int = 0
    chunks_by_type: Dict[str, int] = Field(default_factory=dict)
    confidence_score: float = 0.0
    
    # ูููุงุช ุงูุฅุฎุฑุงุฌ
    visual_groundings: List[str] = Field(default_factory=list)
    visualization_images: List[str] = Field(default_factory=list)
    raw_json_path: Optional[str] = None
    
    # ุฅุญุตุงุฆูุงุช
    text_elements: int = 0
    table_elements: int = 0
    image_elements: int = 0
    title_elements: int = 0


class LandingAIService:
    """ุฎุฏูุฉ LandingAI ููุงุณุชุฎุฑุงุฌ ุงูุฐูู ูููุณุชูุฏุงุช"""
    
    def __init__(self):
        self.api_key = VISION_AGENT_API_KEY
        # ุชูุญูุฏ ุญุงูุฉ ุงูุฎุฏูุฉ ูู ูุชุบูุฑ ูุงุญุฏ: mock_mode
        self.mock_mode = not (bool(self.api_key) and LANDINGAI_AVAILABLE)
        self.agentic_doc_available = LANDINGAI_AVAILABLE
        
        if self.mock_mode:
            logger.warning("LandingAI service is in MOCK MODE due to missing API key or library.")
        else:
            logger.info("โ ุชู ุชูููู LandingAI Service ูุน API ุญูููู.")

        self.api_endpoint = "https://predict.app.landing.ai/inference/v1/predict"
        if not LANDINGAI_AVAILABLE:
            logger.warning("agentic-doc library not installed. LandingAI service will be disabled.")
        if not self.api_key:
            logger.warning("VISION_AGENT_API_KEY not set. LandingAI service will be disabled.")
        
        self.batch_size = int(os.getenv("LANDINGAI_BATCH_SIZE", "4"))
        self.max_workers = int(os.getenv("LANDINGAI_MAX_WORKERS", "4"))  # ุฒูุงุฏุฉ ุงูุนูุงู
        self.max_retries = int(os.getenv("LANDINGAI_MAX_RETRIES", "1"))  # ุชูููู ุงููุญุงููุงุช ููุณุฑุนุฉ
        self.include_marginalia = os.getenv("LANDINGAI_INCLUDE_MARGINALIA", "False").lower() == "true"
        self.include_metadata = os.getenv("LANDINGAI_INCLUDE_METADATA", "True").lower() == "true"
        self.save_visual_groundings = os.getenv("LANDINGAI_SAVE_VISUAL_GROUNDINGS", "False").lower() == "true"  # ุชุนุทูู ูุชูููุฑ ุงูููุช
        
        # ุฅุนุฏุงุฏ Tesseract OCR ูุญุณู ููุณุฑุนุฉ - ููู ูู ูุณุชุฎุฏูู
        self.tesseract_path = os.getenv("TESSERACT_PATH", "tesseract")
        self.ocr_languages = os.getenv("OCR_LANGUAGES", "ara+eng")
        self.ocr_config = os.getenv("OCR_CONFIG", "--oem 3 --psm 6")  # PSM 6 ุฃุณุฑุน ูููุตูุต ุงูููุธูุฉ
        
        # ุญูุธ ุงููุต ุงููุณุชุฎุฑุฌ ุชููุงุฆูุงู
        self.auto_save_md = os.getenv("AUTO_SAVE_MD", "True").lower() == "true"
        
        # ุชุนุทูู Tesseract backup
        self.ocr_available = False
        
    def _downscale_if_needed(self, image_path: str, max_dim: int = 1024, temp_dir: str = None) -> Tuple[str, bool]:
        """
        ุชุตุบูุฑ ุงูุตูุฑ ุงููุจูุฑุฉ ููุญูุงุธ ุนูู ุณุฑุนุฉ ุงุณุชุฌุงุจุฉ LandingAI (ูุญุณู)
        Downscales large images to maintain LandingAI API performance (optimized).
        """
        try:
            img = Image.open(image_path)
            original_size = img.size
            
            # ุงุณุชุฎุฏู ุญุฏ ุฃูุตู ุฃุตุบุฑ ููุชุฃูุฏ ูู ุงููุฌุงุญ
            if max(img.size) > max_dim:
                logger.warning(f"๐ผ๏ธ ุงูุตูุฑุฉ {Path(image_path).name} ูุจูุฑุฉ {original_size}ุ ุณูุชู ุชุตุบูุฑูุง ุฅูู {max_dim}px")
                
                # ุญุณุงุจ ุงููุณุจุฉ ุงูุฌุฏูุฏุฉ
                ratio = max_dim / max(img.size)
                new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))
                
                # ุชุตุบูุฑ ุงูุตูุฑุฉ ูุน ุถุบุท ุฃูุถู
                img = img.resize(new_size, Image.Resampling.LANCZOS)
                
                # ุชุญููู ุฅูู RGB ุฅุฐุง ูุงู RGBA
                if img.mode in ('RGBA', 'LA', 'P'):
                    img = img.convert('RGB')
                
                # ุญูุธ ุงูุตูุฑุฉ ุงููุคูุชุฉ
                if temp_dir is None:
                    temp_dir = tempfile.gettempdir()
                
                new_path = Path(temp_dir) / f"downscaled_{Path(image_path).name}"
                # ุญูุธ ุจุฌูุฏุฉ ูุชูุณุทุฉ ููุญูุงุธ ุนูู ุณุฑุนุฉ ุงูุฑูุน
                img.save(new_path, quality=75, format='JPEG', optimize=True)
                
                logger.info(f"โ ุชู ุชุตุบูุฑ ุงูุตูุฑุฉ ูู {original_size} ุฅูู {new_size} ูุญูุธูุง ูู: {new_path}")
                return str(new_path), True
            
            return image_path, False
        except Exception as e:
            logger.error(f"โ ูุดู ุชุตุบูุฑ ุงูุตูุฑุฉ {image_path}: {e}")
            return image_path, False
    
    async def extract_from_file(
        self, 
        file_path: str, 
        output_dir: Optional[str] = None,
        job_id: Optional[str] = None
    ) -> LandingAIExtractionResult:
        """
        ุงุณุชุฎุฑุงุฌ ูุญุชูู ูู ููู ูุงุญุฏ.
        ุณูุชู ุงุณุชุฎุฏุงู ุงููุญุงูุงุฉ ุชููุงุฆูุงู ุฅุฐุง ูุงูุช ุงูุฎุฏูุฉ ูู mock_mode.
        """
        start_time = datetime.now()
        file_name = Path(file_path).name
        
        # ุชุญุฏูุฏ ุงููุฌูุฏุงุช
        if not output_dir:
            output_dir = os.path.join(settings.UPLOAD_DIR, "landingai_results")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_dir = os.path.join(output_dir, f"extraction_{timestamp}")
        os.makedirs(session_dir, exist_ok=True)
        
        # ุงูุชุญูู ูู ูุถุน ุงููุญุงูุงุฉ ุฃููุงู
        if self.mock_mode:
            logger.warning(f"โ๏ธ ุงุณุชุฏุนุงุก ูุญุงูุงุฉ ุงุณุชุฎุฑุงุฌ ูู {file_name}")
            return await self._mock_extraction(file_path, session_dir, file_name)

        logger.info(f"๐ ุจุฏุก ุงุณุชุฎุฑุงุฌ ุงููุต ุงูุญูููู ูู: {file_name}")
        
        try:
            # ุงุณุชุฎุฏุงู LandingAI API ุงูุญูููู
            logger.info("๐ ุจุฏุก ุงูุงุณุชุฎุฑุงุฌ ุจุงุณุชุฎุฏุงู Landing AI API...")
            result = await self._real_landingai_extraction(file_path, session_dir, file_name)
            
            if not result.success:
                raise Exception(f"ูุดู LandingAI: {result.error_message}")
            
            # ุญุณุงุจ ููุช ุงููุนุงูุฌุฉ
            processing_time = (datetime.now() - start_time).total_seconds()
            result.processing_time = processing_time
            
            logger.info(f"โ ุงูุชูู ุงุณุชุฎุฑุงุฌ {file_name} ูู {processing_time:.2f} ุซุงููุฉ (LandingAI)")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"โ ุฎุทุฃ ูู ุงุณุชุฎุฑุงุฌ {file_name}: {e}")
            
            # ุนุฏู ุงูุชุญูู ุฅูู Tesseract - ูุดู ูุจุงุดุฑ
            return LandingAIExtractionResult(
                file_path=file_path,
                file_name=file_name,
                processing_time=processing_time,
                success=False,
                error_message=f"ูุดู LandingAI: {str(e)}"
            )
    
    async def _preprocess_image_for_ocr(self, image: Image.Image) -> Image.Image:
        """ุชุญุณูู ุงูุตูุฑุฉ ููู OCR"""
        
        logger.debug("๐ง ุชุญุณูู ุงูุตูุฑุฉ...")
        
        # ุชุญููู ุฅูู numpy array ูููุนุงูุฌุฉ ุงููุชูุฏูุฉ
        img_array = np.array(image)
        
        # ุชุญููู ุฅูู ุฑูุงุฏู ุฅุฐุง ูุงูุช ููููุฉ
        if len(img_array.shape) == 3:
            img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            img_gray = img_array
        
        # ุชุญุณูู ุฌูุฏุฉ ุงูุตูุฑุฉ
        logger.debug("๐ ุชุญุณูู ุงูุชุจุงูู ูุงููุถูุญ...")
        
        # 1. ุชุทุจูู Gaussian blur ูุชูููู ุงูุถูุถุงุก
        img_blur = cv2.GaussianBlur(img_gray, (1, 1), 0)
        
        # 2. ุชุญุณูู ุงูุชุจุงูู ุจุงุณุชุฎุฏุงู CLAHE
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        img_enhanced = clahe.apply(img_blur)
        
        # 3. ุชุทุจูู threshold ููุญุตูู ุนูู ูุต ุฃูุถุญ
        # ุงุณุชุฎุฏุงู Otsu's thresholding ููุญุตูู ุนูู ุฃูุถู threshold ุชููุงุฆูุงู
        _, img_thresh = cv2.threshold(img_enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # 4. ุชูุธูู ุงูุตูุฑุฉ ูู ุงูุถูุถุงุก ุงูุตุบูุฑุฉ
        kernel = np.ones((1, 1), np.uint8)
        img_cleaned = cv2.morphologyEx(img_thresh, cv2.MORPH_CLOSE, kernel)
        
        # 5. ุชูุจูุฑ ุงูุตูุฑุฉ ูุชุญุณูู ุฏูุฉ OCR
        scale_factor = 2.0
        width = int(img_cleaned.shape[1] * scale_factor)
        height = int(img_cleaned.shape[0] * scale_factor)
        img_resized = cv2.resize(img_cleaned, (width, height), interpolation=cv2.INTER_CUBIC)
        
        # ุชุญููู ุงูุนูุฏุฉ ุฅูู PIL Image
        processed_image = Image.fromarray(img_resized)
        
        logger.debug(f"โ ุชู ุชุญุณูู ุงูุตูุฑุฉ: {processed_image.size}")
        
        return processed_image
    
    async def _analyze_educational_content(self, text: str) -> CurriculumAnalysis:
        """ุชุญููู ุงููุญุชูู ุงูุชุนูููู ุงููุณุชุฎุฑุฌ"""
        
        logger.debug("๐ ุชุญููู ุงููุญุชูู ุงูุชุนูููู...")
        
        # ุชุญููู ุจุณูุท ูููุญุชูู ุงูุนุฑุจู
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # ุงูุจุญุซ ุนู ุงูุนูุงููู ูุงูููุงุถูุน
        topics = []
        key_concepts = []
        exercises_count = 0
        
        for line in lines:
            # ุงูุจุญุซ ุนู ุงูุฃุณุฆูุฉ
            if any(word in line for word in ['ุงุฎุชุฑ', 'ุฃุฌุจ', 'ุงุญุณุจ', 'ูุถุญ', 'ุงุดุฑุญ', 'ูุงุฑู']):
                exercises_count += 1
            
            # ุงูุจุญุซ ุนู ุงูููุงููู ุงูููุฒูุงุฆูุฉ
            if any(word in line for word in ['ุงูููุจุณ', 'ุงูุถุบุท', 'ุงูููุฉ', 'ุงูููุฏุฑููููู', 'ุงููุณุจุฉ']):
                if line not in key_concepts and len(line) < 200:
                    key_concepts.append(line)
            
            # ุงูุจุญุซ ุนู ุงูููุงุถูุน
            if len(line) > 20 and len(line) < 100:
                topics.append(line)
        
        # ุชุญุฏูุฏ ุงูููุถูุน ุงูุฑุฆูุณู
        subject = "ุงูููุฒูุงุก"
        if any(word in text for word in ['ุงูููุจุณ', 'ุงูููุฏุฑููููู', 'ุงูุถุบุท']):
            chapter_title = "ุงูููุงุจุณ ุงูููุฏุฑูููููุฉ"
        else:
            chapter_title = "ููุถูุน ููุฒูุงุฆู"
        
        return CurriculumAnalysis(
            subject=subject,
            grade_level="ุงูุซุงููู",
            chapter_title=chapter_title,
            learning_objectives=[
                "ููู ูุจุฏุฃ ุนูู ุงูููุงุจุณ ุงูููุฏุฑูููููุฉ",
                "ุญุณุงุจ ุงููุณุจ ูุงูุถุบูุท ูู ุงูุฃูุธูุฉ ุงูููุฏุฑูููููุฉ",
                "ุชุทุจูู ูุจุฏุฃ ุจุงุณูุงู ูู ุงููุณุงุฆู ุงูุนูููุฉ"
            ],
            topics=topics[:5],  # ุฃูู 5 ููุงุถูุน
            key_concepts=key_concepts[:10],  # ุฃูู 10 ููุงููู
            assessment_methods=["ุฃุณุฆูุฉ ุงุฎุชูุงุฑ ูู ูุชุนุฏุฏ", "ูุณุงุฆู ุญุณุงุจูุฉ"],
            exercises_count=exercises_count,
            difficulty_level="ูุชูุณุท ุฅูู ูุชูุฏู",
            page_count=1,
            language="ุงูุนุฑุจูุฉ",
            content_type="ูุธุฑู ูุชุทุจููู"
        )
    
    async def _real_ocr_extraction(
        self, 
        file_path: str, 
        session_dir: str, 
        file_name: str
    ) -> LandingAIExtractionResult:
        """ุงูุงุณุชุฎุฑุงุฌ ุงูุญูููู ุจุงุณุชุฎุฏุงู Tesseract OCR"""
        
        logger.info("๐ ุจุฏุก ุนูููุฉ OCR ุงูุญููููุฉ...")
        
        try:
            # ุชุญููู ูุชุญุถูุฑ ุงูุตูุฑุฉ
            logger.debug("๐ธ ุชุญููู ุงูุตูุฑุฉ...")
            image = Image.open(file_path)
            
            # ูุนูููุงุช ุงูุตูุฑุฉ
            image_info = {
                "width": image.width,
                "height": image.height,
                "format": image.format,
                "mode": image.mode,
                "size_bytes": os.path.getsize(file_path)
            }
            logger.info(f"๐ ูุนูููุงุช ุงูุตูุฑุฉ: {image.width}x{image.height} ({image.format})")
            
            # ุชุญุณูู ุงูุตูุฑุฉ ููู OCR
            logger.debug("๐ง ุชุญุณูู ุงูุตูุฑุฉ ููู OCR...")
            processed_image = await self._preprocess_image_for_ocr(image)
            
            # ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงุณุชุฎุฏุงู Tesseract
            logger.info("๐ ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงุณุชุฎุฏุงู Tesseract...")
            
            ocr_start_time = datetime.now()
            
            # ุฅุนุฏุงุฏุงุช OCR ูุญุณูุฉ ูููุตูุต ุงูุนุฑุจูุฉ
            custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=ุฃุจุชุซุฌุญุฎุฏุฐุฑุฒุณุดุตุถุทุธุนุบููููููููููุกุขุฅุคุฆุฉู0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ()[]{}.,;:!?+-*/=<>%$#@&_|~`"\'\\/'
            
            # ูุญุงููุฉ ุงุณุชุฎุฑุงุฌ ุงููุต ูุน ุฅุนุฏุงุฏุงุช ูุญุณูุฉ ููุณุฑุนุฉ
            extraction_attempts = [
                # ูุญุงููุฉ 1: ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ ูุน PSM 6 (ุงูุฃุณุฑุน)
                {"lang": "ara+eng", "config": r"--oem 3 --psm 6"},
                # ูุญุงููุฉ 2: ุงูุนุฑุจูุฉ ููุท ูุน PSM 6 (ุฃุณุฑุน ูููุตูุต ุงูุนุฑุจูุฉ)
                {"lang": "ara", "config": r"--oem 3 --psm 6"}
            ]
            
            best_result = None
            best_confidence = 0
            
            for i, attempt in enumerate(extraction_attempts):
                try:
                    logger.debug(f"๐ ุงููุญุงููุฉ {i+1}: {attempt['lang']} - {attempt['config']}")
                    
                    # ุงุณุชุฎุฑุงุฌ ุงููุต ูุน ุชูุงุตูู ุงูุซูุฉ
                    data = pytesseract.image_to_data(
                        processed_image, 
                        lang=attempt['lang'], 
                        config=attempt['config'],
                        output_type=pytesseract.Output.DICT
                    )
                    
                    # ุญุณุงุจ ูุชูุณุท ุงูุซูุฉ
                    confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]
                    avg_confidence = sum(confidences) / len(confidences) if confidences else 0
                    
                    # ุงุณุชุฎุฑุงุฌ ุงููุต
                    text = pytesseract.image_to_string(
                        processed_image, 
                        lang=attempt['lang'], 
                        config=attempt['config']
                    )
                    
                    if avg_confidence > best_confidence and text.strip():
                        best_confidence = avg_confidence
                        best_result = {
                            "text": text,
                            "confidence": avg_confidence / 100,
                            "language": attempt['lang'],
                            "config": attempt['config'],
                            "data": data
                        }
                        logger.debug(f"โ ุฃูุถู ูุชูุฌุฉ ุญุชู ุงูุขู: ุซูุฉ {avg_confidence:.1f}%")
                    
                except Exception as e:
                    logger.debug(f"โ๏ธ ูุดู ูู ุงููุญุงููุฉ {i+1}: {e}")
                    continue
            
            if not best_result:
                raise Exception("ูุดู ูู ุฌููุน ูุญุงููุงุช ุงุณุชุฎุฑุงุฌ ุงููุต")
            
            ocr_time = (datetime.now() - ocr_start_time).total_seconds()
            logger.info(f"โฑ๏ธ ููุช OCR: {ocr_time:.2f} ุซุงููุฉ")
            
            # ุชูุธูู ุงููุต ุงููุณุชุฎุฑุฌ
            extracted_text = best_result["text"]
            confidence_score = best_result["confidence"]
            
            # ุฅุญุตุงุฆูุงุช ุงููุต
            character_count = len(extracted_text)
            word_count = len(extracted_text.split())
            
            logger.info(f"๐ ุงููุชุงุฆุฌ: {character_count} ุญุฑู, {word_count} ูููุฉ, ุซูุฉ: {confidence_score:.2f}")
            
            # ุญูุธ ุงููุชุงุฆุฌ
            logger.debug("๐พ ุญูุธ ุงููุชุงุฆุฌ...")
            
            # ุญูุธ ุงููุต ุงูุฎุงู
            raw_text_path = os.path.join(session_dir, f"{file_name}_raw_text.txt")
            with open(raw_text_path, 'w', encoding='utf-8') as f:
                f.write(extracted_text)
            
            # ุญูุธ ุงูุตูุฑุฉ ุงููุนุงูุฌุฉ
            processed_image_path = os.path.join(session_dir, f"{file_name}_processed.png")
            processed_image.save(processed_image_path)
            
            # ุญูุธ ุงููุต ุงููุณุชุฎุฑุฌ ูู .md ูู ููุณ ูุณุงุฑ ุงูุตูุฑุฉ ุงูุฃุตููุฉ
            if self.auto_save_md:
                try:
                    original_file_dir = os.path.dirname(file_path)
                    base_name = os.path.splitext(file_name)[0]
                    md_path = os.path.join(original_file_dir, f"{base_name}_extracted.md")
                    
                    with open(md_path, 'w', encoding='utf-8') as f:
                        f.write(f"# ุงููุต ุงููุณุชุฎุฑุฌ ูู {file_name}\n\n")
                        f.write(f"**ุชุงุฑูุฎ ุงูุงุณุชุฎุฑุงุฌ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                        f.write(f"**ูุณุชูู ุงูุซูุฉ:** {confidence_score:.1%}\n\n")
                        f.write(f"**ุนุฏุฏ ุงููููุงุช:** {word_count}\n\n")
                        f.write("---\n\n")
                        f.write("## ุงููุต ุงููุณุชุฎุฑุฌ\n\n")
                        f.write(extracted_text)
                    
                    logger.info(f"๐ ุชู ุญูุธ ุงููุต ุงููุณุชุฎุฑุฌ: {md_path}")
                except Exception as e:
                    logger.warning(f"โ๏ธ ูุดู ูู ุญูุธ ููู .md: {e}")
            
            # ุญูุธ ุชูุงุตูู OCR
            ocr_details_path = os.path.join(session_dir, f"{file_name}_ocr_details.json")
            with open(ocr_details_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "confidence_score": confidence_score,
                    "language_detected": best_result["language"],
                    "config_used": best_result["config"],
                    "character_count": character_count,
                    "word_count": word_count,
                    "processing_time": ocr_time,
                    "image_info": image_info,
                    "extraction_details": {
                        "total_attempts": len(extraction_attempts),
                        "successful_attempt": best_result["language"] + " - " + best_result["config"]
                    }
                }, f, ensure_ascii=False, indent=2)
            
            # ุชุญููู ุงููุญุชูู ุงูุชุนูููู
            logger.info("๐ง ุชุญููู ุงููุญุชูู ุงูุชุนูููู...")
            structured_analysis = await self._analyze_educational_content(extracted_text)
            
            # ุฅูุดุงุก ูุญุชูู Markdown ููุธู
            markdown_content = self._create_markdown_content(extracted_text, structured_analysis, best_result)
            
            logger.info("โ ุชู ุงูุงุณุชุฎุฑุงุฌ ุงูุญูููู ุจูุฌุงุญ")
            
            return LandingAIExtractionResult(
                file_path=file_path,
                file_name=file_name,
                processing_time=0,  # ุณูุชู ุชุญุฏูุซู ูู ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ
                success=True,
                markdown_content=markdown_content,
                structured_analysis=structured_analysis,
                confidence_score=confidence_score,
                text_elements=word_count,
                total_chunks=1,
                chunks_by_type={"text": 1},
                raw_json_path=ocr_details_path,
                visual_groundings=[processed_image_path]
            )
            
        except Exception as e:
            logger.error(f"โ ุฎุทุฃ ูู ุงูุงุณุชุฎุฑุงุฌ ุงูุญูููู: {e}")
            raise e
    
    def _create_markdown_content(self, text: str, analysis, ocr_result: dict) -> str:
        """ุฅูุดุงุก ูุญุชูู Markdown ููุธู"""
        
        markdown = f"""# ุชูุฑูุฑ ุงุณุชุฎุฑุงุฌ ุงููุต - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ูุนูููุงุช ุงูุงุณุชุฎุฑุงุฌ
- **ุงููุบุฉ ุงูููุชุดูุฉ**: {ocr_result['language']}
- **ูุณุชูู ุงูุซูุฉ**: {ocr_result['confidence']:.1%}
- **ุฅุนุฏุงุฏุงุช OCR**: {ocr_result['config']}
- **ุนุฏุฏ ุงููููุงุช**: {len(text.split())}
- **ุนุฏุฏ ุงูุฃุญุฑู**: {len(text)}

## ุชุญููู ุงููุญุชูู ุงูุชุนูููู
- **ุงูููุถูุน**: {analysis.subject}
- **ุงููุณุชูู**: {analysis.grade_level}
- **ุนููุงู ุงููุตู**: {analysis.chapter_title}
- **ูุณุชูู ุงูุตุนูุจุฉ**: {analysis.difficulty_level}
- **ููุน ุงููุญุชูู**: {analysis.content_type}

## ุงููุต ุงููุณุชุฎุฑุฌ

```
{text}
```

## ุงูููุงุญุธุงุช
- ุชู ุงุณุชุฎุฏุงู ุชูููุงุช ูุนุงูุฌุฉ ุงูุตูุฑ ุงููุชูุฏูุฉ ูุชุญุณูู ุฌูุฏุฉ ุงูุงุณุชุฎุฑุงุฌ
- ุชู ุชุทุจูู ุนุฏุฉ ูุญุงููุงุช ูุงุณุชุฎุฑุงุฌ ุงููุต ููุญุตูู ุนูู ุฃูุถู ูุชูุฌุฉ
- ุงููุต ูุฏ ูุญุชุงุฌ ุฅูู ูุฑุงุฌุนุฉ ูุฏููุฉ ูุถูุงู ุงูุฏูุฉ ุงููุงููุฉ
"""
        
        return markdown
    
    async def _real_landingai_extraction(
        self, 
        file_path: str, 
        session_dir: str, 
        file_name: str
    ) -> LandingAIExtractionResult:
        """
        ุงูุงุณุชุฎุฑุงุฌ ุงูุญูููู ุจุงุณุชุฎุฏุงู agentic_doc.parse ูุน ุชุตุบูุฑ ุงูุตูุฑ ุงููุญุณู ูุขููุฉ fallback
        Real extraction using agentic_doc.parse with optimized image downscaling and fallback mechanism.
        """
        logger.info("๐ ุจุฏุก ุนูููุฉ ุงูุงุณุชุฎุฑุงุฌ ุจุงุณุชุฎุฏุงู agentic_doc.parse...")
        
        start_time = datetime.now()
        
        temp_image_path = None
        temp_dir = os.path.join(session_dir, "temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        try:
            # ุชุตุบูุฑ ุงูุตูุฑุฉ ุฅุฐุง ูุงูุช ูุจูุฑุฉ (ุญุฏ ุฃูุตู 1024px ููุชุฃูุฏ ูู ุงููุฌุงุญ)
            processed_path, was_downscaled = self._downscale_if_needed(file_path, max_dim=1024, temp_dir=temp_dir)
            if was_downscaled:
                temp_image_path = processed_path

            # ุงุณุชุฎุฏุงู parse_with_retry ููุญุตูู ุนูู ุขููุฉ ุฅุนุงุฏุฉ ุงููุญุงููุฉ ุงููุญุณูุฉ
            try:
                logger.info("๐ ุจุฏุก ุงูุชุญููู ูุน ุขููุฉ ุฅุนุงุฏุฉ ุงููุญุงููุฉ ุงููุญุณูุฉ...")
                result = parse_with_retry(processed_path, max_retries=7, base_timeout=120, backoff_factor=1.8)
                
            except Exception as e:
                error_msg = str(e).lower()
                logger.warning(f"โ ูุดู ูู agentic_doc.parse: {e}")
                
                # ูุญุงููุฉ ูุน ุตูุฑุฉ ุฃุตุบุฑ ูุขููุฉ ุงุญุชูุงุทูุฉ
                if not any(keyword in error_msg for keyword in ['rate limit', 'quota', 'authentication']):
                    logger.info("๐ ูุญุงููุฉ ูุน ุตูุฑุฉ ุฃุตุบุฑ (512px) ูุขููุฉ ุงุญุชูุงุทูุฉ...")
                    try:
                        smaller_path, _ = self._downscale_if_needed(file_path, max_dim=512, temp_dir=temp_dir)
                        if smaller_path != processed_path:
                            if temp_image_path and os.path.exists(temp_image_path):
                                os.remove(temp_image_path)
                            temp_image_path = smaller_path
                            processed_path = smaller_path
                            result = parse_with_retry(processed_path, max_retries=5, base_timeout=90, backoff_factor=1.5)
                        else:
                            raise e
                    except Exception as e2:
                        logger.warning(f"โ ูุดู ุฃูุถุงู ูุน ุงูุตูุฑุฉ ุงูุฃุตุบุฑ: {e2}")
                        # ูุญุงููุฉ ูุน ุตูุฑุฉ ุฃุตุบุฑ ุฌุฏุงู (256px) ูุขููุฉ ุงุญุชูุงุทูุฉ ุฃุฎูุฑุฉ
                        try:
                            smallest_path, _ = self._downscale_if_needed(file_path, max_dim=256, temp_dir=temp_dir)
                            if smallest_path != processed_path:
                                if temp_image_path and os.path.exists(temp_image_path):
                                    os.remove(temp_image_path)
                                temp_image_path = smallest_path
                                processed_path = smallest_path
                                logger.info("๐ ูุญุงููุฉ ุฃุฎูุฑุฉ ูุน ุตูุฑุฉ ุตุบูุฑุฉ ุฌุฏุงู (256px)...")
                                result = parse_with_retry(processed_path, max_retries=3, base_timeout=60, backoff_factor=1.3)
                            else:
                                raise e
                        except Exception as e3:
                            logger.error(f"โ ูุดู ูู ุฌููุน ูุญุงููุงุช agentic_doc.parse: {e3}")
                            # ูุง ูุณุชุฎุฏู Tesseract ุชููุงุฆูุงู - ูุญุชุงุฌ ููุงููุฉ ุงููุณุชุฎุฏู
                            raise Exception(f"ูุดู ูู ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงุณุชุฎุฏุงู LandingAI. ูููู ุงููุญุงููุฉ ูุน OCR ุงูุชูููุฏู ุจุนุฏ ููุงููุฉ ุงููุณุชุฎุฏู: {e3}")
                else:
                    raise e
            
            # ุงูุชุญูู ูู ุงููุชูุฌุฉ
            if not result or len(result) == 0:
                raise Exception("ูู ูุชู ุงูุนุซูุฑ ุนูู ูุชุงุฆุฌ ูู agentic_doc.parse")
            
            # ุงูุญุตูู ุนูู ุฃูู ูุชูุฌุฉ
            doc_result = result[0]
            
            # ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช
            markdown_content = doc_result.markdown if hasattr(doc_result, 'markdown') else ""
            chunks = doc_result.chunks if hasattr(doc_result, 'chunks') else []
            
            # ุชุญููู ุงููุทุน ุฅูู ุดูู ูุงุจู ููุชุณูุณู (dict)
            serializable_chunks: List[dict] = []
            for c in chunks:
                if isinstance(c, dict):
                    serializable_chunks.append(c)
                elif hasattr(c, "dict") and callable(getattr(c, "dict")):
                    # ุฏุนู ููุงุฐุฌ Pydantic
                    serializable_chunks.append(c.dict())
                elif hasattr(c, "to_dict") and callable(getattr(c, "to_dict")):
                    serializable_chunks.append(c.to_dict())
                elif hasattr(c, "__dict__"):
                    serializable_chunks.append(vars(c))
                else:
                    serializable_chunks.append({"representation": str(c)})
            
            logger.info("โ ุชู ุงูุญุตูู ุนูู ุงุณุชุฌุงุจุฉ ูุงุฌุญุฉ ูู agentic_doc")
            
            # ุญูุธ ุงููุชุงุฆุฌ
            results_file = os.path.join(session_dir, f"agentic_doc_result.json")
            result_data = {
                "markdown": markdown_content,
                "chunks": serializable_chunks,
                "result_path": getattr(doc_result, 'result_path', None),
                "image_was_downscaled": was_downscaled,
                "processed_image_size": self._get_image_size(processed_path) if processed_path else None
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            markdown_file = os.path.join(session_dir, f"extracted_content.md")
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # ุชุญููู ุงููุชุงุฆุฌ
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # ุนุฏ ุงูุนูุงุตุฑ
            text_elements = len([c for c in serializable_chunks if c.get('chunk_type') == 'text'])
            table_elements = len([c for c in serializable_chunks if c.get('chunk_type') == 'table'])
            image_elements = len([c for c in serializable_chunks if c.get('chunk_type') == 'image'])
            title_elements = len([c for c in serializable_chunks if c.get('chunk_type') == 'title'])
            
            # ุญุณุงุจ ุงูุซูุฉ
            total_text_length = len(markdown_content)
            
            # ุชูุฏูุฑ ุงูุซูุฉ ุจูุงุกู ุนูู ุทูู ุงููุต ููุฌูุฏ chunks
            if total_text_length > 1000 and len(serializable_chunks) > 3:
                avg_confidence = 0.9
            elif total_text_length > 500 and len(serializable_chunks) > 1:
                avg_confidence = 0.85
            elif total_text_length > 100:
                avg_confidence = 0.8
            else:
                avg_confidence = 0.7
            
            # ุชุญููู ุงููุญุชูู ุงูุชุนูููู
            educational_analysis = await self._analyze_educational_content(markdown_content)
            
            logger.info(f"๐ ุงููุชุงุฆุฌ: {total_text_length} ุญุฑู, {len(serializable_chunks)} ูุทุนุฉ, ุซูุฉ: {avg_confidence:.2f}")
            logger.info(f"๐ ูุญุชูู ูุณุชุฎุฑุฌ: {markdown_content[:200]}..." if len(markdown_content) > 200 else f"๐ ูุญุชูู ูุณุชุฎุฑุฌ: {markdown_content}")
            logger.info(f"๐ ุชูุงุตูู ุงูุฃุฌุฒุงุก: ูุต={text_elements}, ุฌุฏุงูู={table_elements}, ุตูุฑ={image_elements}, ุนูุงููู={title_elements}")
            
            # ุฅุถุงูุฉ console.log ูููุฑููุช ุฅูุฏ
            print("๐ FRONTEND_LOG: LANDINGAI_EXTRACTION_SUCCESS")
            print("๐ FRONTEND_LOG: " + json.dumps({
                "file_name": file_name,
                "total_chars": total_text_length,
                "chunks_count": len(serializable_chunks),
                "confidence": avg_confidence,
                "content_preview": markdown_content[:500] if len(markdown_content) > 500 else markdown_content
            }, ensure_ascii=False))
            print("๐ FRONTEND_LOG: LANDINGAI_EXTRACTION_END")
            
            return LandingAIExtractionResult(
                file_path=file_path,
                file_name=file_name,
                processing_time=processing_time,
                success=True,
                markdown_content=markdown_content,
                structured_analysis=educational_analysis,
                total_chunks=len(serializable_chunks),
                chunks_by_type={
                    'text': text_elements,
                    'table': table_elements,
                    'image': image_elements,
                    'title': title_elements
                },
                confidence_score=avg_confidence,
                text_elements=text_elements,
                table_elements=table_elements,
                image_elements=image_elements,
                title_elements=title_elements,
                raw_json_path=results_file
            )
                        
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"โ ุฎุทุฃ ูู agentic_doc.parse: {e}")
            
            # ุฅุถุงูุฉ console.log ูููุฑููุช ุฅูุฏ ูู ุญุงูุฉ ุงููุดู
            print("๐ FRONTEND_LOG: LANDINGAI_EXTRACTION_FAILED")
            print("๐ FRONTEND_LOG: " + json.dumps({
                "file_name": file_name,
                "error": str(e),
                "processing_time": processing_time
            }, ensure_ascii=False))
            print("๐ FRONTEND_LOG: LANDINGAI_EXTRACTION_END")
            
            # ุฅูุดุงุก ูุชูุฌุฉ ูุงุดูุฉ ูุน ุฅุดุงุฑุฉ ุฅูู ุฅููุงููุฉ ุงุณุชุฎุฏุงู fallback
            return LandingAIExtractionResult(
                file_path=file_path,
                file_name=file_name,
                processing_time=processing_time,
                success=False,
                error_message=str(e),
                markdown_content="",
                structured_analysis=None,
                total_chunks=0,
                chunks_by_type={},
                confidence_score=0.0,
                text_elements=0,
                table_elements=0,
                image_elements=0,
                title_elements=0,
                raw_json_path=None
            )
        finally:
            # ุญุฐู ุงูุตูุฑุฉ ุงููุคูุชุฉ ุฅุฐุง ุชู ุฅูุดุงุคูุง
            if temp_image_path and os.path.exists(temp_image_path):
                try:
                    os.remove(temp_image_path)
                    logger.debug(f"๐๏ธ ุชู ุญุฐู ุงูุตูุฑุฉ ุงููุคูุชุฉ: {temp_image_path}")
                except OSError as e:
                    logger.error(f"โ ูุดู ุญุฐู ุงูุตูุฑุฉ ุงููุคูุชุฉ {temp_image_path}: {e}")
    
    def _get_image_size(self, image_path: str) -> dict:
        """ุงูุญุตูู ุนูู ุญุฌู ุงูุตูุฑุฉ"""
        try:
            with Image.open(image_path) as img:
                return {"width": img.size[0], "height": img.size[1]}
        except Exception:
            return {"width": 0, "height": 0}
    
    async def _fallback_ocr_extraction(
        self, 
        file_path: str, 
        session_dir: str, 
        file_name: str
    ) -> LandingAIExtractionResult:
        """
        ุขููุฉ ุงุญุชูุงุทูุฉ ูุงุณุชุฎุฑุงุฌ ุงููุต ุจุงุณุชุฎุฏุงู Tesseract OCR
        Fallback OCR extraction using Tesseract when LandingAI fails
        """
        logger.info(f"๐ ุจุฏุก ุงูุงุณุชุฎุฑุงุฌ ุงูุงุญุชูุงุทู ุจุงุณุชุฎุฏุงู Tesseract OCR ูู {file_name}")
        
        start_time = datetime.now()
        
        try:
            # ุชุญุณูู ุงูุตูุฑุฉ ููู OCR
            with Image.open(file_path) as img:
                # ุชุญููู ุฅูู RGB ุฅุฐุง ูุฒู ุงูุฃูุฑ
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # ุชุญุณูู ุงูุตูุฑุฉ ููู OCR
                enhanced_img = await self._preprocess_image_for_ocr(img)
                
                # ุญูุธ ุงูุตูุฑุฉ ุงููุญุณูุฉ ูุคูุชุงู
                temp_path = os.path.join(session_dir, "temp", f"enhanced_{file_name}")
                enhanced_img.save(temp_path, "JPEG", quality=95)
                
                # ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงุณุชุฎุฏุงู Tesseract
                logger.info("๐ ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงุณุชุฎุฏุงู Tesseract...")
                
                # ุฅุนุฏุงุฏุงุช Tesseract ููุบุฉ ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
                custom_config = r'--oem 3 --psm 6 -l ara+eng'
                
                try:
                    extracted_text = pytesseract.image_to_string(enhanced_img, config=custom_config)
                except Exception as e:
                    logger.warning(f"โ๏ธ ูุดู ูุน ุงููุบุฉ ุงูุนุฑุจูุฉุ ูุญุงููุฉ ูุน ุงูุฅูุฌููุฒูุฉ ููุท: {e}")
                    custom_config = r'--oem 3 --psm 6 -l eng'
                    extracted_text = pytesseract.image_to_string(enhanced_img, config=custom_config)
                
                # ุชูุธูู ุงููุต ุงููุณุชุฎุฑุฌ
                cleaned_text = self._clean_extracted_text(extracted_text)
                
                if not cleaned_text.strip():
                    logger.warning("โ๏ธ ูู ูุชู ุงุณุชุฎุฑุงุฌ ุฃู ูุต ูู ุงูุตูุฑุฉ")
                    cleaned_text = f"ูู ูุชู ุงูุนุซูุฑ ุนูู ูุต ูู ุงูุตูุฑุฉ {file_name}"
                
                # ุฅูุดุงุก ูุญุชูู markdown
                markdown_content = f"# {Path(file_name).stem}\n\n## ุงููุญุชูู ุงููุณุชุฎุฑุฌ (Tesseract OCR)\n\n{cleaned_text}"
                
                # ุชุญููู ุงููุญุชูู ุงูุชุนูููู
                educational_analysis = await self._analyze_educational_content(cleaned_text)
                
                # ุญูุธ ุงููุชุงุฆุฌ
                results_file = os.path.join(session_dir, f"tesseract_result.json")
                result_data = {
                    "markdown": markdown_content,
                    "raw_text": cleaned_text,
                    "extraction_method": "Tesseract OCR",
                    "image_enhanced": True
                }
                
                with open(results_file, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                markdown_file = os.path.join(session_dir, f"extracted_content.md")
                with open(markdown_file, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                
                # ุญุณุงุจ ุงูุฅุญุตุงุฆูุงุช
                processing_time = (datetime.now() - start_time).total_seconds()
                text_length = len(cleaned_text)
                
                # ุชูุฏูุฑ ุงูุซูุฉ ุจูุงุกู ุนูู ุทูู ุงููุต ูุฌูุฏุชู
                confidence = min(0.8, max(0.4, text_length / 1000)) if text_length > 50 else 0.3
                
                logger.info(f"โ ุงูุชูู ุงูุงุณุชุฎุฑุงุฌ ุงูุงุญุชูุงุทู ูู {processing_time:.2f}s")
                logger.info(f"๐ ุงููุต ุงููุณุชุฎุฑุฌ: {text_length} ุญุฑูุ ุซูุฉ: {confidence:.2f}")
                
                return LandingAIExtractionResult(
                    file_path=file_path,
                    file_name=file_name,
                    processing_time=processing_time,
                    success=True,
                    markdown_content=markdown_content,
                    structured_analysis=educational_analysis,
                    total_chunks=1,
                    chunks_by_type={'text': 1},
                    confidence_score=confidence,
                    text_elements=1,
                    table_elements=0,
                    image_elements=0,
                    title_elements=0,
                    raw_json_path=results_file
                )
                
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"โ ูุดู ูู ุงูุงุณุชุฎุฑุงุฌ ุงูุงุญุชูุงุทู: {e}")
            raise Exception(f"ูุดู ูู ุงูุงุณุชุฎุฑุงุฌ ุงูุงุญุชูุงุทู ุจุงุณุชุฎุฏุงู Tesseract: {e}")
    
    def _clean_extracted_text(self, text: str) -> str:
        """ุชูุธูู ุงููุต ุงููุณุชุฎุฑุฌ ูู OCR"""
        if not text:
            return ""
        
        # ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ุงููุชุนุฏุฏุฉ
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                cleaned_lines.append(line)
        
        # ุฏูุฌ ุงูุฃุณุทุฑ ูุน ููุงุตู ููุงุณุจุฉ
        cleaned_text = '\n'.join(cleaned_lines)
        
        # ุชูุธูู ุฅุถุงูู
        import re
        # ุฅุฒุงูุฉ ุงูุฃุญุฑู ุงูุบุฑูุจุฉ
        cleaned_text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF.,!?;:()\[\]{}"\'-]', '', cleaned_text)
        
        return cleaned_text

    async def _mock_extraction(
        self, 
        file_path: str, 
        session_dir: str, 
        file_name: str
    ) -> LandingAIExtractionResult:
        """ูุญุงูุงุฉ ุงูุงุณุชุฎุฑุงุฌ ููุชุทููุฑ"""
        
        logger.info(f"๐ญ ูุถุน ุงููุญุงูุงุฉ - ูุนุงูุฌุฉ {file_name}")
        
        # ูุญุงูุงุฉ ููุช ุงููุนุงูุฌุฉ
        await asyncio.sleep(2)
        
        # ูุญุชูู ุชุฌุฑูุจู
        mock_markdown = f"""# {Path(file_name).stem}

## ุงููุตู ุงูุฃูู: ููุฏูุฉ ูู ุงูุฑูุงุถูุงุช

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:
- ููู ููููู ุงูุฃุนุฏุงุฏ ุงูุทุจูุนูุฉ
- ุชุทุจูู ุงูุนูููุงุช ุงูุญุณุงุจูุฉ ุงูุฃุณุงุณูุฉ
- ุญู ุงููุณุงุฆู ุงูุฑูุงุถูุฉ ุงูุจุณูุทุฉ

### ุงููุญุชูู:
ูุชูุงูู ูุฐุง ุงููุตู ุงูููุงููู ุงูุฃุณุงุณูุฉ ูู ุงูุฑูุงุถูุงุช...

### ุงูุชูุงุฑูู:
1. ุงุญุณุจ ูุฌููุน ุงูุฃุนุฏุงุฏ ูู 1 ุฅูู 10
2. ุงูุฌุฏ ูุงุชุฌ 15 ร 7
3. ุญู ุงููุนุงุฏูุฉ ุณ + 5 = 12

### ุฌุฏูู ุงููุชุงุฆุฌ:
| ุงูุทุงูุจ | ุงูุฏุฑุฌุฉ | ุงูุชูุฏูุฑ |
|---------|---------|----------|
| ุฃุญูุฏ    | 85      | ุฌูุฏ ุฌุฏุงู |
| ูุงุทูุฉ   | 92      | ููุชุงุฒ    |
| ูุญูุฏ    | 78      | ุฌูุฏ      |
"""
        
        mock_analysis = CurriculumAnalysis(
            subject="ุงูุฑูุงุถูุงุช",
            grade_level="ุงูุตู ุงูุฃูู ุงูุซุงููู",
            chapter_title="ููุฏูุฉ ูู ุงูุฑูุงุถูุงุช",
            learning_objectives=["ููู ุงูุฃุนุฏุงุฏ", "ุงูุนูููุงุช ุงูุญุณุงุจูุฉ"],
            topics=["ุงูุฃุนุฏุงุฏ ุงูุทุจูุนูุฉ", "ุงูุนูููุงุช ุงูุฃุณุงุณูุฉ"],
            key_concepts=["ุงูุฌูุน", "ุงูุทุฑุญ", "ุงูุถุฑุจ", "ุงููุณูุฉ"],
            difficulty_level="ูุชูุณุท",
            content_type="ูุธุฑู ูุนููู"
        )
        mock_json_path = os.path.join(session_dir, f"{Path(file_name).stem}_mock.json")
        mock_data = {
            "markdown": mock_markdown,
            "structured_analysis": mock_analysis.dict(),
            "chunks": [
                {
                    "chunk_id": "title_1",
                    "chunk_type": "title",
                    "content": "ุงููุตู ุงูุฃูู: ููุฏูุฉ ูู ุงูุฑูุงุถูุงุช",
                    "page": 1
                },
                {
                    "chunk_id": "text_1", 
                    "chunk_type": "text",
                    "content": "ูุชูุงูู ูุฐุง ุงููุตู ุงูููุงููู ุงูุฃุณุงุณูุฉ ูู ุงูุฑูุงุถูุงุช...",
                    "page": 1
                },
                {
                    "chunk_id": "table_1",
                    "chunk_type": "table", 
                    "content": "ุฌุฏูู ุงููุชุงุฆุฌ",
                    "page": 1
                }
            ]
        }
        
        with open(mock_json_path, 'w', encoding='utf-8') as f:
            json.dump(mock_data, f, ensure_ascii=False, indent=2)
        
        return LandingAIExtractionResult(
            file_path=file_path,
            file_name=file_name,
            processing_time=0,
            success=True,
            markdown_content=mock_markdown,
            structured_analysis=mock_analysis,
            total_chunks=3,
            chunks_by_type={"title": 1, "text": 1, "table": 1},
            confidence_score=0.95,
            visual_groundings=[],
            visualization_images=[],
            raw_json_path=mock_json_path,
            text_elements=1,
            table_elements=1,
            image_elements=0,
            title_elements=1
        )
    
    async def extract_from_multiple_files(
        self, 
        file_paths: List[str], 
        output_dir: Optional[str] = None,
        job_id: Optional[str] = None
    ) -> List[LandingAIExtractionResult]:
        """
        ุงุณุชุฎุฑุงุฌ ูุญุชูู ูู ูููุงุช ูุชุนุฏุฏุฉ
        Extract content from multiple files
        """
        logger.info(f"๐ ุจุฏุก ุงุณุชุฎุฑุงุฌ ูู {len(file_paths)} ููู")
        
        results = []
        for i, file_path in enumerate(file_paths):
            try:
                logger.info(f"๐ ูุนุงูุฌุฉ ุงูููู {i+1}/{len(file_paths)}: {Path(file_path).name}")
                result = await self.extract_from_file(file_path, output_dir, job_id)
                results.append(result)
                
                # ุชุฃุฎูุฑ ูุตูุฑ ูุชุฌูุจ Rate Limiting
                if not self.mock_mode and i < len(file_paths) - 1:
                    await asyncio.sleep(1)
                    
            except Exception as e:
                logger.error(f"โ ุฎุทุฃ ูู ูุนุงูุฌุฉ {file_path}: {e}")
                results.append(LandingAIExtractionResult(
                    file_path=file_path,
                    file_name=Path(file_path).name,
                    processing_time=0,
                    success=False,
                    error_message=str(e)
                ))
        
        successful = len([r for r in results if r.success])
        logger.info(f"โ ุงูุชูู ุงุณุชุฎุฑุงุฌ {successful}/{len(file_paths)} ููู")
        
        return results
    
    def get_supported_formats(self) -> List[str]:
        """ุงูุญุตูู ุนูู ุงูุตูุบ ุงููุฏุนููุฉ"""
        return ["pdf", "png", "jpg", "jpeg", "tiff", "bmp", "webp"]
    
    def validate_file(self, file_path: str) -> bool:
        """ุงูุชุญูู ูู ุตุญุฉ ุงูููู"""
        if not os.path.exists(file_path):
            return False
        
        file_ext = Path(file_path).suffix.lower().lstrip('.')
        return file_ext in self.get_supported_formats()
    
    async def health_check(self) -> Dict[str, Any]:
        """ูุญุต ุตุญุฉ ุงูุฎุฏูุฉ"""
        try:
            if self.mock_mode:
                return {
                    "status": "healthy",
                    "mode": "mock",
                    "message": "LandingAI Service ูู ูุถุน ุงููุญุงูุงุฉ",
                    "api_key_configured": False
                }
            
            # ุงุฎุชุจุงุฑ ุจุณูุท ููู API
            # ูููู ุฅุถุงูุฉ ุงุฎุชุจุงุฑ ุญูููู ููุง
            return {
                "status": "healthy", 
                "mode": "production",
                "message": "LandingAI Service ุฌุงูุฒ",
                "api_key_configured": True,
                "batch_size": self.batch_size,
                "max_workers": self.max_workers
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "message": f"ุฎุทุฃ ูู LandingAI Service: {str(e)}",
                "error": str(e)
            }

    def is_enabled(self) -> bool:
        """ููุชุญูู ูู ุฃู ุงูุฎุฏูุฉ ููุณุช ูู ูุถุน ุงููุญุงูุงุฉ"""
        return not self.mock_mode


# ุฅูุดุงุก instance ูุงุญุฏ ููุฎุฏูุฉ
landing_ai_service = LandingAIService() 