"""
Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø§Ù„Ø°ÙƒÙŠ
Smart Progressive Analysis Service

ÙÙƒØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ù…:
1. ØªØ­Ù„ÙŠÙ„ Ø¨ØµØ±ÙŠ Ø³Ø±ÙŠØ¹ Ù…Ø­Ù„ÙŠ Ø£ÙˆÙ„Ø§Ù‹
2. Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ ØªØ£ÙƒÙŠØ¯ -> LandingAI Ù„Ù„Ù†Øµ ÙÙ‚Ø·
3. Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ -> Gemini Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ

ØªÙˆÙÙŠØ±: 80%+ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… API
"""

import os
import cv2
import numpy as np
from typing import Dict, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
import hashlib
from PIL import Image, ImageChops
from skimage.metrics import structural_similarity as ssim
import asyncio
from loguru import logger

from app.services.gemini_service import GeminiService
from app.core.config import get_settings

settings = get_settings()


class SmartComparisonService:
    """Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self):
        self.gemini_service = GeminiService()
        self.cache_dir = Path(settings.UPLOAD_DIR) / "smart_cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # Ø¹ØªØ¨Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠØ©
        self.VISUAL_SIMILARITY_THRESHOLD = 85  # Ø¥Ø°Ø§ Ø£Ù‚Ù„ Ù…Ù† 85% -> Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ
        self.TEXT_SIMILARITY_THRESHOLD = 75    # Ø¥Ø°Ø§ Ø£Ù‚Ù„ Ù…Ù† 75% -> Ø§Ø³ØªØ®Ø¯Ù… Gemini
        self.HIGH_CONFIDENCE_THRESHOLD = 95    # Ø¥Ø°Ø§ Ø£ÙƒØ«Ø± Ù…Ù† 95% -> ØªÙˆÙ‚Ù
        
        logger.info("âœ… ØªÙ… ØªÙƒÙˆÙŠÙ† Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø§Ù„Ø°ÙƒÙŠ")
    
    async def smart_compare_images(
        self, 
        old_image_path: str, 
        new_image_path: str,
        session_id: str = None
    ) -> Dict[str, Any]:
        """Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ© Ù„Ù„ØµÙˆØ±"""
        
        start_time = datetime.now()
        result = {
            "session_id": session_id or f"smart_{int(datetime.now().timestamp())}",
            "stages_used": [],
            "api_calls_saved": 0,
            "total_processing_time": 0,
            "confidence_level": "low"
        }
        
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø¬Ù„Ø³Ø©: {result['session_id']}")
        
        try:
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ (Ù…Ø­Ù„ÙŠ)
            logger.info("ğŸ“Š Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹...")
            visual_result = await self._stage1_visual_analysis(old_image_path, new_image_path)
            result["stages_used"].append("visual_analysis")
            result["visual_similarity"] = visual_result["similarity"]
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ -> ØªÙˆÙ‚Ù Ù‡Ù†Ø§
            if visual_result["similarity"] >= self.HIGH_CONFIDENCE_THRESHOLD:
                logger.info(f"âœ… ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù„ÙŠ ({visual_result['similarity']:.1f}%) - ØªÙˆÙ‚Ù ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1")
                result["confidence_level"] = "very_high"
                result["api_calls_saved"] = 2  # ÙˆÙØ±Ù†Ø§ LandingAI + Gemini
                result["final_decision"] = "Ù…ØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹ - ØªØºÙŠÙŠØ±Ø§Øª ØªØµÙ…ÙŠÙ…ÙŠØ© ÙÙ‚Ø·"
                result["similarity_percentage"] = visual_result["similarity"]
                result["recommendation"] = "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØºÙŠÙŠØ±Ø§Øª Ø¬ÙˆÙ‡Ø±ÙŠØ©"
                return result
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…ØªÙˆØ³Ø· -> Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2
            if visual_result["similarity"] >= self.VISUAL_SIMILARITY_THRESHOLD:
                logger.info(f"ğŸ“ ØªØ´Ø§Ø¨Ù‡ Ù…ØªÙˆØ³Ø· ({visual_result['similarity']:.1f}%) - Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ")
                text_result = await self._stage2_text_extraction(old_image_path, new_image_path)
                result["stages_used"].append("text_extraction")
                result["api_calls_saved"] = 1  # ÙˆÙØ±Ù†Ø§ Gemini
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†ØµÙŠ
                if text_result["success"]:
                    text_similarity = await self._calculate_text_similarity(
                        text_result["old_text"], 
                        text_result["new_text"]
                    )
                    
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù…ØªØ´Ø§Ø¨Ù‡ Ø¬Ø¯Ø§Ù‹ -> ØªÙˆÙ‚Ù Ù‡Ù†Ø§
                    if text_similarity >= self.TEXT_SIMILARITY_THRESHOLD:
                        logger.info(f"âœ… Ù†Øµ Ù…ØªØ´Ø§Ø¨Ù‡ ({text_similarity:.1f}%) - ØªÙˆÙ‚Ù ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2")
                        result["confidence_level"] = "high"
                        result["text_similarity"] = text_similarity
                        result["final_decision"] = "Ù…ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ ØªØ­Ø¯ÙŠØ«Ø§Øª Ø·ÙÙŠÙØ©"
                        result["similarity_percentage"] = (visual_result["similarity"] + text_similarity) / 2
                        result["recommendation"] = "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª"
                        return result
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù…Ø®ØªÙ„Ù -> Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3
                logger.info(f"ğŸ¤– Ù†Øµ Ù…Ø®ØªÙ„Ù ({text_similarity:.1f}%) - Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚")
                gemini_result = await self._stage3_deep_analysis(
                    text_result["old_text"], 
                    text_result["new_text"]
                )
                result["stages_used"].append("deep_analysis")
                result["api_calls_saved"] = 0
                result["gemini_analysis"] = gemini_result
                result["confidence_level"] = "very_high"
                result["text_similarity"] = text_similarity
                result["similarity_percentage"] = gemini_result.similarity_percentage
                result["final_decision"] = gemini_result.summary
                result["recommendation"] = gemini_result.recommendation
                
            else:
                # ØªØ´Ø§Ø¨Ù‡ Ù…Ù†Ø®ÙØ¶ -> Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ù…Ø±Ø­Ù„Ø© 3
                logger.info(f"ğŸš¨ ØªØ´Ø§Ø¨Ù‡ Ù…Ù†Ø®ÙØ¶ ({visual_result['similarity']:.1f}%) - Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚")
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹
                text_result = await self._stage2_text_extraction(old_image_path, new_image_path)
                result["stages_used"].extend(["text_extraction", "deep_analysis"])
                
                if text_result["success"]:
                    gemini_result = await self._stage3_deep_analysis(
                        text_result["old_text"], 
                        text_result["new_text"]
                    )
                    result["gemini_analysis"] = gemini_result
                    result["confidence_level"] = "very_high"
                    result["similarity_percentage"] = gemini_result.similarity_percentage
                    result["final_decision"] = gemini_result.summary
                    result["recommendation"] = gemini_result.recommendation
                else:
                    result["confidence_level"] = "low"
                    result["similarity_percentage"] = visual_result["similarity"]
                    result["final_decision"] = "ØªØºÙŠÙŠØ±Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ø­ØªÙ…Ù„Ø© - ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©"
                    result["recommendation"] = "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø·Ù„ÙˆØ¨Ø©"
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ: {e}")
            result["error"] = str(e)
            result["confidence_level"] = "error"
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        result["total_processing_time"] = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"ğŸ¯ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {len(result['stages_used'])} Ù…Ø±Ø§Ø­Ù„ØŒ ÙˆÙØ± {result['api_calls_saved']} API calls")
        
        return result
    
    async def _stage1_visual_analysis(self, old_path: str, new_path: str) -> Dict[str, Any]:
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ Ø§Ù„Ù…Ø­Ù„ÙŠ"""
        
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±
            old_img = cv2.imread(old_path)
            new_img = cv2.imread(new_path)
            
            if old_img is None or new_img is None:
                raise ValueError("ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø¥Ø­Ø¯Ù‰ Ø§Ù„ØµÙˆØ±")
            
            # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø±Ù…Ø§Ø¯ÙŠ
            old_gray = cv2.cvtColor(old_img, cv2.COLOR_BGR2GRAY)
            new_gray = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)
            
            # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø¬Ø§Ù…
            height, width = min(old_gray.shape[0], new_gray.shape[0]), min(old_gray.shape[1], new_gray.shape[1])
            old_resized = cv2.resize(old_gray, (width, height))
            new_resized = cv2.resize(new_gray, (width, height))
            
            # Ø­Ø³Ø§Ø¨ SSIM (Structural Similarity Index)
            ssim_score = ssim(old_resized, new_resized)
            
            # Ø­Ø³Ø§Ø¨ Hash Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
            old_hash = self._calculate_image_hash(old_resized)
            new_hash = self._calculate_image_hash(new_resized)
            hash_similarity = self._compare_hashes(old_hash, new_hash)
            
            # Ø­Ø³Ø§Ø¨ Histogram Ù„Ù„Ø£Ù„ÙˆØ§Ù†
            hist_similarity = self._calculate_histogram_similarity(old_img, new_img)
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¬Ø­ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
            visual_similarity = (
                ssim_score * 0.5 +           # 50% Ù„Ù„ØªØ±ÙƒÙŠØ¨
                hash_similarity * 0.3 +      # 30% Ù„Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¹Ø§Ù…
                hist_similarity * 0.2        # 20% Ù„Ù„Ø£Ù„ÙˆØ§Ù†
            ) * 100
            
            logger.debug(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨ØµØ±ÙŠØ©: SSIM={ssim_score:.3f}, Hash={hash_similarity:.3f}, Hist={hist_similarity:.3f}")
            
            return {
                "similarity": round(visual_similarity, 1),
                "ssim_score": round(ssim_score * 100, 1),
                "hash_similarity": round(hash_similarity * 100, 1),
                "histogram_similarity": round(hist_similarity * 100, 1),
                "processing_time": 0.1  # Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ: {e}")
            return {"similarity": 50.0, "error": str(e)}
    
    def _calculate_image_hash(self, image: np.ndarray) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©"""
        # ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ù„Ù€ 8x8
        small = cv2.resize(image, (8, 8))
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©
        avg = small.mean()
        # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ binary
        binary = small > avg
        # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ hash
        return hashlib.md5(binary.tobytes()).hexdigest()
    
    def _compare_hashes(self, hash1: str, hash2: str) -> float:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù€ hashes"""
        if hash1 == hash2:
            return 1.0
        
        # Ø­Ø³Ø§Ø¨ Hamming distance
        diff = sum(c1 != c2 for c1, c2 in zip(hash1, hash2))
        similarity = 1 - (diff / len(hash1))
        return max(similarity, 0.0)
    
    def _calculate_histogram_similarity(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Histogram"""
        # Ø­Ø³Ø§Ø¨ histograms
        hist1 = cv2.calcHist([img1], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
        hist2 = cv2.calcHist([img2], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
        
        # normalization
        cv2.normalize(hist1, hist1)
        cv2.normalize(hist2, hist2)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… correlation
        correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        return max(correlation, 0.0)
    
    async def _stage2_text_extraction(self, old_path: str, new_path: str) -> Dict[str, Any]:
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙƒÙŠ"""
        
        logger.info("ğŸ“ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LandingAI...")
        
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±ØªÙŠÙ†
            # TODO: Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LandingAI Ù‡Ù†Ø§
            # Ù„Ù„Ø¢Ù† Ù†Ø­Ø§ÙƒÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
            await asyncio.sleep(1)  # Ù…Ø­Ø§ÙƒØ§Ø© ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            
            # Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ (Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù€ LandingAI Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ)
            old_text = "Ù‚Ø§Ø¹Ø¯Ø© Ø¨Ø§Ø³ÙƒØ§Ù„: Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¤Ø«Ø± Ø¶ØºØ· Ø¹Ù„Ù‰ Ø³Ø§Ø¦Ù„ Ù…Ø­Ø¨ÙˆØ³ ÙÙŠ Ø¥Ù†Ø§Ø¡..."
            new_text = "Ù‚Ø§Ø¹Ø¯Ø© Ø¨Ø§Ø³ÙƒØ§Ù„: Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¤Ø«Ø± Ø¶ØºØ· Ø¹Ù„Ù‰ Ø³Ø§Ø¦Ù„ Ù…Ø­Ø¨ÙˆØ³ ÙÙŠ Ø¥Ù†Ø§Ø¡..."
            
            return {
                "success": True,
                "old_text": old_text,
                "new_text": new_text,
                "processing_time": 1.0
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _calculate_text_similarity(self, old_text: str, new_text: str) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†Øµ Ø¨Ø³Ø±Ø¹Ø©"""
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ù† GeminiService
        enhanced_similarity = self.gemini_service._calculate_enhanced_similarity(
            old_text, new_text, 
            self.gemini_service.gemini_service._calculate_enhanced_similarity(old_text, new_text, 0)
        )
        
        return enhanced_similarity
    
    async def _stage3_deep_analysis(self, old_text: str, new_text: str) -> Any:
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini"""
        
        logger.info("ğŸ¤– Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini AI...")
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ØªÙˆÙƒÙ†Ø²
        cleaned_old = self._extract_essential_content(old_text)
        cleaned_new = self._extract_essential_content(new_text)
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Gemini Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù
        result = await self.gemini_service.compare_texts(cleaned_old, cleaned_new)
        
        return result
    
    def _extract_essential_content(self, text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ØªÙˆÙƒÙ†Ø²"""
        
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙˆØµÙ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ Ù„Ù€ LandingAI
        lines = text.split('\n')
        essential_lines = []
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ Ù†Ø­ØªÙØ¸ Ø¨Ù‡Ø§
        important_keywords = [
            'Ù‚Ø§Ø¹Ø¯Ø©', 'Ù…Ø¨Ø¯Ø£', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù†Ø¸Ø±ÙŠØ©', 'ØªØ¹Ø±ÙŠÙ', 'Ø¨Ø§Ø³ÙƒØ§Ù„', 'Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ',
            'Ø¶ØºØ·', 'Ø³Ø§Ø¦Ù„', 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª', 'Ù…ÙƒØ¨Ø³', 'ÙØ±Ø§Ù…Ù„', 'Ø±Ø§ÙØ¹Ø©', 'Ø­ÙØ§Ø±',
            'Ù…Ù„Ø§Ø­Ø¸Ø©', 'Ø£Ø³Ø¦Ù„Ø©', 'ØªÙ…Ø§Ø±ÙŠÙ†', 'Ø£Ù…Ø«Ù„Ø©', 'Ø´Ø±Ø­'
        ]
        
        for line in lines:
            line = line.strip()
            
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£ÙˆØµØ§Ù Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…ÙØµÙ„Ø©
            if any(skip_word in line.lower() for skip_word in [
                'scene overview', 'technical details', 'spatial relationships', 
                'analysis', 'photo', 'image', 'figure', 'illustration'
            ]):
                continue
            
            # Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù‡Ù…Ø©
            if any(keyword in line for keyword in important_keywords) or len(line) > 20:
                essential_lines.append(line)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù
        cleaned_text = '\n'.join(essential_lines)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ Ø§Ù‚ØªØ·Ø¹ Ø¨Ø°ÙƒØ§Ø¡
        if len(cleaned_text) > 1000:
            sentences = cleaned_text.split('.')
            important_sentences = []
            total_length = 0
            
            for sentence in sentences:
                if total_length + len(sentence) < 800:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ø¢Ù…Ù†
                    important_sentences.append(sentence)
                    total_length += len(sentence)
                else:
                    break
            
            cleaned_text = '.'.join(important_sentences)
        
        logger.debug(f"ğŸ“ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ: {len(text)} -> {len(cleaned_text)} Ø­Ø±Ù")
        
        return cleaned_text
    
    async def get_analysis_statistics(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ØªÙˆÙÙŠØ± API"""
        
        # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø­ÙØ¸ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        return {
            "total_analyses": 0,
            "api_calls_saved": 0,
            "efficiency_percentage": 0,
            "average_processing_time": 0
        }


# Ø¥Ù†Ø´Ø§Ø¡ instance ÙˆØ§Ø­Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©
smart_comparison_service = SmartComparisonService() 