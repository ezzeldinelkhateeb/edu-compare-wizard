"""
Ù…Ù‡Ø§Ù… Celery Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
Optimized Celery Tasks for Fast Parallel Processing
"""

from celery import group, chain, chord
from celery_app.worker import celery_app
from loguru import logger
import asyncio
import os
import sys
from datetime import datetime
from typing import Dict, Any, List, Tuple
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.config import get_settings
from app.services.landing_ai_service import LandingAIService
from app.services.visual_comparison_service import EnhancedVisualComparisonService

settings = get_settings()

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
MAX_WORKERS = 6  # Ø¹Ø¯Ø¯ Ø§Ù„Ù€ workers Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
BATCH_SIZE = 2   # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ù„ÙƒÙ„ batch


@celery_app.task(bind=True)
def parallel_extract_text_batch(self, image_paths: List[str], job_id: str = None) -> List[Dict[str, Any]]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
    Extract text from multiple images in parallel
    """
    try:
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ù† {len(image_paths)} ØµÙˆØ±Ø©")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø¯Ù…Ø© LandingAI
        landing_service = LandingAIService()
        
        results = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ThreadPoolExecutor Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ø­Ù‚ÙŠÙ‚ÙŠØ©
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Ø¥Ø±Ø³Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ù„Ù„Ù€ executor
            future_to_path = {
                executor.submit(self._extract_single_text, landing_service, path, job_id): path 
                for path in image_paths
            }
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            path_to_result = {}
            completed = 0
            
            for future in as_completed(future_to_path):
                path = future_to_path[future]
                completed += 1
                
                try:
                    result = future.result()
                    path_to_result[path] = result
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
                    progress = int((completed / len(image_paths)) * 100)
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'progress': progress,
                            'message': f'ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† {completed}/{len(image_paths)} ØµÙˆØ±Ø©',
                            'current_file': os.path.basename(path)
                        }
                    )
                    
                    logger.info(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù†: {os.path.basename(path)}")
                    
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† {path}: {e}")
                    path_to_result[path] = {
                        "success": False,
                        "error": str(e),
                        "text": "",
                        "confidence": 0,
                        "processing_time": 0,
                        "service": "LandingAI_Error"
                    }
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            results = [path_to_result[path] for path in image_paths]
        
        logger.info(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ù† {len(image_paths)} ØµÙˆØ±Ø©")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ: {e}")
        raise

    def _extract_single_text(self, landing_service: LandingAIService, image_path: str, job_id: str = None) -> Dict[str, Any]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© (ÙŠØªÙ… ØªØ´ØºÙŠÙ„Ù‡Ø§ ÙÙŠ thread Ù…Ù†ÙØµÙ„)
        Extract text from a single image (runs in separate thread)
        """
        try:
            # ØªØ´ØºÙŠÙ„ async function ÙÙŠ thread pool
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                extraction_result = loop.run_until_complete(
                    landing_service.extract_from_file(image_path, job_id=job_id)
                )
                
                if not extraction_result.success:
                    raise Exception(f"ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ LandingAI: {extraction_result.error_message}")
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                result = {
                    "success": True,
                    "text": extraction_result.markdown_content,
                    "confidence": extraction_result.confidence_score,
                    "word_count": len(extraction_result.markdown_content.split()),
                    "processing_time": extraction_result.processing_time,
                    "service": "LandingAI_Parallel",
                    "image_path": image_path,
                    "structured_analysis": extraction_result.structured_analysis.dict() if extraction_result.structured_analysis else None,
                    "total_chunks": extraction_result.total_chunks,
                    "chunks_by_type": extraction_result.chunks_by_type
                }
                
                return result
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† {image_path}: {e}")
            return {
                "success": False,
                "error": str(e),
                "text": "",
                "confidence": 0,
                "processing_time": 0,
                "service": "LandingAI_Error",
                "image_path": image_path
            }


@celery_app.task(bind=True)
def parallel_visual_comparison_batch(self, image_pairs: List[Tuple[str, str]], job_id: str = None) -> List[Dict[str, Any]]:
    """
    Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ØµØ±ÙŠØ© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø²ÙˆØ§Ø¬ Ù…Ù† Ø§Ù„ØµÙˆØ±
    Parallel visual comparison for multiple image pairs
    """
    try:
        logger.info(f"ğŸ–¼ï¸ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù€ {len(image_pairs)} Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„ØµÙˆØ±")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©
        visual_service = EnhancedVisualComparisonService()
        
        results = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ThreadPoolExecutor Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Ø¥Ø±Ø³Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ù„Ù„Ù€ executor
            future_to_pair = {
                executor.submit(self._compare_single_pair, visual_service, old_path, new_path, job_id): (old_path, new_path)
                for old_path, new_path in image_pairs
            }
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            pair_to_result = {}
            completed = 0
            
            for future in as_completed(future_to_pair):
                pair = future_to_pair[future]
                completed += 1
                
                try:
                    result = future.result()
                    pair_to_result[pair] = result
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
                    progress = int((completed / len(image_pairs)) * 100)
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'progress': progress,
                            'message': f'ØªÙ…Øª Ù…Ù‚Ø§Ø±Ù†Ø© {completed}/{len(image_pairs)} Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„ØµÙˆØ±',
                            'current_pair': f"{os.path.basename(pair[0])} vs {os.path.basename(pair[1])}"
                        }
                    )
                    
                    logger.info(f"âœ… ØªÙ…Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©: {os.path.basename(pair[0])} vs {os.path.basename(pair[1])}")
                    
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© {pair}: {e}")
                    pair_to_result[pair] = {
                        "success": False,
                        "error": str(e),
                        "similarity": 0,
                        "processing_time": 0
                    }
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            results = [pair_to_result[pair] for pair in image_pairs]
        
        logger.info(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù€ {len(image_pairs)} Ø²ÙˆØ¬")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©: {e}")
        raise

    def _compare_single_pair(self, visual_service: EnhancedVisualComparisonService, old_path: str, new_path: str, job_id: str = None) -> Dict[str, Any]:
        """
        Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ØµØ±ÙŠØ© Ù„Ø²ÙˆØ¬ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„ØµÙˆØ± (ÙŠØªÙ… ØªØ´ØºÙŠÙ„Ù‡Ø§ ÙÙŠ thread Ù…Ù†ÙØµÙ„)
        Visual comparison for a single image pair (runs in separate thread)
        """
        try:
            # ØªØ´ØºÙŠÙ„ async function ÙÙŠ thread pool
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                comparison_result = loop.run_until_complete(
                    visual_service.compare_images(old_path, new_path)
                )
                
                if not comparison_result.get("success", False):
                    raise Exception(f"ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©: {comparison_result.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                
                return {
                    "success": True,
                    "similarity": comparison_result.get("similarity", 0),
                    "ssim_score": comparison_result.get("ssim_score", 0),
                    "phash_similarity": comparison_result.get("phash_similarity", 0),
                    "processing_time": comparison_result.get("processing_time", 0),
                    "changed_regions": comparison_result.get("changed_regions", []),
                    "old_image_path": old_path,
                    "new_image_path": new_path
                }
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© {old_path} vs {new_path}: {e}")
            return {
                "success": False,
                "error": str(e),
                "similarity": 0,
                "processing_time": 0,
                "old_image_path": old_path,
                "new_image_path": new_path
            }


@celery_app.task(bind=True)
def parallel_gemini_analysis_batch(self, text_pairs: List[Tuple[str, str]], job_id: str = None) -> List[Dict[str, Any]]:
    """
    ØªØ­Ù„ÙŠÙ„ Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ù€ Gemini Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø²ÙˆØ§Ø¬ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ
    Parallel Gemini analysis for multiple text pairs
    """
    try:
        logger.info(f"ğŸ¤– Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ù€ Gemini Ù„Ù€ {len(text_pairs)} Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ")
        
        results = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ThreadPoolExecutor Ù…Ø¹ Ø¹Ø¯Ø¯ Ù…Ø­Ø¯ÙˆØ¯ Ù…Ù† Ø§Ù„Ù€ workers Ù„ØªØ¬Ù†Ø¨ rate limiting
        max_gemini_workers = min(3, MAX_WORKERS)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 3 workers Ù„Ù€ Gemini
        
        with ThreadPoolExecutor(max_workers=max_gemini_workers) as executor:
            # Ø¥Ø±Ø³Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ù„Ù„Ù€ executor
            future_to_pair = {
                executor.submit(self._analyze_single_text_pair, old_text, new_text, job_id): (old_text, new_text)
                for old_text, new_text in text_pairs
            }
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            pair_to_result = {}
            completed = 0
            
            for future in as_completed(future_to_pair):
                pair = future_to_pair[future]
                completed += 1
                
                try:
                    result = future.result()
                    pair_to_result[pair] = result
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
                    progress = int((completed / len(text_pairs)) * 100)
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'progress': progress,
                            'message': f'ØªÙ… ØªØ­Ù„ÙŠÙ„ {completed}/{len(text_pairs)} Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù€ Gemini',
                            'current_analysis': f'Ù†Øµ {completed}'
                        }
                    )
                    
                    logger.info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù€ Gemini Ù„Ù„Ø²ÙˆØ¬ {completed}")
                    
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Gemini Ù„Ù„Ø²ÙˆØ¬ {completed}: {e}")
                    pair_to_result[pair] = {
                        "success": False,
                        "error": str(e),
                        "similarity_percentage": 0,
                        "processing_time": 0
                    }
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            results = [pair_to_result[pair] for pair in text_pairs]
        
        logger.info(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ù€ Gemini Ù„Ù€ {len(text_pairs)} Ø²ÙˆØ¬")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ù€ Gemini: {e}")
        raise

    def _analyze_single_text_pair(self, old_text: str, new_text: str, job_id: str = None) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Gemini Ù„Ø²ÙˆØ¬ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ (ÙŠØªÙ… ØªØ´ØºÙŠÙ„Ù‡Ø§ ÙÙŠ thread Ù…Ù†ÙØµÙ„)
        Gemini analysis for a single text pair (runs in separate thread)
        """
        try:
            # ØªØ´ØºÙŠÙ„ async function ÙÙŠ thread pool
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø®Ø¯Ù…Ø© Gemini
                from app.services.gemini_service import gemini_service
                
                analysis_result = loop.run_until_complete(
                    gemini_service.compare_educational_content(old_text, new_text)
                )
                
                if not analysis_result.get("success", False):
                    raise Exception(f"ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Gemini: {analysis_result.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                
                return {
                    "success": True,
                    "similarity_percentage": analysis_result.get("similarity_percentage", 0),
                    "content_changes": analysis_result.get("content_changes", []),
                    "summary": analysis_result.get("summary", ""),
                    "processing_time": analysis_result.get("processing_time", 0),
                    "detailed_analysis": analysis_result.get("detailed_analysis", {}),
                    "old_text_length": len(old_text),
                    "new_text_length": len(new_text)
                }
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Gemini: {e}")
            return {
                "success": False,
                "error": str(e),
                "similarity_percentage": 0,
                "processing_time": 0,
                "old_text_length": len(old_text),
                "new_text_length": len(new_text)
            }


@celery_app.task(bind=True)
def ultra_fast_comparison_workflow(self, session_id: str, old_image_paths: List[str], new_image_paths: List[str]) -> Dict[str, Any]:
    """
    Ø³ÙŠØ± Ø¹Ù…Ù„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© ÙƒØ§Ù…Ù„Ø©
    Ultra-fast comparison workflow with full parallel processing
    """
    try:
        logger.info(f"âš¡ Ø¨Ø¯Ø¡ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© - Ø§Ù„Ø¬Ù„Ø³Ø©: {session_id}")
        start_time = datetime.now()
        
        total_pairs = min(len(old_image_paths), len(new_image_paths))
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 5,
                'message': 'Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©...',
                'total_pairs': total_pairs,
                'stage': 'initialization'
            }
        )
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
        logger.info("ğŸ“ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ...")
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 10,
                'message': 'Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ±...',
                'stage': 'text_extraction'
            }
        )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
        all_image_paths = old_image_paths + new_image_paths
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ batches Ù„ØªØ¬Ù†Ø¨ overload
        batches = [all_image_paths[i:i + BATCH_SIZE * 2] for i in range(0, len(all_image_paths), BATCH_SIZE * 2)]
        
        all_extraction_results = []
        for i, batch in enumerate(batches):
            batch_results = parallel_extract_text_batch.delay(batch, session_id).get(timeout=600)
            all_extraction_results.extend(batch_results)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            progress = 10 + int(((i + 1) / len(batches)) * 30)
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': progress,
                    'message': f'ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† batch {i+1}/{len(batches)}',
                    'stage': 'text_extraction'
                }
            )
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ old Ùˆ new
        old_extraction_results = all_extraction_results[:len(old_image_paths)]
        new_extraction_results = all_extraction_results[len(old_image_paths):]
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        logger.info("ğŸ–¼ï¸ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©...")
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 45,
                'message': 'Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©...',
                'stage': 'visual_comparison'
            }
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ØµÙˆØ± Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        image_pairs = list(zip(old_image_paths, new_image_paths))
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ batches
        visual_batches = [image_pairs[i:i + BATCH_SIZE] for i in range(0, len(image_pairs), BATCH_SIZE)]
        
        all_visual_results = []
        for i, batch in enumerate(visual_batches):
            batch_results = parallel_visual_comparison_batch.delay(batch, session_id).get(timeout=400)
            all_visual_results.extend(batch_results)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            progress = 45 + int(((i + 1) / len(visual_batches)) * 25)
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': progress,
                    'message': f'ØªÙ…Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù€ batch {i+1}/{len(visual_batches)}',
                    'stage': 'visual_comparison'
                }
            )
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ­Ù„ÙŠÙ„ Gemini Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
        logger.info("ğŸ¤– Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ­Ù„ÙŠÙ„ Gemini Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ...")
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 75,
                'message': 'Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Gemini Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ...',
                'stage': 'gemini_analysis'
            }
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„
        text_pairs = []
        for old_result, new_result in zip(old_extraction_results, new_extraction_results):
            if old_result.get("success") and new_result.get("success"):
                text_pairs.append((old_result["text"], new_result["text"]))
            else:
                # Ø¥Ø¶Ø§ÙØ© Ø²ÙˆØ¬ ÙØ§Ø±Øº ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
                text_pairs.append(("", ""))
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ batches
        gemini_batches = [text_pairs[i:i + BATCH_SIZE] for i in range(0, len(text_pairs), BATCH_SIZE)]
        
        all_gemini_results = []
        for i, batch in enumerate(gemini_batches):
            batch_results = parallel_gemini_analysis_batch.delay(batch, session_id).get(timeout=500)
            all_gemini_results.extend(batch_results)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            progress = 75 + int(((i + 1) / len(gemini_batches)) * 15)
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': progress,
                    'message': f'ØªÙ… ØªØ­Ù„ÙŠÙ„ Gemini Ù„Ù€ batch {i+1}/{len(gemini_batches)}',
                    'stage': 'gemini_analysis'
                }
            )
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        logger.info("ğŸ“Š Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 95,
                'message': 'ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...',
                'stage': 'results_compilation'
            }
        )
        
        # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        final_results = []
        for i in range(total_pairs):
            pair_result = {
                "pair_index": i,
                "old_image_path": old_image_paths[i],
                "new_image_path": new_image_paths[i],
                "old_text_extraction": old_extraction_results[i],
                "new_text_extraction": new_extraction_results[i],
                "visual_comparison": all_visual_results[i],
                "gemini_analysis": all_gemini_results[i],
                "overall_similarity": self._calculate_overall_similarity(
                    all_visual_results[i], all_gemini_results[i]
                ),
                "processed_at": datetime.now().isoformat()
            }
            final_results.append(pair_result)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        end_time = datetime.now()
        total_processing_time = (end_time - start_time).total_seconds()
        
        successful_pairs = len([r for r in final_results if 
                              r["old_text_extraction"].get("success") and 
                              r["new_text_extraction"].get("success") and
                              r["visual_comparison"].get("success") and
                              r["gemini_analysis"].get("success")])
        
        summary = {
            "session_id": session_id,
            "total_pairs": total_pairs,
            "successful_pairs": successful_pairs,
            "failed_pairs": total_pairs - successful_pairs,
            "success_rate": (successful_pairs / total_pairs) * 100 if total_pairs > 0 else 0,
            "total_processing_time": total_processing_time,
            "average_time_per_pair": total_processing_time / total_pairs if total_pairs > 0 else 0,
            "parallel_efficiency": self._calculate_parallel_efficiency(final_results, total_processing_time),
            "average_visual_similarity": sum(r["visual_comparison"].get("similarity", 0) for r in final_results) / total_pairs if total_pairs > 0 else 0,
            "average_text_similarity": sum(r["gemini_analysis"].get("similarity_percentage", 0) for r in final_results) / total_pairs if total_pairs > 0 else 0,
            "completed_at": end_time.isoformat()
        }
        
        final_response = {
            "summary": summary,
            "results": final_results,
            "processing_stats": {
                "text_extraction_time": sum(r["old_text_extraction"].get("processing_time", 0) + r["new_text_extraction"].get("processing_time", 0) for r in final_results),
                "visual_comparison_time": sum(r["visual_comparison"].get("processing_time", 0) for r in final_results),
                "gemini_analysis_time": sum(r["gemini_analysis"].get("processing_time", 0) for r in final_results),
                "total_parallel_time": total_processing_time
            }
        }
        
        logger.info(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø±ÙŠØ¹! {total_pairs} Ø£Ø²ÙˆØ§Ø¬ ÙÙŠ {total_processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"ğŸ“ˆ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {summary['success_rate']:.1f}%")
        logger.info(f"âš¡ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©: {summary['parallel_efficiency']:.1f}%")
        
        return final_response
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø±ÙŠØ¹: {e}")
        self.update_state(
            state='FAILURE',
            meta={
                'progress': 0,
                'message': f'ÙØ´Ù„ ÙÙŠ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„: {str(e)}',
                'stage': 'failed'
            }
        )
        raise

    def _calculate_overall_similarity(self, visual_result: Dict[str, Any], gemini_result: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""
        visual_sim = visual_result.get("similarity", 0) if visual_result.get("success") else 0
        text_sim = gemini_result.get("similarity_percentage", 0) / 100 if gemini_result.get("success") else 0
        
        # ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„Ù†Øµ (70%) Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„ØµÙˆØ±Ø© (30%)
        return (visual_sim * 0.3) + (text_sim * 0.7)

    def _calculate_parallel_efficiency(self, results: List[Dict[str, Any]], total_time: float) -> float:
        """Ø­Ø³Ø§Ø¨ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©"""
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø°ÙŠ ÙƒØ§Ù† Ø³ÙŠØ³ØªØºØ±Ù‚Ù‡ Ù„Ùˆ ØªÙ… ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø´ÙƒÙ„ ØªØ³Ù„Ø³Ù„ÙŠ
        sequential_time = 0
        for result in results:
            sequential_time += result["old_text_extraction"].get("processing_time", 0)
            sequential_time += result["new_text_extraction"].get("processing_time", 0)
            sequential_time += result["visual_comparison"].get("processing_time", 0)
            sequential_time += result["gemini_analysis"].get("processing_time", 0)
        
        if total_time == 0:
            return 0
        
        # ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© = (Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ / Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ) * 100
        efficiency = (sequential_time / total_time) * 100
        return min(efficiency, 100)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 100%


@celery_app.task(bind=True)
def quick_dual_comparison(self, session_id: str, old_image_path: str, new_image_path: str) -> Dict[str, Any]:
    """
    Ù…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±ÙŠØ¹Ø© Ù„ØµÙˆØ±ØªÙŠÙ† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© ÙƒØ§Ù…Ù„Ø©
    Quick comparison of two images with full parallel processing
    """
    try:
        logger.info(f"âš¡ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØµÙˆØ±ØªÙŠÙ† - Ø§Ù„Ø¬Ù„Ø³Ø©: {session_id}")
        start_time = datetime.now()
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        self.update_state(
            state='PROGRESS',
            meta={
                'progress': 5,
                'message': 'Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©...',
                'stage': 'initialization'
            }
        )
        
        # ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
        with ThreadPoolExecutor(max_workers=4) as executor:
            # Ø¥Ø±Ø³Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
            future_old_text = executor.submit(self._extract_text_worker, old_image_path, session_id)
            future_new_text = executor.submit(self._extract_text_worker, new_image_path, session_id)
            future_visual = executor.submit(self._compare_visual_worker, old_image_path, new_image_path, session_id)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': 20,
                    'message': 'ØªØ´ØºÙŠÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©...',
                    'stage': 'parallel_extraction'
                }
            )
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            old_extraction = future_old_text.result()
            new_extraction = future_new_text.result()
            visual_comparison = future_visual.result()
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
            self.update_state(
                state='PROGRESS',
                meta={
                    'progress': 70,
                    'message': 'ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù€ Gemini...',
                    'stage': 'gemini_analysis'
                }
            )
            
            # ØªØ­Ù„ÙŠÙ„ Gemini Ù„Ù„Ù†ØµÙˆØµ
            gemini_result = {"success": False, "similarity_percentage": 0, "error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„"}
            if old_extraction.get("success") and new_extraction.get("success"):
                future_gemini = executor.submit(
                    self._analyze_gemini_worker, 
                    old_extraction["text"], 
                    new_extraction["text"], 
                    session_id
                )
                gemini_result = future_gemini.result()
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        end_time = datetime.now()
        total_processing_time = (end_time - start_time).total_seconds()
        
        overall_similarity = self._calculate_overall_similarity(visual_comparison, gemini_result)
        
        final_result = {
            "session_id": session_id,
            "old_image_path": old_image_path,
            "new_image_path": new_image_path,
            "old_text_extraction": old_extraction,
            "new_text_extraction": new_extraction,
            "visual_comparison": visual_comparison,
            "gemini_analysis": gemini_result,
            "overall_similarity": overall_similarity,
            "total_processing_time": total_processing_time,
            "parallel_efficiency": self._calculate_efficiency(
                old_extraction, new_extraction, visual_comparison, gemini_result, total_processing_time
            ),
            "success": all([
                old_extraction.get("success", False),
                new_extraction.get("success", False),
                visual_comparison.get("success", False),
                gemini_result.get("success", False)
            ]),
            "completed_at": end_time.isoformat()
        }
        
        logger.info(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ÙÙŠ {total_processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"ğŸ“Š Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {overall_similarity:.1%}")
        
        return final_result
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©: {e}")
        self.update_state(
            state='FAILURE',
            meta={
                'progress': 0,
                'message': f'ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: {str(e)}',
                'stage': 'failed'
            }
        )
        raise

    def _extract_text_worker(self, image_path: str, session_id: str) -> Dict[str, Any]:
        """Worker Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ"""
        try:
            from app.services.landing_ai_service import LandingAIService
            landing_service = LandingAIService()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    landing_service.extract_from_file(image_path, job_id=session_id)
                )
                
                return {
                    "success": result.success,
                    "text": result.markdown_content,
                    "confidence": result.confidence_score,
                    "word_count": len(result.markdown_content.split()),
                    "processing_time": result.processing_time,
                    "service": "LandingAI_Parallel",
                    "image_path": image_path
                }
            finally:
                loop.close()
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "text": "",
                "confidence": 0,
                "processing_time": 0,
                "service": "LandingAI_Error"
            }

    def _compare_visual_worker(self, old_path: str, new_path: str, session_id: str) -> Dict[str, Any]:
        """Worker Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©"""
        try:
            from app.services.visual_comparison_service import EnhancedVisualComparisonService
            visual_service = EnhancedVisualComparisonService()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    visual_service.compare_images(old_path, new_path)
                )
                
                return {
                    "success": result.get("success", False),
                    "similarity": result.get("similarity", 0),
                    "ssim_score": result.get("ssim_score", 0),
                    "processing_time": result.get("processing_time", 0),
                    "changed_regions": result.get("changed_regions", [])
                }
            finally:
                loop.close()
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "similarity": 0,
                "processing_time": 0
            }

    def _analyze_gemini_worker(self, old_text: str, new_text: str, session_id: str) -> Dict[str, Any]:
        """Worker Ù„ØªØ­Ù„ÙŠÙ„ Gemini"""
        try:
            from app.services.gemini_service import gemini_service
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    gemini_service.compare_educational_content(old_text, new_text)
                )
                
                return {
                    "success": result.get("success", False),
                    "similarity_percentage": result.get("similarity_percentage", 0),
                    "content_changes": result.get("content_changes", []),
                    "summary": result.get("summary", ""),
                    "processing_time": result.get("processing_time", 0)
                }
            finally:
                loop.close()
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "similarity_percentage": 0,
                "processing_time": 0
            }

    def _calculate_efficiency(self, old_extraction: Dict, new_extraction: Dict, 
                            visual_comparison: Dict, gemini_result: Dict, 
                            total_time: float) -> float:
        """Ø­Ø³Ø§Ø¨ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©"""
        sequential_time = (
            old_extraction.get("processing_time", 0) +
            new_extraction.get("processing_time", 0) +
            visual_comparison.get("processing_time", 0) +
            gemini_result.get("processing_time", 0)
        )
        
        if total_time == 0:
            return 0
        
        return min((sequential_time / total_time) * 100, 100) 