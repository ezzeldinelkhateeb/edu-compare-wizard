#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø­Ø³Ù† - Enhanced Text Processing System
Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©

ÙŠØ´Ù…Ù„:
1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù†ØµÙˆØµ Landing AI
2. ØªÙ†Ø¸ÙŠÙ Ø°ÙƒÙŠ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨
3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙ…Ø¬Ù„Ø¯Ø§Øª ÙƒØ§Ù…Ù„Ø©
4. ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
5. ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª
"""

import os
import re
import json
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing
from loguru import logger
import aiofiles
import tempfile
import hashlib
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
from functools import lru_cache

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ logging
logger.remove()
logger.add(
    lambda msg: print(msg, end=""),
    format="<green>{time:HH:mm:ss}</green> | <level>{level}</level> | {message}",
    level="INFO"
)

class ContentType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    EDUCATIONAL_TEXT = "educational_text"
    FIGURE_DESCRIPTION = "figure_description"
    TECHNICAL_METADATA = "technical_metadata"
    COORDINATES = "coordinates"
    NOISE = "noise"
    UNKNOWN = "unknown"

@dataclass
class ProcessingResult:
    """Ù†ØªÙŠØ¬Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Øµ"""
    original_text: str
    cleaned_text: str
    content_type: ContentType
    confidence: float
    processing_time: float
    metadata: Dict[str, Any]
    chunks_removed: List[str]
    chunks_kept: List[str]

@dataclass
class BatchProcessingResult:
    """Ù†ØªÙŠØ¬Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª"""
    total_files: int
    processed_files: int
    failed_files: int
    total_time: float
    results: List[ProcessingResult]
    summary_stats: Dict[str, Any]

class AdvancedTextProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, num_workers: int = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        
        Args:
            num_workers: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª)
        """
        self.num_workers = num_workers or min(8, multiprocessing.cpu_count())
        
        # Ø£Ù†Ù…Ø§Ø· ØªÙ†Ø¸ÙŠÙ Ù…Ø­Ø³Ù†Ø©
        self.cleanup_patterns = self._initialize_cleanup_patterns()
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ
        self.educational_patterns = self._initialize_educational_patterns()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.stats = {
            'total_processed': 0,
            'total_cleaned': 0,
            'average_reduction': 0.0,
            'processing_times': []
        }
        
        logger.info(f"ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ {self.num_workers} Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªÙˆØ§Ø²ÙŠØ©")

    def _initialize_cleanup_patterns(self) -> Dict[str, List[str]]:
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
        return {
            # Ø¥Ø²Ø§Ù„Ø© ØªØ¹Ù„ÙŠÙ‚Ø§Øª HTML ÙˆØªÙØ§ØµÙŠÙ„ Landing AI
            'html_comments': [
                r'<!--[\s\S]*?-->',
                r'<!-- text,[\s\S]*?-->',
                r'<!-- figure,[\s\S]*?-->',
                r'<!-- marginalia,[\s\S]*?-->',
                r'<!-- illustration,[\s\S]*?-->',
                r'<!-- table,[\s\S]*?-->'
            ],
            
            # Ø¥Ø²Ø§Ù„Ø© Ø£Ù‚Ø³Ø§Ù… ÙˆØµÙ Ø§Ù„ØµÙˆØ±
            'image_descriptions': [
                r'Summary\s*:\s*This[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'photo\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'illustration\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'Scene Overview\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'Technical Details\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'Spatial Relationships\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'Analysis\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])'
            ],
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©
            'coordinates': [
                r'from page \d+ \(l=[\d.]+,t=[\d.]+,r=[\d.]+,b=[\d.]+\)',
                r'with ID [a-f0-9\-]+',
                r'\(l=[\d.]+,t=[\d.]+,r=[\d.]+,b=[\d.]+\)',
                r'page \d+',
                r'ID [a-f0-9\-]+'
            ],
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©
            'english_noise': [
                r'The image[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'This figure[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'The figure[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ])',
                r'^\s*â€¢\s+The[\s\S]*?(?=\n)',
                r'^\s*â€¢\s+Each[\s\S]*?(?=\n)',
                r'^\s*â€¢\s+No scale[\s\S]*?(?=\n)'
            ],
            
            # ØªÙ†Ø¸ÙŠÙ Ø¹Ø§Ù…
            'whitespace': [
                r'\n\s*\n\s*\n+',  # Ø£Ø³Ø·Ø± ÙØ§Ø±ØºØ© Ù…ØªØ¹Ø¯Ø¯Ø©
                r'^\s+',  # Ù…Ø³Ø§ÙØ§Øª ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³Ø·Ø±
                r'\s+$',  # Ù…Ø³Ø§ÙØ§Øª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø³Ø·Ø±
                r'\s+',   # Ù…Ø³Ø§ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
            ]
        }

    def _initialize_educational_patterns(self) -> List[str]:
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ"""
        return [
            # Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
            r'[Ø£-ÙŠ]+.*[:.ØŸ!]',
            r'Ø§Ù„ÙØµÙ„\s+\d+',
            r'Ù‚Ø§Ø¹Ø¯Ø©|Ù…Ø¨Ø¯Ø£|Ù†Ø¸Ø±ÙŠØ©|Ù‚Ø§Ù†ÙˆÙ†',
            r'ØªØ¹Ø±ÙŠÙ|ØªØ·Ø¨ÙŠÙ‚|Ù…Ø«Ø§Ù„|Ù…Ù„Ø§Ø­Ø¸Ø©',
            r'ÙŠØ¤Ø«Ø±|ÙŠÙ†ØªÙ‚Ù„|ÙŠØ¹Ù…Ù„|ÙŠØ³ØªØ®Ø¯Ù…',
            
            # Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ø±Ù…ÙˆØ²
            r'\$.*?\$',
            r'=\s*[\d\w]+',
            r'[A-Za-z]\s*=\s*[A-Za-z\d]+',
            
            # Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„ØªØ±Ù‚ÙŠÙ…
            r'^\d+[\.\-]\d*',
            r'^[Ø£-ÙŠ]+\s*:',
            r'^\*\s+[Ø£-ÙŠ]+'
        ]

    def classify_content(self, text: str) -> Tuple[ContentType, float]:
        """
        ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        
        Returns:
            Tuple[ContentType, float]: Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©
        """
        if not text.strip():
            return ContentType.NOISE, 1.0
        
        # ÙØ­Øµ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        if re.search(r'<!--.*?-->', text):
            return ContentType.TECHNICAL_METADATA, 0.9
        
        # ÙØ­Øµ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
        if re.search(r'\(l=[\d.]+,t=[\d.]+', text):
            return ContentType.COORDINATES, 0.95
        
        # ÙØ­Øµ Ø£ÙˆØµØ§Ù Ø§Ù„ØµÙˆØ±
        if any(keyword in text.lower() for keyword in ['summary :', 'photo:', 'scene overview', 'analysis :']):
            return ContentType.FIGURE_DESCRIPTION, 0.8
        
        # ÙØ­Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ
        educational_score = 0
        for pattern in self.educational_patterns:
            if re.search(pattern, text):
                educational_score += 1
        
        if educational_score > 0:
            confidence = min(educational_score / len(self.educational_patterns), 1.0)
            return ContentType.EDUCATIONAL_TEXT, confidence
        
        return ContentType.UNKNOWN, 0.3

    def clean_text_advanced(self, text: str) -> ProcessingResult:
        """
        ØªÙ†Ø¸ÙŠÙ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Øµ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ†Ø¸ÙŠÙÙ‡
            
        Returns:
            ProcessingResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„
        """
        start_time = datetime.now()
        original_text = text
        chunks_removed = []
        chunks_kept = []
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ
        content_type, confidence = self.classify_content(text)
        
        logger.debug(f"ğŸ” ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {content_type.value} (Ø«Ù‚Ø©: {confidence:.2f})")
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
        for category, patterns in self.cleanup_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
                if matches:
                    chunks_removed.extend(matches)
                    text = re.sub(pattern, '', text, flags=re.MULTILINE | re.DOTALL)
        
        # ØªÙ†Ø¸ÙŠÙ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        text = self._clean_arabic_text(text)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ©
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        text = text.strip()
        
        # Ø­ÙØ¸ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡Ø§
        if text.strip():
            chunks_kept = [chunk.strip() for chunk in text.split('\n\n') if chunk.strip()]
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        reduction_percentage = ((len(original_text) - len(text)) / len(original_text)) * 100 if original_text else 0
        
        metadata = {
            'original_length': len(original_text),
            'cleaned_length': len(text),
            'reduction_percentage': reduction_percentage,
            'removed_chunks_count': len(chunks_removed),
            'kept_chunks_count': len(chunks_kept)
        }
        
        return ProcessingResult(
            original_text=original_text,
            cleaned_text=text,
            content_type=content_type,
            confidence=confidence,
            processing_time=processing_time,
            metadata=metadata,
            chunks_removed=chunks_removed,
            chunks_kept=chunks_kept
        )

    def _clean_arabic_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø®Ø§Øµ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„ØºØ±ÙŠØ¨Ø© ÙˆØ§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\w\s\.\,\!\?\:\;\(\)\[\]\{\}\-\+\=\$\%\@\#\*\/\\\|]', '', text)
        
        # ØªØµØ­ÙŠØ­ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø­ÙˆÙ„ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text = re.sub(r'\s*([ØŒØ›ØŸ!])\s*', r'\1 ', text)
        text = re.sub(r'\s*([:])\s*', r'\1 ', text)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        text = re.sub(r'(\d+)\s*([Ø£-ÙŠ])', r'\1 \2', text)
        text = re.sub(r'([Ø£-ÙŠ])\s*(\d+)', r'\1 \2', text)
        
        return text

    async def process_file_async(self, file_path: Union[str, Path]) -> ProcessingResult:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
        
        Args:
            file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
            
        Returns:
            ProcessingResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        file_path = Path(file_path)
        
        try:
            logger.info(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {file_path.name}")
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
            if file_path.suffix.lower() == '.json':
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                    text = data.get('markdown', '') or data.get('text', '')
            else:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    text = await f.read()
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
            result = self.clean_text_advanced(text)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            await self._save_processed_result(file_path, result)
            
            logger.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {file_path.name} - ØªØ­Ø³Ù†: {result.metadata['reduction_percentage']:.1f}%")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© {file_path.name}: {e}")
            return ProcessingResult(
                original_text="",
                cleaned_text="",
                content_type=ContentType.UNKNOWN,
                confidence=0.0,
                processing_time=0.0,
                metadata={'error': str(e)},
                chunks_removed=[],
                chunks_kept=[]
            )

    async def _save_processed_result(self, original_path: Path, result: ProcessingResult):
        """Ø­ÙØ¸ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        output_dir = original_path.parent / "processed"
        output_dir.mkdir(exist_ok=True)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù
        cleaned_file = output_dir / f"{original_path.stem}_cleaned.md"
        async with aiofiles.open(cleaned_file, 'w', encoding='utf-8') as f:
            await f.write(result.cleaned_text)
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        report_file = output_dir / f"{original_path.stem}_processing_report.json"
        report_data = {
            'file_path': str(original_path),
            'processing_time': result.processing_time,
            'content_type': result.content_type.value,
            'confidence': result.confidence,
            'metadata': result.metadata,
            'chunks_removed_count': len(result.chunks_removed),
            'chunks_kept_count': len(result.chunks_kept)
        }
        
        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report_data, ensure_ascii=False, indent=2))

    async def process_directory_batch(self, directory_path: Union[str, Path], 
                                    file_extensions: List[str] = None) -> BatchProcessingResult:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯
        
        Args:
            directory_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¬Ù„Ø¯
            file_extensions: Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§
            
        Returns:
            BatchProcessingResult: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©
        """
        directory_path = Path(directory_path)
        file_extensions = file_extensions or ['.md', '.txt', '.json']
        
        logger.info(f"ğŸ“ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹ÙŠØ© Ù„Ù„Ù…Ø¬Ù„Ø¯: {directory_path}")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
        files_to_process = []
        for ext in file_extensions:
            files_to_process.extend(directory_path.glob(f"**/*{ext}"))
        
        total_files = len(files_to_process)
        logger.info(f"ğŸ“Š ÙˆÙØ¬Ø¯ {total_files} Ù…Ù„Ù Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        
        if total_files == 0:
            return BatchProcessingResult(
                total_files=0,
                processed_files=0,
                failed_files=0,
                total_time=0.0,
                results=[],
                summary_stats={}
            )
        
        start_time = datetime.now()
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
        semaphore = asyncio.Semaphore(self.num_workers)
        
        async def process_with_semaphore(file_path):
            async with semaphore:
                return await self.process_file_async(file_path)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        tasks = [process_with_semaphore(file_path) for file_path in files_to_process]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        processed_results = []
        failed_count = 0
        
        for result in results:
            if isinstance(result, Exception):
                failed_count += 1
                logger.error(f"âŒ ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù: {result}")
            else:
                processed_results.append(result)
        
        total_time = (datetime.now() - start_time).total_seconds()
        processed_count = len(processed_results)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø¬Ù…Ø¹Ø©
        summary_stats = self._calculate_batch_stats(processed_results, total_time)
        
        logger.success(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ÙÙŠ {total_time:.2f}s")
        logger.info(f"ğŸ“Š Ù…Ø¹Ø§Ù„Ø¬: {processed_count}, ÙØ´Ù„: {failed_count}")
        
        return BatchProcessingResult(
            total_files=total_files,
            processed_files=processed_count,
            failed_files=failed_count,
            total_time=total_time,
            results=processed_results,
            summary_stats=summary_stats
        )

    def _calculate_batch_stats(self, results: List[ProcessingResult], total_time: float) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©"""
        if not results:
            return {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø·ÙˆÙ„
        original_lengths = [r.metadata['original_length'] for r in results]
        cleaned_lengths = [r.metadata['cleaned_length'] for r in results]
        reduction_percentages = [r.metadata['reduction_percentage'] for r in results]
        processing_times = [r.processing_time for r in results]
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_types = {}
        for result in results:
            content_type = result.content_type.value
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        return {
            'total_original_length': sum(original_lengths),
            'total_cleaned_length': sum(cleaned_lengths),
            'average_reduction_percentage': statistics.mean(reduction_percentages),
            'median_reduction_percentage': statistics.median(reduction_percentages),
            'average_processing_time': statistics.mean(processing_times),
            'total_processing_time': total_time,
            'content_type_distribution': content_types,
            'files_per_second': len(results) / total_time if total_time > 0 else 0,
            'efficiency_score': (sum(reduction_percentages) / len(results)) * (len(results) / total_time) if total_time > 0 else 0
        }

    def generate_comparison_optimized_text(self, result: ProcessingResult) -> str:
        """
        Ø¥Ù†ØªØ§Ø¬ Ù†Øµ Ù…Ø­Ø³Ù† Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        
        Args:
            result: Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            
        Returns:
            str: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        """
        text = result.cleaned_text
        
        # ØªØ­Ø³ÙŠÙ†Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        # 1. ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª
        text = self._normalize_terminology(text)
        
        # 2. ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        text = self._organize_concepts(text)
        
        # 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        text = self._remove_comparison_noise(text)
        
        return text.strip()

    def _normalize_terminology(self, text: str) -> str:
        """ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©"""
        # Ù‚Ø§Ù…ÙˆØ³ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª
        terminology_map = {
            r'Ù‚Ø§Ø¹Ø¯Ø©\s+Ø¨Ø§Ø³ÙƒØ§Ù„|Ù…Ø¨Ø¯Ø£\s+Ø¨Ø§Ø³ÙƒØ§Ù„': 'Ù‚Ø§Ø¹Ø¯Ø© Ø¨Ø§Ø³ÙƒØ§Ù„',
            r'Ø§Ù„Ø¶ØºØ·\s+Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ|Ø§Ù„Ø¶ØºØ·\s+Ø§Ù„Ù…Ø§Ø¦ÙŠ': 'Ø§Ù„Ø¶ØºØ· Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ',
            r'Ø§Ù„Ù…ÙƒØ¨Ø³\s+Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ|Ø§Ù„ÙƒØ¨Ø§Ø³\s+Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ': 'Ø§Ù„Ù…ÙƒØ¨Ø³ Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ',
            r'Ø§Ù„Ø³Ø§Ø¦Ù„\s+Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ|Ø§Ù„Ø³Ø§Ø¦Ù„\s+Ø§Ù„Ù…Ø§Ø¦ÙŠ': 'Ø§Ù„Ø³Ø§Ø¦Ù„ Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ'
        }
        
        for pattern, replacement in terminology_map.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text

    def _organize_concepts(self, text: str) -> str:
        """ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"""
        lines = text.split('\n')
        organized_lines = []
        
        # ØªØ±ØªÙŠØ¨ Ø£ÙˆÙ„ÙˆÙŠØ§Øª: ØªØ¹Ø±ÙŠÙØ§ØªØŒ Ù‚ÙˆØ§Ù†ÙŠÙ†ØŒ ØªØ·Ø¨ÙŠÙ‚Ø§ØªØŒ Ø£Ù…Ø«Ù„Ø©
        priorities = {
            'definition': [],  # ØªØ¹Ø±ÙŠÙØ§Øª
            'law': [],         # Ù‚ÙˆØ§Ù†ÙŠÙ†
            'application': [], # ØªØ·Ø¨ÙŠÙ‚Ø§Øª
            'example': [],     # Ø£Ù…Ø«Ù„Ø©
            'other': []        # Ø£Ø®Ø±Ù‰
        }
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if re.search(r'ØªØ¹Ø±ÙŠÙ|ÙŠÙØ¹Ø±Ù|Ù‡Ùˆ', line):
                priorities['definition'].append(line)
            elif re.search(r'Ù‚Ø§Ø¹Ø¯Ø©|Ù‚Ø§Ù†ÙˆÙ†|Ù…Ø¨Ø¯Ø£|Ù†Ø¸Ø±ÙŠØ©', line):
                priorities['law'].append(line)
            elif re.search(r'ØªØ·Ø¨ÙŠÙ‚|Ø§Ø³ØªØ®Ø¯Ø§Ù…|ÙŠØ³ØªØ®Ø¯Ù…', line):
                priorities['application'].append(line)
            elif re.search(r'Ù…Ø«Ø§Ù„|Ù…Ø«Ù„Ø§Ù‹|Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„', line):
                priorities['example'].append(line)
            else:
                priorities['other'].append(line)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†Øµ Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        for category in ['definition', 'law', 'application', 'example', 'other']:
            organized_lines.extend(priorities[category])
        
        return '\n'.join(organized_lines)

    def _remove_comparison_noise(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ´ÙˆØ´ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        noise_patterns = [
            r'ÙƒÙ…Ø§ Ø¨Ø§Ù„Ø´ÙƒÙ„',
            r'Ø§Ù†Ø¸Ø± Ø§Ù„Ø´ÙƒÙ„',
            r'ÙÙŠ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ',
            r'Ø§Ù„Ø´ÙƒÙ„ ÙŠÙˆØ¶Ø­',
            r'Ù…Ù† Ø§Ù„Ø´ÙƒÙ„ Ù†Ù„Ø§Ø­Ø¸'
        ]
        
        for pattern in noise_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text

class SmartBatchProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ù…Ø¬Ù…ÙˆØ¹ÙŠ Ø°ÙƒÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"""
    
    def __init__(self, max_workers: int = None, chunk_size: int = 1000):
        self.processor = AdvancedTextProcessor(max_workers)
        self.chunk_size = chunk_size
        
    async def process_large_dataset(self, root_directory: Path, 
                                  output_directory: Path = None) -> Dict[str, Any]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        
        Args:
            root_directory: Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ø°Ø±
            output_directory: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            
        Returns:
            Dict: ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        root_directory = Path(root_directory)
        output_directory = output_directory or root_directory / "batch_processed"
        output_directory.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ­ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ© Ù„Ù€: {root_directory}")
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        all_files = list(root_directory.rglob("*.md")) + \
                   list(root_directory.rglob("*.txt")) + \
                   list(root_directory.rglob("*.json"))
        
        total_files = len(all_files)
        logger.info(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {total_files}")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø¯ÙØ¹Ø§Øª
        batches = [all_files[i:i + self.chunk_size] 
                  for i in range(0, total_files, self.chunk_size)]
        
        all_results = []
        batch_reports = []
        
        start_time = datetime.now()
        
        for i, batch in enumerate(batches, 1):
            logger.info(f"ğŸ“¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {i}/{len(batches)} ({len(batch)} Ù…Ù„Ù)")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø©
            batch_start = datetime.now()
            
            tasks = [self.processor.process_file_async(file_path) for file_path in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            batch_time = (datetime.now() - batch_start).total_seconds()
            
            # ØªØ­Ù„ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯ÙØ¹Ø©
            successful_results = [r for r in batch_results if not isinstance(r, Exception)]
            failed_count = len(batch_results) - len(successful_results)
            
            batch_report = {
                'batch_number': i,
                'files_count': len(batch),
                'successful_count': len(successful_results),
                'failed_count': failed_count,
                'processing_time': batch_time,
                'files_per_second': len(successful_results) / batch_time if batch_time > 0 else 0
            }
            
            batch_reports.append(batch_report)
            all_results.extend(successful_results)
            
            logger.success(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ø¯ÙØ¹Ø© {i} ÙÙŠ {batch_time:.2f}s")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            await self._save_batch_report(output_directory, i, batch_report, successful_results)
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        # Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        final_report = {
            'processing_summary': {
                'total_files': total_files,
                'total_batches': len(batches),
                'successful_files': len(all_results),
                'failed_files': total_files - len(all_results),
                'total_processing_time': total_time,
                'overall_files_per_second': len(all_results) / total_time if total_time > 0 else 0
            },
            'batch_details': batch_reports,
            'performance_metrics': self._calculate_performance_metrics(all_results, total_time)
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        await self._save_final_report(output_directory, final_report)
        
        logger.success(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ© ÙÙŠ {total_time:.2f}s")
        logger.info(f"ğŸ“Š Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {final_report['processing_summary']['overall_files_per_second']:.2f} Ù…Ù„Ù/Ø«Ø§Ù†ÙŠØ©")
        
        return final_report

    async def _save_batch_report(self, output_dir: Path, batch_num: int, 
                               report: Dict, results: List[ProcessingResult]):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¯ÙØ¹Ø©"""
        batch_dir = output_dir / f"batch_{batch_num:03d}"
        batch_dir.mkdir(exist_ok=True)
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¯ÙØ¹Ø©
        report_file = batch_dir / "batch_report.json"
        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report, ensure_ascii=False, indent=2))
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù†Ø¸ÙØ©
        for i, result in enumerate(results):
            if result.cleaned_text.strip():
                cleaned_file = batch_dir / f"cleaned_{i:03d}.md"
                async with aiofiles.open(cleaned_file, 'w', encoding='utf-8') as f:
                    await f.write(result.cleaned_text)

    async def _save_final_report(self, output_dir: Path, report: Dict):
        """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        final_report_file = output_dir / "final_processing_report.json"
        async with aiofiles.open(final_report_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report, ensure_ascii=False, indent=2))

    def _calculate_performance_metrics(self, results: List[ProcessingResult], 
                                     total_time: float) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if not results:
            return {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        processing_times = [r.processing_time for r in results]
        reduction_percentages = [r.metadata.get('reduction_percentage', 0) for r in results]
        
        return {
            'average_file_processing_time': statistics.mean(processing_times),
            'median_file_processing_time': statistics.median(processing_times),
            'fastest_file_time': min(processing_times),
            'slowest_file_time': max(processing_times),
            'average_size_reduction': statistics.mean(reduction_percentages),
            'total_efficiency_score': (sum(reduction_percentages) / len(results)) * (len(results) / total_time) if total_time > 0 else 0,
            'throughput_files_per_minute': (len(results) / total_time) * 60 if total_time > 0 else 0
        }

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø´Ø±

async def process_single_file(file_path: str) -> ProcessingResult:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯"""
    processor = AdvancedTextProcessor()
    return await processor.process_file_async(file_path)

async def process_directory(directory_path: str, extensions: List[str] = None) -> BatchProcessingResult:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„"""
    processor = AdvancedTextProcessor()
    return await processor.process_directory_batch(directory_path, extensions)

async def process_large_dataset(root_dir: str, output_dir: str = None) -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©"""
    batch_processor = SmartBatchProcessor()
    return await batch_processor.process_large_dataset(Path(root_dir), Path(output_dir) if output_dir else None)

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
async def test_enhanced_system():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†"""
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø­Ø³Ù†")
    print("=" * 60)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯
    test_file = "d:/ezz/compair/edu-compare-wizard/backend/uploads/landingai_results/extraction_20250705_172654/extracted_content.md"
    if Path(test_file).exists():
        print("ğŸ“„ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯...")
        result = await process_single_file(test_file)
        print(f"âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ {result.metadata['reduction_percentage']:.1f}% Ù…Ù† Ø§Ù„Ù†Øµ")
        print(f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {result.processing_time:.3f}s")
        print(f"ğŸ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {result.content_type.value}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯
    test_dir = "d:/ezz/compair/edu-compare-wizard/backend/uploads/landingai_results"
    if Path(test_dir).exists():
        print(f"\nğŸ“ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯: {test_dir}")
        batch_result = await process_directory(test_dir, ['.md', '.json'])
        print(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {batch_result.processed_files} Ù…Ù„Ù")
        print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {batch_result.total_time:.2f}s")
        print(f"ğŸ¯ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ­Ø³Ù†: {batch_result.summary_stats.get('average_reduction_percentage', 0):.1f}%")

if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_enhanced_system())
