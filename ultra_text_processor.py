#!/usr/bin/env python3
"""
Ultra Text Processor - Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙØ§Ø¦Ù‚
Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ù…Ø®Ø±Ø¬Ø§Øª Landing AI Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØµÙŠØ©

Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ
- ØªØ­Ø³ÙŠÙ† Ø®Ø§Øµ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª Ù…Ø¹ Gemini
- Ù…Ø¹Ø§Ù„Ø¬Ø© PDF Ù…Ø¨Ø§Ø´Ø±Ø©
- ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ
- ØªÙ‚Ø§Ø±ÙŠØ± Ù…ÙØµÙ„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
"""

import asyncio
import json
import re
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
import difflib
import hashlib
import unicodedata

class ContentQuality(Enum):
    """ØªØµÙ†ÙŠÙ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
    EXCELLENT = "Ù…Ù…ØªØ§Ø²"
    GOOD = "Ø¬ÙŠØ¯" 
    ACCEPTABLE = "Ù…Ù‚Ø¨ÙˆÙ„"
    POOR = "Ø¶Ø¹ÙŠÙ"
    VERY_POOR = "Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ø§Ù‹"

class TextType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"""
    THEORY = "Ù†Ø¸Ø±ÙŠØ©"
    EXAMPLE = "Ù…Ø«Ø§Ù„"
    EXERCISE = "ØªÙ…Ø±ÙŠÙ†"
    DEFINITION = "ØªØ¹Ø±ÙŠÙ"
    FORMULA = "Ù…Ø¹Ø§Ø¯Ù„Ø©"
    DIAGRAM_DESC = "ÙˆØµÙ Ø±Ø³Ù…"
    MIXED = "Ù…Ø®ØªÙ„Ø·"

@dataclass
class TextAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ"""
    original_length: int
    cleaned_length: int
    reduction_percentage: float
    quality_score: float
    content_type: TextType
    quality_rating: ContentQuality
    arabic_ratio: float
    educational_keywords: List[str]
    technical_terms: List[str]
    formulas_count: int
    issues_found: List[str]
    suggestions: List[str]
    readability_score: float

class UltraTextProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙØ§Ø¦Ù‚"""
    
    def __init__(self):
        self.educational_keywords = {
            'theory': ['Ù‚Ø§Ù†ÙˆÙ†', 'Ù†Ø¸Ø±ÙŠØ©', 'Ù…Ø¨Ø¯Ø£', 'Ù‚Ø§Ø¹Ø¯Ø©', 'Ø£Ø³Ø§Ø³', 'Ù…ÙÙ‡ÙˆÙ…'],
            'example': ['Ù…Ø«Ø§Ù„', 'ØªÙˆØ¶ÙŠØ­', 'ØªØ·Ø¨ÙŠÙ‚', 'Ø­Ø§Ù„Ø©', 'Ù…Ø³Ø£Ù„Ø©'],
            'definition': ['ØªØ¹Ø±ÙŠÙ', 'Ù…Ø¹Ù†Ù‰', 'Ù…ØµØ·Ù„Ø­', 'ÙŠØ¹Ù†ÙŠ', 'Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù†'],
            'formula': ['Ù…Ø¹Ø§Ø¯Ù„Ø©', 'ØµÙŠØºØ©', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ø­Ø³Ø§Ø¨', 'Ù†ØªÙŠØ¬Ø©'],
            'exercise': ['ØªÙ…Ø±ÙŠÙ†', 'Ø³Ø¤Ø§Ù„', 'Ù…Ø³Ø£Ù„Ø©', 'Ø§Ø­Ø³Ø¨', 'Ø£ÙˆØ¬Ø¯', 'Ø­Ù„']
        }
        
        self.noise_patterns = [
            # HTML ÙˆØªØ¹Ù„ÙŠÙ‚Ø§Øª Landing AI
            r'<!--[\s\S]*?-->',
            r'<!-- text,[\s\S]*?-->',
            r'<!-- figure,[\s\S]*?-->',
            r'<!-- marginalia,[\s\S]*?-->',
            r'<!-- illustration,[\s\S]*?-->',
            r'<!-- table,[\s\S]*?-->',
            
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
            r'from page \d+ \([^)]+\)',
            r'with ID [a-f0-9\-]+',
            r'\(l=[\d.]+,t=[\d.]+,r=[\d.]+,b=[\d.]+\)',
            
            # Ø£ÙˆØµØ§Ù Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            r'Summary\s*:\s*This[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'Scene Overview\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'Technical Details\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'Spatial Relationships\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'Analysis\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'photo\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'illustration\s*:[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            
            # Ù†Ù‚Ø§Ø· ÙˆØµÙ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            r'^\s*â€¢\s+The[\s\S]*?(?=\n)',
            r'^\s*â€¢\s+Each[\s\S]*?(?=\n)',
            r'^\s*â€¢\s+No scale[\s\S]*?(?=\n)',
            r'^\s*â€¢\s+Arabic text[\s\S]*?(?=\n)',
            
            # Ø¹Ø¨Ø§Ø±Ø§Øª ÙˆØµÙÙŠØ© Ø¹Ø§Ù…Ø©
            r'The image[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'This figure[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
            r'The figure[\s\S]*?(?=\n\n|\n[Ø£-ÙŠ]|\Z)',
        ]
        
        self.improvement_patterns = [
            # ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            (r'\s*([ØŒØ›ØŸ!])\s*', r'\1 '),
            (r'\s*([:])\s*', r'\1 '),
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            (r'\s+', ' '),
            (r'\n\s*\n\s*\n+', '\n\n'),
            
            # ØªØ­Ø³ÙŠÙ† ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            (r'(\d+)\s*([Ø£-ÙŠ])', r'\1. \2'),
        ]

    async def ultra_clean_text(self, text: str) -> TextAnalysis:
        """ØªÙ†Ø¸ÙŠÙ ÙØ§Ø¦Ù‚ Ù„Ù„Ù†Øµ Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„"""
        if not text:
            return self._create_empty_analysis()
        
        original_length = len(text)
        original_text = text
        
        # 1. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
        for pattern in self.noise_patterns:
            text = re.sub(pattern, '', text, flags=re.MULTILINE | re.DOTALL)
        
        # 2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø·ÙˆØ±
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if self._is_useful_line(line):
                cleaned_lines.append(line)
        
        text = '\n'.join(cleaned_lines)
        
        # 3. ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª
        for pattern, replacement in self.improvement_patterns:
            text = re.sub(pattern, replacement, text, flags=re.MULTILINE)
        
        # 4. ØªÙ†Ø¸ÙŠÙ Ù†Ù‡Ø§Ø¦ÙŠ
        text = text.strip()
        
        # 5. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
        return await self._analyze_text(original_text, text, original_length)

    def _is_useful_line(self, line: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø·Ø± Ù…ÙÙŠØ¯Ø§Ù‹ Ø£Ù… Ù„Ø§"""
        if not line or len(line.strip()) < 3:
            return False
        
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·ÙˆØ± Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙˆØµÙ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ÙÙ‚Ø·
        if re.match(r'^[a-zA-Z\s\.,;:!?\-()0-9]+$', line):
            return False
            
        # ØªØ¬Ø§Ù‡Ù„ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
        if re.match(r'^\d+$|^page \d+|^\([lr]=[\d.]+', line):
            return False
            
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·ÙˆØ± Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØ§Øª ÙÙ‚Ø·
        if re.match(r'^[a-f0-9\-]+$', line):
            return False
            
        return True

    async def _analyze_text(self, original: str, cleaned: str, original_length: int) -> TextAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ"""
        cleaned_length = len(cleaned)
        reduction_percentage = ((original_length - cleaned_length) / original_length * 100) if original_length > 0 else 0
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ©
        language_analysis = self._detect_languages(cleaned)
        arabic_ratio = language_analysis['arabic_ratio']
        english_ratio = language_analysis['english_ratio']
        primary_language = language_analysis['primary_language']
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        educational_keywords = self._extract_educational_keywords(cleaned)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        technical_terms = self._extract_technical_terms(cleaned)
        
        # Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„ØµÙŠØº
        formulas_count = len(re.findall(r'=|âˆ‘|âˆ«|âˆš|Ï€|Î±|Î²|Î³|\+|\-|\*|Ã·', cleaned))
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_type = self._classify_content_type(cleaned, educational_keywords)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_score = self._calculate_quality_score(cleaned, arabic_ratio, educational_keywords, technical_terms)
        quality_rating = self._get_quality_rating(quality_score)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¨Ù„ÙŠØ© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©
        readability_score = self._calculate_readability(cleaned)
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙˆØ§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
        issues_found = self._find_issues(cleaned)
        suggestions = self._generate_suggestions(issues_found, quality_score, arabic_ratio)
        
        return TextAnalysis(
            original_length=original_length,
            cleaned_length=cleaned_length,
            reduction_percentage=reduction_percentage,
            quality_score=quality_score,
            content_type=content_type,
            quality_rating=quality_rating,
            arabic_ratio=arabic_ratio,
            educational_keywords=educational_keywords,
            technical_terms=technical_terms,
            formulas_count=formulas_count,
            issues_found=issues_found,
            suggestions=suggestions,
            readability_score=readability_score
        )

    def _extract_educational_keywords(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"""
        found_keywords = []
        text_lower = text.lower()
        
        for category, keywords in self.educational_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    found_keywords.append(keyword)
        
        return list(set(found_keywords))

    def _extract_technical_terms(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©"""
        # Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        tech_terms = [
            'Ø§Ù„Ø¬Ø²ÙŠØ¡', 'Ø§Ù„Ø°Ø±Ø©', 'Ø§Ù„ØªÙØ§Ø¹Ù„', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©', 'Ø§Ù„Ø®Ø§ØµÙŠØ©',
            'Ø§Ù„Ù†Ø³Ø¨Ø©', 'Ø§Ù„Ù…Ø¹Ø¯Ù„', 'Ø§Ù„Ø«Ø§Ø¨Øª', 'Ø§Ù„Ù…ØªØºÙŠØ±', 'Ø§Ù„Ø¯Ø§Ù„Ø©',
            'Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©', 'Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©', 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª', 'Ø§Ù„Ø´Ø¨ÙƒØ©', 'Ø§Ù„Ù†Ø¸Ø§Ù…'
        ]
        
        found_terms = []
        text_lower = text.lower()
        
        for term in tech_terms:
            if term in text_lower:
                found_terms.append(term)
        
        return found_terms

    def _classify_content_type(self, text: str, keywords: List[str]) -> TextType:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        text_lower = text.lower()
        
        # Ø¹Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        theory_count = sum(1 for kw in keywords if any(t in kw for t in self.educational_keywords['theory']))
        example_count = sum(1 for kw in keywords if any(t in kw for t in self.educational_keywords['example']))
        definition_count = sum(1 for kw in keywords if any(t in kw for t in self.educational_keywords['definition']))
        formula_count = sum(1 for kw in keywords if any(t in kw for t in self.educational_keywords['formula']))
        exercise_count = sum(1 for kw in keywords if any(t in kw for t in self.educational_keywords['exercise']))
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯
        counts = {
            TextType.THEORY: theory_count,
            TextType.EXAMPLE: example_count,
            TextType.DEFINITION: definition_count,
            TextType.FORMULA: formula_count,
            TextType.EXERCISE: exercise_count
        }
        
        max_type = max(counts.keys(), key=lambda k: counts[k])
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªÙ†ÙˆØ¹ ÙÙŠ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ØŒ ÙÙ‡Ùˆ Ù…Ø®ØªÙ„Ø·
        if sum(1 for c in counts.values() if c > 0) > 2:
            return TextType.MIXED
        
        return max_type

    def _calculate_quality_score(self, text: str, arabic_ratio: float, 
                                educational_keywords: List[str], technical_terms: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        score = 0.0
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (40%)
        score += arabic_ratio * 40
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© (30%)
        if len(educational_keywords) > 0:
            score += min(len(educational_keywords) * 5, 30)
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© (20%)
        if len(technical_terms) > 0:
            score += min(len(technical_terms) * 4, 20)
        
        # Ù†Ù‚Ø§Ø· Ù„Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ (10%)
        if 50 <= len(text) <= 1000:
            score += 10
        elif len(text) > 1000:
            score += 8
        elif len(text) > 20:
            score += 5
        
        return min(score, 100)

    def _get_quality_rating(self, score: float) -> ContentQuality:
        """ØªØ­Ø¯ÙŠØ¯ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        if score >= 85:
            return ContentQuality.EXCELLENT
        elif score >= 70:
            return ContentQuality.GOOD
        elif score >= 55:
            return ContentQuality.ACCEPTABLE
        elif score >= 35:
            return ContentQuality.POOR
        else:
            return ContentQuality.VERY_POOR

    def _calculate_readability(self, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        if not text:
            return 0.0
        
        sentences = len(re.findall(r'[.!?ØŸ]', text))
        words = len(text.split())
        
        if sentences == 0:
            return 50.0  # Ù…ØªÙˆØ³Ø· Ø§ÙØªØ±Ø§Ø¶ÙŠ
        
        avg_words_per_sentence = words / sentences
        
        # Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…Ø¨Ø³Ø·Ø© Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        readability = 100 - (avg_words_per_sentence * 1.5)
        
        return max(0, min(100, readability))

    def _find_issues(self, text: str) -> List[str]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù†Øµ"""
        issues = []
        
        if len(text) < 20:
            issues.append("Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹")
        
        if len(re.findall(r'[Ø£-ÙŠ]', text)) / len(text) < 0.3:
            issues.append("Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ù†Ø®ÙØ¶Ø©")
        
        if len(text.split()) < 5:
            issues.append("Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹")
        
        # ÙØ­Øµ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        if not re.search(r'[.!?ØŸØŒ]', text):
            issues.append("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ…")
        
        # ÙØ­Øµ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        if re.search(r'[Ø£-ÙŠ].*[0-9].*[Ø£-ÙŠ]', text):
            issues.append("Ø®Ù„Ø· Ø¨ÙŠÙ† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ")
        
        return issues

    def _generate_suggestions(self, issues: List[str], quality_score: float, arabic_ratio: float) -> List[str]:
        """Ø¥Ù†ØªØ§Ø¬ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        suggestions = []
        
        if quality_score < 50:
            suggestions.append("ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©")
        
        if arabic_ratio < 0.5:
            suggestions.append("Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø³ÙŠØ­Ø³Ù† Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©")
        
        if "Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹" in issues:
            suggestions.append("Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©")
        
        if "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ…" in issues:
            suggestions.append("Ø¥Ø¶Ø§ÙØ© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©")
        
        if not suggestions:
            suggestions.append("Ø§Ù„Ù†Øµ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø© ÙˆÙ„Ø§ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†Ø§Øª ÙƒØ¨ÙŠØ±Ø©")
        
        return suggestions

    def _create_empty_analysis(self) -> TextAnalysis:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ ÙØ§Ø±Øº"""
        return TextAnalysis(
            original_length=0,
            cleaned_length=0,
            reduction_percentage=0.0,
            quality_score=0.0,
            content_type=TextType.MIXED,
            quality_rating=ContentQuality.VERY_POOR,
            arabic_ratio=0.0,
            educational_keywords=[],
            technical_terms=[],
            formulas_count=0,
            issues_found=["Ø§Ù„Ù†Øµ ÙØ§Ø±Øº"],
            suggestions=["Ø¥Ø¶Ø§ÙØ© Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"],
            readability_score=0.0
        )

    async def process_multiple_files(self, file_paths: List[str]) -> Dict[str, Tuple[str, TextAnalysis]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ"""
        results = {}
        
        async def process_single(file_path: str):
            try:
                if not Path(file_path).exists():
                    return file_path, ("", self._create_empty_analysis())
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                analysis = await self.ultra_clean_text(content)
                return file_path, (content, analysis)
            except Exception as e:
                return file_path, ("", self._create_empty_analysis())
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
        tasks = [process_single(fp) for fp in file_paths]
        task_results = await asyncio.gather(*tasks)
        
        for file_path, (content, analysis) in task_results:
            results[file_path] = (content, analysis)
        
        return results

    async def process_folder(self, folder_path: str, file_extensions: List[str] = ['.txt', '.md']) -> Dict[str, Tuple[str, TextAnalysis]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„"""
        folder = Path(folder_path)
        if not folder.exists():
            return {}
        
        file_paths = []
        for ext in file_extensions:
            file_paths.extend([str(f) for f in folder.rglob(f'*{ext}')])
        
        return await self.process_multiple_files(file_paths)

    def compare_texts_ultra(self, text1: str, text2: str) -> Dict[str, Any]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© ÙØ§Ø¦Ù‚Ø© Ø¨ÙŠÙ† Ù†ØµÙŠÙ†"""
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø£ÙˆÙ„Ø§Ù‹
        import asyncio
        loop = asyncio.get_event_loop()
        
        analysis1 = loop.run_until_complete(self.ultra_clean_text(text1))
        analysis2 = loop.run_until_complete(self.ultra_clean_text(text2))
        
        # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
        similarity = difflib.SequenceMatcher(None, text1, text2).ratio() * 100
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª
        diff = list(difflib.unified_diff(
            text1.splitlines(),
            text2.splitlines(),
            lineterm='',
            fromfile='Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„',
            tofile='Ø§Ù„Ù†Øµ Ø§Ù„Ø«Ø§Ù†ÙŠ'
        ))
        
        return {
            'similarity_percentage': similarity,
            'text1_analysis': asdict(analysis1),
            'text2_analysis': asdict(analysis2),
            'quality_difference': abs(analysis1.quality_score - analysis2.quality_score),
            'content_type_match': analysis1.content_type == analysis2.content_type,
            'differences_count': len(diff),
            'differences': diff[:10],  # Ø£ÙˆÙ„ 10 Ø§Ø®ØªÙ„Ø§ÙØ§Øª
            'recommendation': self._get_comparison_recommendation(analysis1, analysis2, similarity)
        }

    def _get_comparison_recommendation(self, analysis1: TextAnalysis, analysis2: TextAnalysis, similarity: float) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        if similarity > 85:
            return "Ø§Ù„Ù†ØµØ§Ù† Ù…ØªØ´Ø§Ø¨Ù‡Ø§Ù† Ø¬Ø¯Ø§Ù‹ - Ù…Ù‚Ø§Ø±Ù†Ø© Ù…ÙˆØ«ÙˆÙ‚Ø©"
        elif similarity > 70:
            return "Ø§Ù„Ù†ØµØ§Ù† Ù…ØªØ´Ø§Ø¨Ù‡Ø§Ù† Ø¥Ù„Ù‰ Ø­Ø¯ ÙƒØ¨ÙŠØ± - Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬ÙŠØ¯Ø©"
        elif similarity > 50:
            return "Ø§Ù„Ù†ØµØ§Ù† Ù„Ø¯ÙŠÙ‡Ù…Ø§ ØªØ´Ø§Ø¨Ù‡ Ù…ØªÙˆØ³Ø· - Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ù‚Ø¨ÙˆÙ„Ø©"
        elif analysis1.content_type == analysis2.content_type:
            return "Ø§Ù„Ù†ØµØ§Ù† Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù„ÙƒÙ† Ù…Ø®ØªÙ„ÙØ§Ù† ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"
        else:
            return "Ø§Ù„Ù†ØµØ§Ù† Ù…Ø®ØªÙ„ÙØ§Ù† Ø¬Ø¯Ø§Ù‹ - Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"

    def generate_detailed_report(self, analysis: TextAnalysis, original_text: str = "", cleaned_text: str = "") -> str:
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        report = f"""
# ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØµÙ„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
- **Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ**: {analysis.original_length:,} Ø­Ø±Ù
- **Ø§Ù„Ø·ÙˆÙ„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ**: {analysis.cleaned_length:,} Ø­Ø±Ù
- **Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ®ÙÙŠØ¶**: {analysis.reduction_percentage:.1f}%
- **Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ**: {analysis.arabic_ratio:.1f}%

## ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©
- **Ø§Ù„Ù†Ù‚Ø§Ø·**: {analysis.quality_score:.1f}/100
- **Ø§Ù„ØªÙ‚ÙŠÙŠÙ…**: {analysis.quality_rating.value}
- **Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©**: {analysis.readability_score:.1f}/100

## ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
- **Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰**: {analysis.content_type.value}
- **Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª**: {analysis.formulas_count}

## Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
{', '.join(analysis.educational_keywords) if analysis.educational_keywords else 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©'}

## Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
{', '.join(analysis.technical_terms) if analysis.technical_terms else 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ØµØ·Ù„Ø­Ø§Øª ØªÙ‚Ù†ÙŠØ©'}

## Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
"""
        
        for issue in analysis.issues_found:
            report += f"âŒ {issue}\n"
        
        report += "\n## Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†\n"
        for suggestion in analysis.suggestions:
            report += f"ğŸ’¡ {suggestion}\n"
        
        if cleaned_text:
            report += f"\n## Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n```\n{cleaned_text[:500]}{'...' if len(cleaned_text) > 500 else ''}\n```"
        
        report += "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        
        return report

    async def process_file(self, file_path: str) -> Tuple[str, TextAnalysis]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯"""
        try:
            if not Path(file_path).exists():
                return "", self._create_empty_analysis()
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            analysis = await self.ultra_clean_text(content)
            return content, analysis
        except Exception as e:
            return "", self._create_empty_analysis()

    def generate_report(self, analysis: TextAnalysis, filename: str = "") -> str:
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ù…Ø¨Ø³Ø·"""
        return f"""
ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {filename}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:
   Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ: {analysis.original_length:,} Ø­Ø±Ù
   Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ù†Ø¸Ù: {analysis.cleaned_length:,} Ø­Ø±Ù
   Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ®ÙÙŠØ¶: {analysis.reduction_percentage:.1f}%

ğŸ¯ Ø§Ù„Ø¬ÙˆØ¯Ø©: {analysis.quality_score:.1f}/100 ({analysis.quality_rating.value})
ğŸ“– Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {analysis.content_type.value}
ğŸ”¤ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {analysis.arabic_ratio:.1f}%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    async def process_directory(self, directory: str, extensions: Optional[List[str]] = None) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„"""
        if extensions is None:
            extensions = ['.txt', '.md', '.json']
        
        results = await self.process_folder(directory, extensions)
        
        total_files = len(results)
        processed_files = len([r for r in results.values() if r[1].original_length > 0])
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        total_original = sum(r[1].original_length for r in results.values())
        total_cleaned = sum(r[1].cleaned_length for r in results.values())
        avg_quality = sum(r[1].quality_score for r in results.values()) / total_files if total_files > 0 else 0
        
        return {
            'total_files': total_files,
            'processed': processed_files,
            'total_original_length': total_original,
            'total_cleaned_length': total_cleaned,
            'overall_reduction': ((total_original - total_cleaned) / total_original * 100) if total_original > 0 else 0,
            'average_quality': avg_quality,
            'results': results
        }

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹
async def ultra_process_single_file(file_path: str) -> Tuple[str, TextAnalysis, str]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
    processor = UltraTextProcessor()
    original_text, analysis = await processor.process_file(file_path)
    report = processor.generate_report(analysis, Path(file_path).name)
    return original_text, analysis, report

async def ultra_process_directory(directory: str, extensions: Optional[List[str]] = None) -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„"""
    processor = UltraTextProcessor()
    return await processor.process_directory(directory, extensions)

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    async def test_ultra_processor():
        # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯
        test_file = "backend/uploads/landingai_results/extraction_20250705_172654/extracted_content.md"
        if Path(test_file).exists():
            original, analysis, report = await ultra_process_single_file(test_file)
            print(report)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯
        test_dir = "backend/uploads/landingai_results"
        if Path(test_dir).exists():
            results = await ultra_process_directory(test_dir)
            print(f"\nØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {results['processed']} Ù…Ù„Ù Ù…Ù† Ø£ØµÙ„ {results['total_files']}")
    
    asyncio.run(test_ultra_processor())
    
    async def test_landing_ai_optimization():
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ø³ÙŠÙ† Ù…Ø®Ø±Ø¬Ø§Øª Landing AI"""
        
        # Ù…Ù„Ù Landing AI Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨
        landing_ai_file = "backend/uploads/landingai_results/extraction_20250705_172654/agentic_doc_result.json"
        
        if Path(landing_ai_file).exists():
            print("ğŸ” Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Landing AI...")
            result = await process_landing_ai_file(landing_ai_file)
            
            if 'error' not in result:
                print(f"âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ø¨Ù†Ø¬Ø§Ø­")
                print(f"ğŸ“Š Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {result['optimization_stats']['original_length']} Ø­Ø±Ù")
                print(f"ğŸ“Š Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù†: {result['optimization_stats']['optimized_length']} Ø­Ø±Ù")
                print(f"ğŸ“Š Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†: {result['optimization_stats']['reduction_percentage']:.1f}%")
                print(f"ğŸ¯ Ø¬Ø§Ù‡Ø²ÙŠØ© Gemini: {result['optimization_stats']['gemini_readiness_score']:.1f}/100")
                
                # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                output_file = "optimized_landing_ai_output.txt"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(result['optimized_text'])
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø³Ù† ÙÙŠ: {output_file}")
            else:
                print(f"âŒ Ø®Ø·Ø£: {result['error']}")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results_dir = "backend/uploads/landingai_results"
        if Path(results_dir).exists():
            print(f"\nğŸ“ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {results_dir}")
            folder_results = await ultra_process_directory(results_dir, ['.md', '.json', '.txt'])
            
            print(f"ğŸ“Š ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {folder_results['processed']} Ù…Ù„Ù Ù…Ù† Ø£ØµÙ„ {folder_results['total_files']}")
            print(f"ğŸ“Š Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©: {folder_results['average_quality']:.1f}/100")
            print(f"ğŸ“Š Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ®ÙÙŠØ¶ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {folder_results['overall_reduction']:.1f}%")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_landing_ai_optimization())

# Ø¯ÙˆØ§Ù„ Ø¥Ø¶Ø§ÙÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Landing AI
async def process_landing_ai_file(file_path: str) -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù…Ù„Ù Landing AI JSON"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        extracted_text = extract_text_from_landing_ai(data)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ù„Ù€ Gemini
        processor = UltraTextProcessor()
        optimized_text = await optimize_for_gemini(extracted_text, processor)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
        original_length = len(extracted_text)
        optimized_length = len(optimized_text)
        reduction_percentage = ((original_length - optimized_length) / original_length * 100) if original_length > 0 else 0
        
        # ØªÙ‚ÙŠÙŠÙ… Ø¬Ø§Ù‡Ø²ÙŠØ© Gemini
        gemini_score = calculate_gemini_readiness(optimized_text)
        
        return {
            'original_text': extracted_text,
            'optimized_text': optimized_text,
            'optimization_stats': {
                'original_length': original_length,
                'optimized_length': optimized_length,
                'reduction_percentage': reduction_percentage,
                'gemini_readiness_score': gemini_score
            }
        }
    except Exception as e:
        return {'error': str(e)}

def extract_text_from_landing_ai(data: Dict) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Landing AI"""
    text_parts = []
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    if 'agentic_doc_result' in data:
        result = data['agentic_doc_result']
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
        if 'extracted_text' in result:
            text_parts.append(result['extracted_text'])
        
        # ÙˆØµÙ Ø§Ù„ØµÙˆØ± (Ù…Ø¨Ø³Ø·)
        if 'image_descriptions' in result:
            for desc in result['image_descriptions']:
                if len(desc) < 200:  # ÙÙ‚Ø· Ø§Ù„Ø£ÙˆØµØ§Ù Ø§Ù„Ù…Ø®ØªØµØ±Ø©
                    text_parts.append(f"[ØµÙˆØ±Ø©: {desc}]")
        
        # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª
        if 'pages' in result:
            for page in result['pages']:
                if 'text_content' in page:
                    text_parts.append(page['text_content'])
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
    elif 'extracted_text' in data:
        text_parts.append(data['extracted_text'])
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
    elif 'result' in data:
        if isinstance(data['result'], str):
            text_parts.append(data['result'])
        elif isinstance(data['result'], dict):
            for key, value in data['result'].items():
                if isinstance(value, str) and len(value) > 50:
                    text_parts.append(value)
    
    return '\n'.join(text_parts)

async def optimize_for_gemini(text: str, processor: UltraTextProcessor) -> str:
    """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ù„ÙŠÙƒÙˆÙ† Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ Gemini"""
    # ØªÙ†Ø¸ÙŠÙ Ø£ÙˆÙ„ÙŠ
    analysis = await processor.ultra_clean_text(text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    lines = text.split('\n')
    optimized_lines = []
    
    for line in lines:
        line = line.strip()
        
        # ØªØ®Ø·ÙŠ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
        if not line or len(line) > 500:
            continue
        
        # ØªØ®Ø·ÙŠ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        if any(keyword in line.lower() for keyword in [
            'metadata', 'coordinate', 'pixel', 'rgb', 'hex',
            'json', 'array', 'object', 'null', 'undefined'
        ]):
            continue
        
        # ØªØ¨Ø³ÙŠØ· Ø£ÙˆØµØ§Ù Ø§Ù„ØµÙˆØ±
        if line.startswith('[ØµÙˆØ±Ø©:'):
            # Ø§Ø®ØªØµØ§Ø± Ø§Ù„ÙˆØµÙ
            if len(line) > 100:
                line = line[:97] + '...]'
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…Ø·ÙˆÙ„Ø©
        if re.search(r'\d{5,}', line) or line.count('=') > 3:
            continue
        
        optimized_lines.append(line)
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
    final_lines = []
    prev_line = ""
    
    for line in optimized_lines:
        # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        if line != prev_line and not (len(line) < 10 and line in prev_line):
            final_lines.append(line)
        prev_line = line
    
    return '\n'.join(final_lines)

def calculate_gemini_readiness(text: str) -> float:
    """Ø­Ø³Ø§Ø¨ Ù…Ø¯Ù‰ Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ù†Øµ Ù„Ù€ Gemini"""
    if not text:
        return 0
    
    score = 100
    
    # Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
    length = len(text)
    if length > 10000:
        score -= 20
    elif length > 5000:
        score -= 10
    elif length < 100:
        score -= 30
    
    # Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    arabic_chars = len(re.findall(r'[Ø£-ÙŠ]', text))
    arabic_ratio = arabic_chars / length if length > 0 else 0
    
    if arabic_ratio < 0.3:
        score -= 15
    elif arabic_ratio > 0.8:
        score += 10
    
    # ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ ØªØ¹Ù„ÙŠÙ…ÙŠ
    educational_keywords = [
        'ØªØ¹Ù„ÙŠÙ…', 'Ø¯Ø±Ø§Ø³Ø©', 'ØªØ¹Ù„Ù…', 'Ø´Ø±Ø­', 'ØªÙˆØ¶ÙŠØ­',
        'Ù…ÙÙ‡ÙˆÙ…', 'Ø¯Ø±Ø³', 'Ù…Ø­Ø§Ø¶Ø±Ø©', 'ÙƒØªØ§Ø¨', 'Ù…Ù‚Ø±Ø±'
    ]
    
    found_keywords = sum(1 for keyword in educational_keywords if keyword in text)
    if found_keywords == 0:
        score -= 10
    else:
        score += min(found_keywords * 2, 10)
    
    # ÙˆØ¶ÙˆØ­ Ø§Ù„Ø¨Ù†ÙŠØ©
    if text.count('\n') < 3:
        score -= 5
    
    # ØªØ¬Ù†Ø¨ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
    if text.count('{') > 5 or text.count('[') > 10:
        score -= 15
    
    return max(0, min(100, score))

# Ø¥Ø¶Ø§ÙØ© ÙØ¦Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
class EnhancedComparison:
    """ÙØ¦Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
    
    def __init__(self):
        self.processor = UltraTextProcessor()
    
    async def compare_educational_content(self, text1: str, text2: str) -> Dict[str, Any]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ
        analysis1 = await self.processor.ultra_clean_text(text1)
        analysis2 = await self.processor.ultra_clean_text(text2)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        common_keywords = set(analysis1.educational_keywords) & set(analysis2.educational_keywords)
        keyword_similarity = len(common_keywords) / max(len(analysis1.educational_keywords), len(analysis2.educational_keywords), 1) * 100
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        common_terms = set(analysis1.technical_terms) & set(analysis2.technical_terms)
        terms_similarity = len(common_terms) / max(len(analysis1.technical_terms), len(analysis2.technical_terms), 1) * 100
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_type_match = analysis1.content_type == analysis2.content_type
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        quality_similarity = 100 - abs(analysis1.quality_score - analysis2.quality_score)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        overall_similarity = (keyword_similarity + terms_similarity + quality_similarity) / 3
        
        return {
            'overall_similarity': overall_similarity,
            'keyword_similarity': keyword_similarity,
            'terms_similarity': terms_similarity,
            'quality_similarity': quality_similarity,
            'content_type_match': content_type_match,
            'common_keywords': list(common_keywords),
            'common_terms': list(common_terms),
            'quality_difference': abs(analysis1.quality_score - analysis2.quality_score),
            'recommendation': self._get_enhanced_recommendation(overall_similarity, content_type_match),
            'detailed_analysis': {
                'text1_stats': {
                    'length': analysis1.cleaned_length,
                    'quality': analysis1.quality_score,
                    'type': analysis1.content_type.value,
                    'arabic_ratio': analysis1.arabic_ratio
                },
                'text2_stats': {
                    'length': analysis2.cleaned_length,
                    'quality': analysis2.quality_score,
                    'type': analysis2.content_type.value,
                    'arabic_ratio': analysis2.arabic_ratio
                }
            }
        }
    
    def _get_enhanced_recommendation(self, similarity: float, type_match: bool) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ø­Ø³Ù† Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        if similarity > 80 and type_match:
            return "Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ·Ø§Ø¨Ù‚ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ - Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ù…ØªØ§Ø²Ø©"
        elif similarity > 60 and type_match:
            return "Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ - Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬ÙŠØ¯Ø©"
        elif similarity > 40:
            return "ØªØ´Ø§Ø¨Ù‡ Ù…ØªÙˆØ³Ø· - Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©"
        elif type_match:
            return "Ù†ÙØ³ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„ÙƒÙ† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø®ØªÙ„ÙØ©"
        else:
            return "Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªÙ„Ù - ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"

# Ø¯Ø§Ù„Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
async def quick_process_and_compare(file1: str, file2: str) -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù…Ù„ÙÙŠÙ†"""
    processor = UltraTextProcessor()
    comparator = EnhancedComparison()
    
    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
        with open(file1, 'r', encoding='utf-8') as f:
            text1 = f.read()
        
        with open(file2, 'r', encoding='utf-8') as f:
            text2 = f.read()
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„
        analysis1 = await processor.ultra_clean_text(text1)
        analysis2 = await processor.ultra_clean_text(text2)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø­Ø³Ù†Ø©
        comparison = await comparator.compare_educational_content(text1, text2)
        
        return {
            'file1_analysis': asdict(analysis1),
            'file2_analysis': asdict(analysis2),
            'comparison_results': comparison,
            'processing_time': time.time(),
            'success': True
        }
    
    except Exception as e:
        return {
            'error': str(e),
            'success': False
        }

# Ø¯Ø§Ù„Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
async def advanced_folder_processing(folder_path: str, output_dir: str = "processed_results") -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ø¹ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    
    processor = UltraTextProcessor()
    results_dir = Path(output_dir)
    results_dir.mkdir(exist_ok=True)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯
    folder_results = await processor.process_directory(folder_path)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø©
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
    summary_file = results_dir / f"processing_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': timestamp,
            'folder_path': folder_path,
            'total_files': folder_results['total_files'],
            'processed_files': folder_results['processed'],
            'overall_reduction': folder_results['overall_reduction'],
            'average_quality': folder_results['average_quality']
        }, f, ensure_ascii=False, indent=2)
    
    # Ø­ÙØ¸ ØªÙØ§ØµÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù
    details_file = results_dir / f"detailed_results_{timestamp}.json"
    serializable_results = {}
    
    for file_path, (content, analysis) in folder_results['results'].items():
        serializable_results[file_path] = {
            'analysis': asdict(analysis),
            'content_preview': content[:500] + '...' if len(content) > 500 else content
        }
    
    with open(details_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    
    return {
        'summary': folder_results,
        'summary_file': str(summary_file),
        'details_file': str(details_file),
        'timestamp': timestamp
    }
