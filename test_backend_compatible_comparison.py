#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯
Enhanced Visual Comparison Test with Backend Results Validation
"""

import cv2
import numpy as np
import imagehash
from PIL import Image, ImageEnhance, ImageOps
from skimage.metrics import structural_similarity as ssim
import json
import time
import requests
import os
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import base64
from datetime import datetime

class EnhancedVisualComparisonTester:
    """ÙØ¦Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯"""
    
    def __init__(self, backend_url: str = "http://localhost:8000"):
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© (Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)
        self.weights = {
            'ssim': 0.25,
            'phash': 0.15,
            'clip': 0.25,  # Ø³ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„Ù‡ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ
            'histogram': 0.10,
            'features': 0.15,
            'edges': 0.10
        }
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø¯ÙˆÙ† CLIP
        remaining_weight = 1.0 - self.weights['clip']
        factor = 1.0 / remaining_weight
        for key in ['ssim', 'phash', 'histogram', 'features', 'edges']:
            self.weights[key] *= factor
        self.weights['clip'] = 0.0
        
        self.backend_url = backend_url
        self.similarity_threshold = 0.75
        self.high_similarity_threshold = 0.90
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ SIFT Ù„Ù„Ù…ÙŠØ²Ø§Øª
        try:
            self.sift = cv2.SIFT_create(nfeatures=1000)
            self.feature_matching_available = True
        except Exception:
            self.feature_matching_available = False
            
        print(f"ğŸ”§ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©:")
        for key, value in self.weights.items():
            print(f"   {key.upper()}: {value:.2f}")
    
    def load_and_preprocess_image(self, image_path: str) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ OpenCV
            img_cv = cv2.imread(str(image_path))
            if img_cv is None:
                raise ValueError(f"ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©: {image_path}")
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB
            img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)
            
            # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ PIL
            img_pil = Image.open(image_path)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø±Ù…Ø§Ø¯ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)
            img_gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
            img_gray = cv2.GaussianBlur(img_gray, (3, 3), 0)  # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¬ÙˆØ¯Ø©
            
            return {
                'cv': img_cv,
                'rgb': img_rgb,
                'pil': img_pil,
                'gray': img_gray,
                'shape': img_cv.shape,
                'size': img_pil.size
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {image_path}: {e}")
            return None
    
    def resize_images_to_match(self, img1: Dict, img2: Dict) -> Tuple[Dict, Dict]:
        """ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù„ØªØªØ·Ø§Ø¨Ù‚ (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        h1, w1 = img1['gray'].shape
        h2, w2 = img2['gray'].shape
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ØµØºØ± Ø­Ø¬Ù… Ù…Ø´ØªØ±Ùƒ (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)
        target_h = min(h1, h2)
        target_w = min(w1, w2)
        
        def resize_image_dict(img_dict, target_size):
            result = {}
            for key, img in img_dict.items():
                if key in ['shape', 'size']:
                    continue
                elif key == 'pil':
                    result[key] = img.resize((target_w, target_h), Image.Resampling.LANCZOS)
                elif len(img.shape) == 2:  # grayscale
                    result[key] = cv2.resize(img, (target_w, target_h))
                else:  # color
                    result[key] = cv2.resize(img, (target_w, target_h))
            
            result['shape'] = (target_h, target_w, 3)
            result['size'] = (target_w, target_h)
            return result
        
        img1_resized = resize_image_dict(img1, (target_w, target_h))
        img2_resized = resize_image_dict(img2, (target_w, target_h))
        
        return img1_resized, img2_resized
    
    def calculate_ssim(self, img1_gray: np.ndarray, img2_gray: np.ndarray) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ SSIM (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        try:
            # Ø­Ø³Ø§Ø¨ SSIM Ù…Ø¹ full=True Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø±ÙŠØ·Ø©
            score, ssim_map = ssim(img1_gray, img2_gray, full=True)
            score = max(0.0, min(1.0, score))  # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ÙŠÙ† 0 Ùˆ 1
            
            return {
                'score': float(score),
                'map_available': True,
                'mean_map': float(np.mean(ssim_map)),
                'std_map': float(np.std(ssim_map)),
                'min_map': float(np.min(ssim_map)),
                'max_map': float(np.max(ssim_map))
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ SSIM: {e}")
            return {'score': 0.0, 'error': str(e)}
    
    def calculate_phash_similarity(self, img1_pil: Image.Image, img2_pil: Image.Image) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ pHash (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        try:
            # Ø­Ø³Ø§Ø¨ pHash
            hash1 = imagehash.phash(img1_pil)
            hash2 = imagehash.phash(img2_pil)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
            distance = hash1 - hash2
            max_distance = 64  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù…Ø³Ø§ÙØ© pHash
            similarity = 1.0 - (distance / max_distance)
            similarity = max(0.0, min(1.0, similarity))
            
            return {
                'score': float(similarity),
                'hash1': str(hash1),
                'hash2': str(hash2),
                'distance': int(distance),
                'max_distance': max_distance
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ pHash: {e}")
            return {'score': 0.0, 'error': str(e)}
    
    def calculate_histogram_correlation(self, img1: np.ndarray, img2: np.ndarray) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù… (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ grayscale Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY) if len(img1.shape) == 3 else img1
            gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY) if len(img2.shape) == 3 else img2
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
            hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])
            hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])
            
            # Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
            correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
            correlation = max(0.0, min(1.0, correlation))
            
            return {
                'score': float(correlation),
                'method': 'HISTCMP_CORREL',
                'hist1_mean': float(np.mean(hist1)),
                'hist2_mean': float(np.mean(hist2))
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…: {e}")
            return {'score': 0.0, 'error': str(e)}
    
    def calculate_feature_matching(self, img1_gray: np.ndarray, img2_gray: np.ndarray) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        if not self.feature_matching_available:
            return {'score': 0.0, 'error': 'SIFT not available'}
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø´Ù SIFT
            kp1, des1 = self.sift.detectAndCompute(img1_gray, None)
            kp2, des2 = self.sift.detectAndCompute(img2_gray, None)
            
            if des1 is None or des2 is None:
                return {
                    'score': 0.0,
                    'keypoints1': len(kp1) if kp1 else 0,
                    'keypoints2': len(kp2) if kp2 else 0,
                    'error': 'No descriptors found'
                }
            
            # Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
            matches = bf.match(des1, des2)
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)
            if len(matches) > 0:
                distances = [m.distance for m in matches]
                feature_matching_score = 1.0 - (np.mean(distances) / 256.0)
                feature_matching_score = max(0.0, min(1.0, feature_matching_score))
            else:
                feature_matching_score = 0.0
            
            return {
                'score': float(feature_matching_score),
                'total_matches': len(matches),
                'keypoints1': len(kp1),
                'keypoints2': len(kp2),
                'avg_distance': float(np.mean(distances) if matches else 0),
                'min_distance': float(np.min(distances) if matches else 0),
                'max_distance': float(np.max(distances) if matches else 0)
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª: {e}")
            return {'score': 0.0, 'error': str(e)}
    
    def calculate_edge_similarity(self, img1_gray: np.ndarray, img2_gray: np.ndarray) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­ÙˆØ§Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Canny
            edges1 = cv2.Canny(img1_gray, 100, 200)
            edges2 = cv2.Canny(img2_gray, 100, 200)
            
            # Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… matchShapes (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)
            similarity = cv2.matchShapes(edges1, edges2, cv2.CONTOURS_MATCH_I2, 0)
            similarity = max(0.0, min(1.0, similarity))
            
            return {
                'score': float(similarity),
                'edges1_pixels': int(np.sum(edges1 > 0)),
                'edges2_pixels': int(np.sum(edges2 > 0)),
                'method': 'CONTOURS_MATCH_I2'
            }
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù: {e}")
            return {'score': 0.0, 'error': str(e)}
    
    def run_comprehensive_comparison(self, image1_path: str, image2_path: str) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ø¨ÙŠÙ† ØµÙˆØ±ØªÙŠÙ† (Ù…Ø­Ø³Ù†)"""
        start_time = time.time()
        
        print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©...")
        print(f"ğŸ“· Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: {image1_path}")
        print(f"ğŸ“· Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: {image2_path}")
        print("-" * 70)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        img1 = self.load_and_preprocess_image(image1_path)
        img2 = self.load_and_preprocess_image(image2_path)
        
        if not img1 or not img2:
            return None
        
        print(f"ğŸ“ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: {img1['shape']}")
        print(f"ğŸ“ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: {img2['shape']}")
        
        # ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ù„Ù„ØªØ·Ø§Ø¨Ù‚
        img1_resized, img2_resized = self.resize_images_to_match(img1, img2)
        print(f"ğŸ“ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: {img1_resized['shape']}")
        print("-" * 70)
        
        # Ø¥Ø¬Ø±Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª
        results = {}
        
        print("ğŸ”¬ Ø­Ø³Ø§Ø¨ SSIM...")
        results['ssim'] = self.calculate_ssim(img1_resized['gray'], img2_resized['gray'])
        print(f"   âœ… SSIM Score: {results['ssim']['score']:.4f}")
        
        print("ğŸ”¬ Ø­Ø³Ø§Ø¨ Perceptual Hash...")
        results['phash'] = self.calculate_phash_similarity(img1_resized['pil'], img2_resized['pil'])
        print(f"   âœ… pHash Score: {results['phash']['score']:.4f}")
        
        print("ğŸ”¬ Ø­Ø³Ø§Ø¨ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…...")
        results['histogram'] = self.calculate_histogram_correlation(img1_resized['gray'], img2_resized['gray'])
        print(f"   âœ… Histogram Correlation: {results['histogram']['score']:.4f}")
        
        print("ğŸ”¬ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª...")
        results['features'] = self.calculate_feature_matching(img1_resized['gray'], img2_resized['gray'])
        print(f"   âœ… Feature Matching Score: {results['features']['score']:.4f}")
        
        print("ğŸ”¬ Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø­ÙˆØ§Ù...")
        results['edges'] = self.calculate_edge_similarity(img1_resized['gray'], img2_resized['gray'])
        print(f"   âœ… Edge Similarity Score: {results['edges']['score']:.4f}")
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© (Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯)
        overall_score = (
            results['ssim']['score'] * self.weights['ssim'] +
            results['phash']['score'] * self.weights['phash'] +
            results['histogram']['score'] * self.weights['histogram'] +
            results['features']['score'] * self.weights['features'] +
            results['edges']['score'] * self.weights['edges']
        ) * 100
        
        processing_time = time.time() - start_time
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_results = {
            'overall_score': overall_score,
            'processing_time': processing_time,
            'detailed_scores': results,
            'weights_used': self.weights,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return final_results

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù†"""
    # Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ±
    image1_path = Path("104.jpg")
    image2_path = Path("101.jpg")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ØµÙˆØ±
    if not image1_path.exists():
        print(f"âŒ Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {image1_path}")
        return
    
    if not image2_path.exists():
        print(f"âŒ Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {image2_path}")
        return
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØ®ØªØ¨Ø± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ø³Ù†
    tester = EnhancedVisualComparisonTester()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©
    print("ğŸ”¬ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")
    local_results = tester.run_comprehensive_comparison(str(image1_path), str(image2_path))
    
    if local_results:
        print("\n" + "=" * 80)
        print("ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©")
        print("=" * 80)
        print(f"ğŸ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {local_results['overall_score']:.2f}%")
        print(f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {local_results['processing_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        # Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
        details = local_results['detailed_scores']
        print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ:")
        print(f"   ğŸ“Š SSIM: {details['ssim']['score']:.4f}")
        print(f"   ğŸ” pHash: {details['phash']['score']:.4f}")
        print(f"   ğŸ“ˆ Histogram: {details['histogram']['score']:.4f}")
        print(f"   ğŸ¯ Features: {details['features']['score']:.4f}")
        print(f"   ğŸ“ Edges: {details['edges']['score']:.4f}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
        output_file = f"enhanced_backend_compatible_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(local_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© ÙÙŠ: {output_file}")

if __name__ == "__main__":
    main()
