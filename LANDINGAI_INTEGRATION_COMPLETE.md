# Ø¯Ù„ÙŠÙ„ ØªÙƒØ§Ù…Ù„ LandingAI Agentic Document Extraction Ø§Ù„Ø´Ø§Ù…Ù„
# Complete LandingAI Agentic Document Extraction Integration Guide

## ğŸ“‹ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© / Overview

LandingAI Agentic Document Extraction (ADE) Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¨ØµØ±ÙŠØ§Ù‹ Ù…Ø«Ù„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ù…Ø®Ø·Ø·Ø§Øª ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬. ÙŠØªØ¬Ø§ÙˆØ² ØªÙ‚Ù†ÙŠØ§Øª OCR Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø¨ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¨ØµØ±ÙŠ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ©.

### ğŸŒŸ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© / Core Features

1. **Layout-agnostic parsing**: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø·ÙŠØ·Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ø¯ÙˆÙ† Ù‚ÙˆØ§Ù„Ø¨ Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨
2. **Element detection**: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© (Ù†ØµØŒ Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ù†Ù…Ø§Ø°Ø¬ØŒ checkboxes)
3. **Hierarchical relationships**: ÙÙ‡Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù†Ø§ØµØ±
4. **Visual grounding**: ØªÙˆÙÙŠØ± Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù„ÙƒÙ„ Ø¹Ù†ØµØ± Ù…Ø³ØªØ®Ø±Ø¬
5. **Multiple formats**: Ø¯Ø¹Ù… PDF ÙˆØµÙŠØº Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
6. **Structured output**: Ø¥Ø®Ø±Ø§Ø¬ JSON Ùˆ Markdown Ù…Ù†Ø¸Ù…

## ğŸš€ Ø§Ù„ØªØ«Ø¨ÙŠØª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ / Installation & Setup

### 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø© / Library Installation

```bash
pip install agentic-doc
```

### 2. Ø¥Ø¹Ø¯Ø§Ø¯ API Key

```bash
# Set environment variable
export VISION_AGENT_API_KEY=<your-api-key>

# Or create .env file
echo "VISION_AGENT_API_KEY=<your-api-key>" > .env
```

### 3. Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… / System Requirements

- Python 3.9, 3.10, 3.11, or 3.12
- OpenCV-Python (cv2)
- LandingAI API Key

## ğŸ“ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø© / Library Usage

### Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ / Basic Extraction

```python
from agentic_doc.parse import parse

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ù…Ù„Ù Ù…Ø­Ù„ÙŠ / Parse local file
result = parse("path/to/document.pdf")

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ / Get results
markdown_content = result[0].markdown
structured_chunks = result[0].chunks

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† URL
result = parse("https://example.com/document.pdf")
```

### Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª / Multiple Files Processing

```python
from agentic_doc.parse import parse

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© / Process multiple files
file_paths = [
    "path/to/document1.pdf",
    "path/to/document2.pdf",
    "path/to/image.png"
]

results = parse(file_paths)

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ / Save results
results = parse(
    file_paths,
    result_save_dir="path/to/save/results"
)

for result in results:
    print(f"File processed: {result.result_path}")
```

### Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…Ø© / Structured Data Extraction

```python
from pydantic import BaseModel, Field
from agentic_doc.parse import parse

class EducationalContent(BaseModel):
    chapter_title: str = Field(description="Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙØµÙ„")
    lesson_objectives: list[str] = Field(description="Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø¯Ø±Ø³")
    key_concepts: list[str] = Field(description="Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")
    exercises_count: int = Field(description="Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ†")
    difficulty_level: str = Field(description="Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©")

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†ÙŠ Ù…Ù†Ø¸Ù… / Structured extraction
results = parse(
    "curriculum_document.pdf",
    extraction_model=EducationalContent
)

extracted_fields = results[0].extraction
print(f"Chapter: {extracted_fields.chapter_title}")
print(f"Objectives: {extracted_fields.lesson_objectives}")
```

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© / Large Files Processing

```python
from agentic_doc.parse import parse

# Ù…Ø¹Ø§Ù„Ø¬Ø© PDF ÙƒØ¨ÙŠØ± (1000+ ØµÙØ­Ø©) / Process large PDF
large_pdf_results = parse(
    "large_curriculum.pdf",
    result_save_dir="results/large_processing"
)

# Ø§Ù„Ù…ÙƒØªØ¨Ø© ØªÙ‚Ø³Ù… Ø§Ù„Ù…Ù„Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙˆØªØ¹Ø§Ù„Ø¬Ù‡ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
# Library automatically splits and processes in parallel
```

### Ø­ÙØ¸ Visual Groundings

```python
from agentic_doc.parse import parse

# Ø­ÙØ¸ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© / Save visual groundings
results = parse(
    "document.pdf",
    grounding_save_dir="path/to/save/groundings"
)

# Ø§Ù„ØµÙˆØ± Ø³ØªØ­ÙØ¸ ÙÙŠ: / Images saved to:
# path/to/save/groundings/document_TIMESTAMP/page_X/CHUNK_TYPE_CHUNK_ID_Y.png

for chunk in results[0].chunks:
    for grounding in chunk.grounding:
        if grounding.image_path:
            print(f"Visual grounding: {grounding.image_path}")
```

### ØªØµÙˆØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ / Results Visualization

```python
from agentic_doc.parse import parse
from agentic_doc.utils import viz_parsed_document
from agentic_doc.config import VisualizationConfig

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ / Parse document
results = parse("document.pdf")
parsed_doc = results[0]

# Ø¥Ù†Ø´Ø§Ø¡ ØªØµÙˆØ± / Create visualization
images = viz_parsed_document(
    "document.pdf",
    parsed_doc,
    output_dir="visualizations"
)

# ØªØ®ØµÙŠØµ Ø§Ù„ØªØµÙˆØ± / Custom visualization
viz_config = VisualizationConfig(
    thickness=2,
    text_bg_opacity=0.8,
    font_scale=0.7,
    color_map={
        ChunkType.TITLE: (0, 0, 255),
        ChunkType.TEXT: (255, 0, 0),
    }
)

custom_images = viz_parsed_document(
    "document.pdf",
    parsed_doc,
    output_dir="custom_visualizations",
    viz_config=viz_config
)
```

## ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© / Advanced Configuration

### Ù…Ù„Ù .env Ù„Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª

```bash
# Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ / Parallel file processing
BATCH_SIZE=4

# Ø¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠÙˆØ· Ù„ÙƒÙ„ Ù…Ù„Ù / Threads per file
MAX_WORKERS=2

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© / Retry settings
MAX_RETRIES=80
MAX_RETRY_WAIT_TIME=30

# Ù†Ù…Ø· ØªØ³Ø¬ÙŠÙ„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© / Retry logging style
RETRY_LOGGING_STYLE=log_msg  # Options: log_msg, inline_block, none
```

### Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ / Error Handling Setup

```python
from agentic_doc.parse import parse

try:
    results = parse(
        "document.pdf",
        include_marginalia=False,  # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù‡ÙˆØ§Ù…Ø´
        include_metadata_in_markdown=False  # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ metadata
    )
    
    # ÙØ­Øµ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ / Check for errors
    for result in results:
        if hasattr(result, 'errors') and result.errors:
            for error in result.errors:
                print(f"Error on page {error.page}: {error.message}")
        
except Exception as e:
    print(f"Processing failed: {e}")
```

## ğŸ“Š Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© / Extracted Data Types

### JSON Response Structure

```python
{
    "chunks": [
        {
            "chunk_id": "unique_identifier",
            "chunk_type": "text|table|title|form_field|checkbox|image",
            "content": "extracted_content",
            "grounding": [
                {
                    "page": 1,
                    "coordinates": {
                        "x": 100,
                        "y": 150,
                        "width": 200,
                        "height": 50
                    },
                    "image_path": "optional_visual_grounding.png"
                }
            ],
            "metadata": {
                "confidence": 0.95,
                "language": "ar",
                "font_size": 12
            }
        }
    ],
    "markdown": "# Document Content\n\nExtracted text...",
    "extraction": {
        # Structured fields if extraction_model used
    }
}
```

### Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© / Supported Element Types

1. **TEXT**: Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
2. **TITLE**: Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø±Ø¤ÙˆØ³  
3. **TABLE**: Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…Ø©
4. **FORM_FIELD**: Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
5. **CHECKBOX**: ØµÙ†Ø§Ø¯ÙŠÙ‚ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±
6. **IMAGE**: Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù…Ø®Ø·Ø·Ø§Øª
7. **LIST**: Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ÙˆØ§Ù„Ù†Ù‚Ø§Ø·
8. **FOOTER**: ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø§Øª
9. **HEADER**: Ø±Ø£Ø³ Ø§Ù„ØµÙØ­Ø§Øª

## ğŸ”— Connectors Ù„Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© / Different Source Connectors

### Google Drive Connector

```python
from agentic_doc.parse import parse
from agentic_doc.connectors import GoogleDriveConnectorConfig

config = GoogleDriveConnectorConfig(
    client_secret_file="credentials.json",
    folder_id="google-drive-folder-id"
)

results = parse(config, connector_pattern="*.pdf")
```

### Amazon S3 Connector

```python
from agentic_doc.connectors import S3ConnectorConfig

config = S3ConnectorConfig(
    bucket_name="your-bucket",
    aws_access_key_id="your-key",
    aws_secret_access_key="your-secret",
    region_name="us-east-1"
)

results = parse(config, connector_path="documents/")
```

### Local Directory Connector

```python
from agentic_doc.connectors import LocalConnectorConfig

config = LocalConnectorConfig(recursive=True)
results = parse(
    config,
    connector_path="/path/to/documents",
    connector_pattern="*.pdf"
)
```

### URL Connector

```python
from agentic_doc.connectors import URLConnectorConfig

config = URLConnectorConfig(
    headers={"Authorization": "Bearer token"},
    timeout=60
)

results = parse(config, connector_path="https://example.com/doc.pdf")
```

## ğŸ’¾ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ© / Raw Bytes Processing

```python
from agentic_doc.parse import parse

# ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù ÙƒÙ€ bytes / Load file as bytes
with open("document.pdf", "rb") as f:
    raw_bytes = f.read()

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ© / Process raw bytes
results = parse(raw_bytes)

# Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ±Ø© Ù…Ù† bytes / Process image bytes
with open("image.png", "rb") as f:
    image_bytes = f.read()

results = parse(image_bytes)
```

## ğŸš¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø­Ø¯ÙˆØ¯ / Error Handling & Rate Limits

### Automatic Retry Mechanism

- **Retry Status Codes**: 408, 429, 502, 503, 504
- **Exponential Backoff**: Ø¨Ø¯Ø§ÙŠØ© Ù…Ù† Ø«Ø§Ù†ÙŠØ© ÙˆØ§Ø­Ø¯Ø©
- **Max Retry Time**: 60 Ø«Ø§Ù†ÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹
- **Jitter**: Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ØªØµÙ„ Ø¥Ù„Ù‰ 10 Ø«ÙˆØ§Ù†

### Rate Limits Best Practices

```python
# Ø¥Ø¹Ø¯Ø§Ø¯ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø­Ø¬Ù… / High-volume processing setup
import os

os.environ["BATCH_SIZE"] = "2"  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø²ÙŠ
os.environ["MAX_WORKERS"] = "3"  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø®ÙŠÙˆØ·
os.environ["MAX_RETRIES"] = "50"  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
```

## ğŸ“ˆ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ / Performance Comparison

Ø­Ø³Ø¨ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØŒ LandingAI Ø­ØµÙ„ Ø¹Ù„Ù‰:
- **Overall Score**: 69/100
- **Node Accuracy**: Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù‚Ø¯
- **Edge Accuracy**: Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
- **Table Extraction**: Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²

### Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰:
1. **LandingAI**: 69/100 - Ø§Ù„Ø£ÙØ¶Ù„ Ø´Ù…ÙˆÙ„ÙŠØ§Ù‹
2. **Mistral OCR**: Ù…ØªØ®ØµØµ ÙÙŠ RAG
3. **Claude Sonnet 3.7**: Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡Ø¬ÙŠÙ†
4. **OpenAI o3-mini**: ÙØ¹Ø§Ù„ Ù…Ù† Ø­ÙŠØ« Ø§Ù„ØªÙƒÙ„ÙØ©
5. **Docsumo**: ØªØ¯Ø±ÙŠØ¨ Ù…Ø®ØµØµ

## ğŸ’° Ø§Ù„ØªØ³Ø¹ÙŠØ± / Pricing

- **LandingAI**: ~$50 per 1M tokens
- **Enterprise Pricing**: Ù…ØªØ§Ø­ Ù„Ù„Ø´Ø±ÙƒØ§Øª
- **Pay-as-you-go**: Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

## ğŸ”§ ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ / Curriculum Comparison Implementation

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ / Curriculum Files Processing

```python
from agentic_doc.parse import parse
from pydantic import BaseModel, Field
import json
import os
from datetime import datetime

class CurriculumAnalysis(BaseModel):
    subject: str = Field(description="Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø£Ùˆ Ø§Ù„Ù…Ø§Ø¯Ø©")
    grade_level: str = Field(description="Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ")
    learning_objectives: list[str] = Field(description="Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©")
    topics: list[str] = Field(description="Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…ØºØ·Ø§Ø©")
    assessment_methods: list[str] = Field(description="Ø·Ø±Ù‚ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
    page_count: int = Field(description="Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª")
    difficulty_indicators: list[str] = Field(description="Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØµØ¹ÙˆØ¨Ø©")

def process_curriculum_folder(folder_path: str, output_dir: str):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª PDF ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯
    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]
    file_paths = [os.path.join(folder_path, f) for f in pdf_files]
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø¸Ù…
    results = parse(
        file_paths,
        extraction_model=CurriculumAnalysis,
        result_save_dir=f"{output_dir}/raw_extraction_{timestamp}",
        grounding_save_dir=f"{output_dir}/visual_groundings_{timestamp}"
    )
    
    processed_curricula = []
    
    for i, result in enumerate(results):
        curriculum_data = {
            "filename": pdf_files[i],
            "processed_at": timestamp,
            "markdown_content": result.markdown,
            "structured_analysis": result.extraction.dict() if result.extraction else None,
            "chunks": [
                {
                    "chunk_id": chunk.chunk_id,
                    "type": chunk.chunk_type,
                    "content": chunk.content,
                    "page": chunk.grounding[0].page if chunk.grounding else None,
                    "coordinates": chunk.grounding[0].coordinates if chunk.grounding else None
                }
                for chunk in result.chunks
            ],
            "visual_groundings": [
                grounding.image_path 
                for chunk in result.chunks 
                for grounding in chunk.grounding 
                if grounding.image_path
            ]
        }
        
        processed_curricula.append(curriculum_data)
        
        # Ø­ÙØ¸ Ù…Ù„Ù JSON Ù…Ù†ÙØµÙ„ Ù„ÙƒÙ„ Ù…Ù†Ù‡Ø¬
        individual_file = f"{output_dir}/curriculum_{i+1}_{timestamp}.json"
        with open(individual_file, 'w', encoding='utf-8') as f:
            json.dump(curriculum_data, f, ensure_ascii=False, indent=2)
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¬Ù…Ø¹
    combined_file = f"{output_dir}/all_curricula_{timestamp}.json"
    with open(combined_file, 'w', encoding='utf-8') as f:
        json.dump(processed_curricula, f, ensure_ascii=False, indent=2)
    
    return processed_curricula

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø©
old_curricula = process_curriculum_folder(
    "uploads/old_curriculum",
    "results/old_curriculum_analysis"
)

new_curricula = process_curriculum_folder(
    "uploads/new_curriculum", 
    "results/new_curriculum_analysis"
)
```

### ØªØ­Ù„ÙŠÙ„ Ù…Ù‚Ø§Ø±Ù† Ù„Ù„Ù…Ù†Ø§Ù‡Ø¬ / Comparative Curriculum Analysis

```python
def compare_curricula(old_analysis, new_analysis):
    """Ù…Ù‚Ø§Ø±Ù†Ø© ØªÙØµÙŠÙ„ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
    
    comparison_results = {
        "comparison_timestamp": datetime.now().isoformat(),
        "files_compared": {
            "old": len(old_analysis),
            "new": len(new_analysis)
        },
        "detailed_comparisons": []
    }
    
    for old_curr in old_analysis:
        for new_curr in new_analysis:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ·Ø§Ø¨Ù‚Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
            if old_curr.get("structured_analysis") and new_curr.get("structured_analysis"):
                old_subject = old_curr["structured_analysis"].get("subject", "")
                new_subject = new_curr["structured_analysis"].get("subject", "")
                
                # Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø³ÙŠØ·Ø© Ø¨Ø§Ù„Ù†Øµ
                if old_subject.lower() in new_subject.lower() or new_subject.lower() in old_subject.lower():
                    comparison = {
                        "old_file": old_curr["filename"],
                        "new_file": new_curr["filename"],
                        "subject_match": f"{old_subject} -> {new_subject}",
                        "changes": {
                            "learning_objectives": {
                                "old": old_curr["structured_analysis"].get("learning_objectives", []),
                                "new": new_curr["structured_analysis"].get("learning_objectives", []),
                                "added": [],
                                "removed": [],
                                "common": []
                            },
                            "topics": {
                                "old": old_curr["structured_analysis"].get("topics", []),
                                "new": new_curr["structured_analysis"].get("topics", []),
                                "added": [],
                                "removed": [],
                                "common": []
                            }
                        },
                        "content_similarity": len(set(old_curr["markdown_content"].split()) & 
                                                 set(new_curr["markdown_content"].split())) / 
                                               max(len(old_curr["markdown_content"].split()), 
                                                   len(new_curr["markdown_content"].split())),
                        "visual_elements": {
                            "old_groundings_count": len(old_curr.get("visual_groundings", [])),
                            "new_groundings_count": len(new_curr.get("visual_groundings", []))
                        }
                    }
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
                    old_objectives = set(old_curr["structured_analysis"].get("learning_objectives", []))
                    new_objectives = set(new_curr["structured_analysis"].get("learning_objectives", []))
                    
                    comparison["changes"]["learning_objectives"]["added"] = list(new_objectives - old_objectives)
                    comparison["changes"]["learning_objectives"]["removed"] = list(old_objectives - new_objectives)
                    comparison["changes"]["learning_objectives"]["common"] = list(old_objectives & new_objectives)
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
                    old_topics = set(old_curr["structured_analysis"].get("topics", []))
                    new_topics = set(new_curr["structured_analysis"].get("topics", []))
                    
                    comparison["changes"]["topics"]["added"] = list(new_topics - old_topics)
                    comparison["changes"]["topics"]["removed"] = list(old_topics - new_topics)
                    comparison["changes"]["topics"]["common"] = list(old_topics & new_topics)
                    
                    comparison_results["detailed_comparisons"].append(comparison)
    
    return comparison_results
```

## ğŸ“‹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ / Implementation Checklist

### âœ… Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

1. **ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø©**
   ```bash
   pip install agentic-doc
   ```

2. **Ø¥Ø¹Ø¯Ø§Ø¯ API Key**
   ```bash
   export VISION_AGENT_API_KEY=your_key_here
   ```

3. **Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„**
   ```python
   from agentic_doc.parse import parse
   result = parse("test_document.pdf")
   ```

4. **Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡**
5. **ØªØ®ØµÙŠØµ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©**
6. **ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†Ø¸Ù…**
7. **Ø­ÙØ¸ Visual Groundings**
8. **ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø°ÙƒÙŠØ©**

### ğŸ”§ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ·ÙˆÙŠØ±

1. **Backend Integration**
   - FastAPI endpoints Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
   - Celery tasks Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
   - Database models Ù„Ù„Ù†ØªØ§Ø¦Ø¬
   - WebSocket Ù„Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©

2. **Frontend Integration**
   - Progress tracking Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
   - Results visualization
   - File management interface
   - Comparison viewer

3. **Testing & Validation**
   - Unit tests Ù„Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
   - Integration tests Ù„Ù„Ù€ API
   - Performance benchmarks
   - Error handling validation

## ğŸ¯ Ø§Ù„Ø®Ù„Ø§ØµØ© / Summary

LandingAI Agentic Document Extraction ÙŠÙˆÙØ±:
- **Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©** ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
- **Ù…Ø±ÙˆÙ†Ø© ÙƒØ¨ÙŠØ±Ø©** ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
- **ØªÙƒØ§Ù…Ù„ Ø³Ù‡Ù„** Ù…Ø¹ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
- **Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©** Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
- **Ø¥Ø®Ø±Ø§Ø¬ Ù…Ù†Ø¸Ù…** Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ

Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø«Ø§Ù„ÙŠ Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ù‡:
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø¨Ø¯Ù‚Ø©
- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨ØµØ±ÙŠØ© ÙˆØ§Ù„Ø¬Ø¯Ø§ÙˆÙ„
- ØªÙˆÙÙŠØ± Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù„ÙƒÙ„ Ø¹Ù†ØµØ±
- Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØµÙŠØº Ù…ØªØ¹Ø¯Ø¯Ø© (JSON, Markdown)
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯Ø§Øª ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª

---

**Ù…Ù„Ø§Ø­Ø¸Ø©**: Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙŠØºØ·ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù„ØªØ·Ø¨ÙŠÙ‚ LandingAI ÙÙŠ Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ ÙŠÙÙ†ØµØ­ Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙƒØªØ¨Ø© Ù…Ø¹ Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©. 