#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ø¬Ù„Ø¯Ø§Øª - Fast Parallel Folder Processing System
Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯Ø§Øª ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©

Ø§Ù„Ù…ÙŠØ²Ø§Øª:
1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
2. ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… async/await ÙˆÙ…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
3. Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯
4. ØªØªØ¨Ø¹ Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
5. Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø§Ø±ÙŠØ± Ù…ÙØµÙ„Ø© ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
6. Ø¯Ø¹Ù… Ø£Ù†ÙˆØ§Ø¹ Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
7. Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
"""

import os
import asyncio
import aiofiles
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, AsyncGenerator
from datetime import datetime, timedelta
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass, asdict
from enum import Enum
import time
import psutil
import queue
import threading
from loguru import logger
import hashlib
import pickle
from contextlib import asynccontextmanager
import signal
import sys
from collections import defaultdict, deque
import statistics
import resource
import gc

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ logging Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
logger.remove()
logger.add(
    lambda msg: print(msg, end=""),
    format="<green>{time:HH:mm:ss}</green> | <level>{level}</level> | <cyan>{extra[component]}</cyan> | {message}",
    level="INFO"
)

class ProcessingStatus(Enum):
    """Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
    CANCELLED = "cancelled"

@dataclass
class FileTask:
    """Ù…Ù‡Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù"""
    file_path: Path
    task_id: str
    priority: int = 0
    estimated_time: float = 0.0
    status: ProcessingStatus = ProcessingStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    result_path: Optional[Path] = None
    metadata: Dict[str, Any] = None

@dataclass
class ProcessingStats:
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    total_files: int = 0
    processed_files: int = 0
    failed_files: int = 0
    skipped_files: int = 0
    total_time: float = 0.0
    average_time_per_file: float = 0.0
    files_per_second: float = 0.0
    total_size_reduction: float = 0.0
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0

class ProgressReporter:
    """Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""
    
    def __init__(self, update_interval: float = 1.0):
        self.update_interval = update_interval
        self.stats = ProcessingStats()
        self.start_time = datetime.now()
        self.last_update = time.time()
        self.completed_files = deque(maxlen=100)  # Ø¢Ø®Ø± 100 Ù…Ù„Ù
        self.is_running = False
        
    def start(self):
        """Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙ‚Ø¯Ù…"""
        self.is_running = True
        self.start_time = datetime.now()
        
    def stop(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙ‚Ø¯Ù…"""
        self.is_running = False
        
    def update_file_completed(self, task: FileTask, processing_time: float):
        """ØªØ­Ø¯ÙŠØ« Ø¹Ù†Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ù…Ù„Ù"""
        self.stats.processed_files += 1
        self.completed_files.append(processing_time)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        if self.completed_files:
            self.stats.average_time_per_file = sum(self.completed_files) / len(self.completed_files)
        
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        self.stats.total_time = elapsed_time
        
        if elapsed_time > 0:
            self.stats.files_per_second = self.stats.processed_files / elapsed_time
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬
        process = psutil.Process()
        self.stats.memory_usage_mb = process.memory_info().rss / 1024 / 1024
        self.stats.cpu_usage_percent = process.cpu_percent()
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
        if time.time() - self.last_update >= self.update_interval:
            self._print_progress()
            self.last_update = time.time()
    
    def update_file_failed(self, task: FileTask, error: str):
        """ØªØ­Ø¯ÙŠØ« Ø¹Ù†Ø¯ ÙØ´Ù„ Ù…Ù„Ù"""
        self.stats.failed_files += 1
        
    def _print_progress(self):
        """Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚Ø¯Ù…"""
        completion_rate = (self.stats.processed_files / self.stats.total_files * 100) if self.stats.total_files > 0 else 0
        
        eta_seconds = 0
        if self.stats.files_per_second > 0 and self.stats.total_files > 0:
            remaining_files = self.stats.total_files - self.stats.processed_files - self.stats.failed_files
            eta_seconds = remaining_files / self.stats.files_per_second
        
        eta_str = str(timedelta(seconds=int(eta_seconds))) if eta_seconds > 0 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        logger.info("", 
                   component="PROGRESS",
                   extra={
                       "component": "PROGRESS",
                       "progress": f"{completion_rate:.1f}% ({self.stats.processed_files}/{self.stats.total_files})",
                       "speed": f"{self.stats.files_per_second:.2f} Ù…Ù„Ù/Ø«Ø§Ù†ÙŠØ©",
                       "eta": eta_str,
                       "memory": f"{self.stats.memory_usage_mb:.1f}MB",
                       "cpu": f"{self.stats.cpu_usage_percent:.1f}%"
                   })

class ResourceManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©"""
    
    def __init__(self, max_memory_mb: int = 4096, max_cpu_percent: float = 80.0):
        self.max_memory_mb = max_memory_mb
        self.max_cpu_percent = max_cpu_percent
        self.monitoring = False
        
    async def monitor_resources(self) -> bool:
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        process = psutil.Process()
        
        # ÙØ­Øµ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory_mb = process.memory_info().rss / 1024 / 1024
        if memory_mb > self.max_memory_mb:
            logger.warning("", component="RESOURCE", extra={"component": "RESOURCE"})
            logger.warning(f"âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø¹Ø§Ù„ÙŠ: {memory_mb:.1f}MB")
            gc.collect()  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            await asyncio.sleep(0.1)
            return False
        
        # ÙØ­Øµ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        cpu_percent = process.cpu_percent()
        if cpu_percent > self.max_cpu_percent:
            logger.warning("", component="RESOURCE", extra={"component": "RESOURCE"})
            logger.warning(f"âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬ Ø¹Ø§Ù„ÙŠ: {cpu_percent:.1f}%")
            await asyncio.sleep(0.2)
            return False
        
        return True

class FastFolderProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹"""
    
    def __init__(self, 
                 max_workers: int = None,
                 chunk_size: int = 100,
                 max_memory_mb: int = 4096,
                 progress_interval: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        
        Args:
            max_workers: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
            chunk_size: Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©
            max_memory_mb: Ø£Ù‚ØµÙ‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„Ø°Ø§ÙƒØ±Ø©
            progress_interval: ÙØªØ±Ø© ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
        """
        self.max_workers = max_workers or min(16, mp.cpu_count() * 2)
        self.chunk_size = chunk_size
        self.resource_manager = ResourceManager(max_memory_mb)
        self.progress_reporter = ProgressReporter(progress_interval)
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ù‡Ø§Ù…
        self.task_queue: asyncio.Queue = None
        self.result_queue: asyncio.Queue = None
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆÙ…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø©
        self.is_running = False
        self.workers: List[asyncio.Task] = []
        self.processed_tasks: List[FileTask] = []
        
        # Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        self.error_log: List[Dict[str, Any]] = []
        self.retry_queue: List[FileTask] = []
        
        logger.info("", component="INIT", extra={"component": "INIT"})
        logger.info(f"ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹ Ù…Ø¹ {self.max_workers} Ø¹Ø§Ù…Ù„")

    async def process_folder_structure(self, 
                                     root_path: Union[str, Path], 
                                     output_path: Union[str, Path] = None,
                                     file_patterns: List[str] = None,
                                     recursive: bool = True,
                                     preserve_structure: bool = True) -> Dict[str, Any]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‡ÙŠÙƒÙ„ Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„
        
        Args:
            root_path: Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ø°Ø±
            output_path: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            file_patterns: Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§
            recursive: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©
            preserve_structure: Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
            
        Returns:
            Dict: ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        root_path = Path(root_path)
        output_path = Path(output_path) if output_path else root_path / "processed_fast"
        file_patterns = file_patterns or ["*.md", "*.txt", "*.json"]
        
        logger.info("", component="MAIN", extra={"component": "MAIN"})
        logger.info(f"ğŸ“ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯: {root_path}")
        logger.info(f"ğŸ“¤ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {output_path}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        output_path.mkdir(parents=True, exist_ok=True)
        
        start_time = datetime.now()
        
        try:
            # 1. Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù„ÙØ§Øª
            files_to_process = await self._discover_files(root_path, file_patterns, recursive)
            total_files = len(files_to_process)
            
            if total_files == 0:
                logger.warning("", component="MAIN", extra={"component": "MAIN"})
                logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
                return self._create_empty_report()
            
            logger.info("", component="MAIN", extra={"component": "MAIN"})
            logger.info(f"ğŸ“Š ØªÙ… Ø§ÙƒØªØ´Ø§Ù {total_files} Ù…Ù„Ù Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
            
            # 2. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
            tasks = await self._create_file_tasks(files_to_process, root_path, output_path, preserve_structure)
            
            # 3. ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            self.task_queue = asyncio.Queue()
            self.result_queue = asyncio.Queue()
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‡Ø§Ù… Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
            for task in tasks:
                await self.task_queue.put(task)
            
            # 4. Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            self.progress_reporter.stats.total_files = total_files
            self.progress_reporter.start()
            
            results = await self._process_tasks_parallel()
            
            # 5. Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()
            
            report = await self._generate_final_report(
                root_path, output_path, results, total_time, total_files
            )
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            await self._save_report(output_path, report)
            
            logger.success("", component="MAIN", extra={"component": "MAIN"})
            logger.success(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ {total_time:.2f}s")
            logger.success(f"ğŸ“Š Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {total_files / total_time:.2f} Ù…Ù„Ù/Ø«Ø§Ù†ÙŠØ©")
            
            return report
            
        except Exception as e:
            logger.error("", component="ERROR", extra={"component": "ERROR"})
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯: {e}")
            raise
        finally:
            self.progress_reporter.stop()

    async def _discover_files(self, root_path: Path, patterns: List[str], recursive: bool) -> List[Path]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§"""
        logger.info("", component="DISCOVER", extra={"component": "DISCOVER"})
        logger.info("ğŸ” Ø¨Ø¯Ø¡ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        files = []
        
        for pattern in patterns:
            if recursive:
                found_files = list(root_path.rglob(pattern))
            else:
                found_files = list(root_path.glob(pattern))
            files.extend(found_files)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù… (Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù„Ù„ØªØ³Ø±ÙŠØ¹)
        unique_files = list(set(files))
        unique_files.sort(key=lambda f: f.stat().st_size)
        
        logger.info("", component="DISCOVER", extra={"component": "DISCOVER"})
        logger.info(f"âœ… ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(unique_files)} Ù…Ù„Ù")
        
        return unique_files

    async def _create_file_tasks(self, files: List[Path], root_path: Path, 
                               output_path: Path, preserve_structure: bool) -> List[FileTask]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        logger.info("", component="TASKS", extra={"component": "TASKS"})
        logger.info("ğŸ“‹ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
        
        tasks = []
        
        for i, file_path in enumerate(files):
            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            if preserve_structure:
                relative_path = file_path.relative_to(root_path)
                output_file_path = output_path / relative_path.parent / f"{relative_path.stem}_processed.md"
            else:
                output_file_path = output_path / f"{file_path.stem}_processed.md"
            
            # ØªÙ‚Ø¯ÙŠØ± ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
            file_size = file_path.stat().st_size
            estimated_time = min(file_size / 50000, 10.0)  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ
            
            task = FileTask(
                file_path=file_path,
                task_id=f"task_{i:06d}",
                priority=0,  # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
                estimated_time=estimated_time,
                result_path=output_file_path,
                metadata={'file_size': file_size, 'index': i}
            )
            
            tasks.append(task)
        
        logger.info("", component="TASKS", extra={"component": "TASKS"})
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(tasks)} Ù…Ù‡Ù…Ø©")
        
        return tasks

    async def _process_tasks_parallel(self) -> List[FileTask]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ"""
        logger.info("", component="WORKER", extra={"component": "WORKER"})
        logger.info(f"âš¡ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…Ø¹ {self.max_workers} Ø¹Ø§Ù…Ù„")
        
        self.is_running = True
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù…Ø§Ù„
        self.workers = []
        for i in range(self.max_workers):
            worker = asyncio.create_task(self._worker(f"worker_{i}"))
            self.workers.append(worker)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§ÙƒØªÙ…Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù…
        await self.task_queue.join()
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ø§Ù„
        self.is_running = False
        for worker in self.workers:
            worker.cancel()
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results = []
        while not self.result_queue.empty():
            try:
                result = self.result_queue.get_nowait()
                results.append(result)
            except asyncio.QueueEmpty:
                break
        
        return results

    async def _worker(self, worker_name: str):
        """Ø¹Ø§Ù…Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ø§Ù…"""
        logger.debug("", component=worker_name.upper(), extra={"component": worker_name.upper()})
        logger.debug(f"ğŸ”§ Ø¨Ø¯Ø¡ Ø§Ù„Ø¹Ø§Ù…Ù„: {worker_name}")
        
        processed_count = 0
        
        while self.is_running:
            try:
                # ÙØ­Øµ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
                if not await self.resource_manager.monitor_resources():
                    await asyncio.sleep(0.5)
                    continue
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‡Ù…Ø©
                try:
                    task = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
                task.status = ProcessingStatus.PROCESSING
                task.start_time = datetime.now()
                
                try:
                    await self._process_single_task(task)
                    task.status = ProcessingStatus.COMPLETED
                    
                    processing_time = (datetime.now() - task.start_time).total_seconds()
                    self.progress_reporter.update_file_completed(task, processing_time)
                    
                    processed_count += 1
                    
                except Exception as e:
                    task.status = ProcessingStatus.FAILED
                    task.error_message = str(e)
                    self.progress_reporter.update_file_failed(task, str(e))
                    
                    logger.error("", component=worker_name.upper(), extra={"component": worker_name.upper()})
                    logger.error(f"âŒ ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© {task.file_path.name}: {e}")
                
                finally:
                    task.end_time = datetime.now()
                    await self.result_queue.put(task)
                    self.task_queue.task_done()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("", component=worker_name.upper(), extra={"component": worker_name.upper()})
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ø§Ù…Ù„ {worker_name}: {e}")
        
        logger.debug("", component=worker_name.upper(), extra={"component": worker_name.upper()})
        logger.debug(f"ğŸ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø¹Ø§Ù…Ù„ {worker_name} (Ù…Ø¹Ø§Ù„Ø¬: {processed_count} Ù…Ù„Ù)")

    async def _process_single_task(self, task: FileTask):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø©"""
        from enhanced_text_processing_system import AdvancedTextProcessor
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬ Ù†Øµ
        processor = AdvancedTextProcessor(num_workers=1)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù
        result = await processor.process_file_async(task.file_path)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        task.result_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        async with aiofiles.open(task.result_path, 'w', encoding='utf-8') as f:
            await f.write(result.cleaned_text)
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        task.metadata.update({
            'original_length': len(result.original_text),
            'cleaned_length': len(result.cleaned_text),
            'reduction_percentage': result.metadata.get('reduction_percentage', 0),
            'content_type': result.content_type.value
        })

    async def _generate_final_report(self, root_path: Path, output_path: Path, 
                                   results: List[FileTask], total_time: float, 
                                   total_files: int) -> Dict[str, Any]:
        """Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        logger.info("", component="REPORT", extra={"component": "REPORT"})
        logger.info("ğŸ“Š Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        completed_tasks = [t for t in results if t.status == ProcessingStatus.COMPLETED]
        failed_tasks = [t for t in results if t.status == ProcessingStatus.FAILED]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        stats = {
            'total_files': total_files,
            'completed_files': len(completed_tasks),
            'failed_files': len(failed_tasks),
            'success_rate': (len(completed_tasks) / total_files * 100) if total_files > 0 else 0,
            'total_processing_time': total_time,
            'average_time_per_file': total_time / total_files if total_files > 0 else 0,
            'files_per_second': total_files / total_time if total_time > 0 else 0
        }
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
        if completed_tasks:
            reductions = [t.metadata.get('reduction_percentage', 0) for t in completed_tasks if t.metadata]
            original_sizes = [t.metadata.get('original_length', 0) for t in completed_tasks if t.metadata]
            cleaned_sizes = [t.metadata.get('cleaned_length', 0) for t in completed_tasks if t.metadata]
            
            optimization_stats = {
                'average_reduction_percentage': statistics.mean(reductions) if reductions else 0,
                'median_reduction_percentage': statistics.median(reductions) if reductions else 0,
                'total_original_size': sum(original_sizes),
                'total_cleaned_size': sum(cleaned_sizes),
                'total_size_saved': sum(original_sizes) - sum(cleaned_sizes)
            }
        else:
            optimization_stats = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        error_analysis = {}
        if failed_tasks:
            error_types = defaultdict(int)
            for task in failed_tasks:
                error_types[task.error_message or 'Unknown'] += 1
            error_analysis = dict(error_types)
        
        report = {
            'processing_summary': stats,
            'optimization_summary': optimization_stats,
            'error_analysis': error_analysis,
            'configuration': {
                'root_path': str(root_path),
                'output_path': str(output_path),
                'max_workers': self.max_workers,
                'chunk_size': self.chunk_size
            },
            'performance_metrics': {
                'peak_memory_mb': self.progress_reporter.stats.memory_usage_mb,
                'average_cpu_usage': self.progress_reporter.stats.cpu_usage_percent,
                'throughput_efficiency': (stats['files_per_second'] * stats['success_rate']) / 100
            },
            'file_details': [
                {
                    'file_path': str(task.file_path),
                    'status': task.status.value,
                    'processing_time': (task.end_time - task.start_time).total_seconds() if task.start_time and task.end_time else 0,
                    'metadata': task.metadata or {}
                } for task in results
            ]
        }
        
        return report

    async def _save_report(self, output_path: Path, report: Dict[str, Any]):
        """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        report_file = output_path / f"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report, ensure_ascii=False, indent=2, default=str))
        
        logger.info("", component="REPORT", extra={"component": "REPORT"})
        logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_file}")

    def _create_empty_report(self) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ÙØ§Ø±Øº"""
        return {
            'processing_summary': {
                'total_files': 0,
                'completed_files': 0,
                'failed_files': 0,
                'success_rate': 0,
                'total_processing_time': 0,
                'average_time_per_file': 0,
                'files_per_second': 0
            },
            'optimization_summary': {},
            'error_analysis': {},
            'configuration': {},
            'performance_metrics': {},
            'file_details': []
        }

# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø³Ø±ÙŠØ¹Ø©

async def quick_process_folder(folder_path: str, output_path: str = None, 
                             max_workers: int = None) -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù…Ø¬Ù„Ø¯"""
    processor = FastFolderProcessor(max_workers=max_workers)
    return await processor.process_folder_structure(folder_path, output_path)

async def benchmark_processing_speed(test_folder: str, worker_counts: List[int] = None) -> Dict[str, Any]:
    """Ù‚ÙŠØ§Ø³ Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ Ø¹Ø¯Ø¯ Ù…Ø®ØªÙ„Ù Ù…Ù† Ø§Ù„Ø¹Ù…Ø§Ù„"""
    worker_counts = worker_counts or [1, 2, 4, 8, 16]
    results = {}
    
    logger.info("", component="BENCHMARK", extra={"component": "BENCHMARK"})
    logger.info("ğŸ Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡...")
    
    for worker_count in worker_counts:
        logger.info("", component="BENCHMARK", extra={"component": "BENCHMARK"})
        logger.info(f"âš¡ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ {worker_count} Ø¹Ø§Ù…Ù„...")
        
        start_time = time.time()
        processor = FastFolderProcessor(max_workers=worker_count)
        
        result = await processor.process_folder_structure(
            test_folder, 
            output_path=f"{test_folder}_benchmark_{worker_count}workers"
        )
        
        end_time = time.time()
        
        results[f"{worker_count}_workers"] = {
            'total_time': end_time - start_time,
            'files_per_second': result['processing_summary']['files_per_second'],
            'success_rate': result['processing_summary']['success_rate'],
            'memory_usage': result['performance_metrics']['peak_memory_mb']
        }
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    best_performance = max(results.values(), key=lambda x: x['files_per_second'])
    
    benchmark_report = {
        'test_folder': test_folder,
        'worker_performance': results,
        'best_configuration': best_performance,
        'recommendations': _generate_performance_recommendations(results)
    }
    
    return benchmark_report

def _generate_performance_recommendations(results: Dict[str, Any]) -> Dict[str, str]:
    """Ø¥Ù†ØªØ§Ø¬ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    recommendations = {}
    
    # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¥Ø¹Ø¯Ø§Ø¯
    best_speed = max(results.values(), key=lambda x: x['files_per_second'])
    best_config = [k for k, v in results.items() if v == best_speed][0]
    
    recommendations['optimal_workers'] = f"Ø£ÙØ¶Ù„ Ø¹Ø¯Ø¯ Ø¹Ù…Ø§Ù„: {best_config}"
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    high_memory_configs = [k for k, v in results.items() if v['memory_usage'] > 2048]
    if high_memory_configs:
        recommendations['memory_warning'] = f"Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªØ³ØªØ®Ø¯Ù… Ø°Ø§ÙƒØ±Ø© Ø¹Ø§Ù„ÙŠØ©: {', '.join(high_memory_configs)}"
    
    return recommendations

# Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
class RealTimeMonitor:
    """Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""
    
    def __init__(self, update_interval: float = 2.0):
        self.update_interval = update_interval
        self.is_monitoring = False
        self.stats_history = deque(maxlen=100)
        
    async def start_monitoring(self, processor: FastFolderProcessor):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        self.is_monitoring = True
        
        while self.is_monitoring:
            stats = {
                'timestamp': datetime.now(),
                'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'cpu_percent': psutil.cpu_percent(),
                'processed_files': processor.progress_reporter.stats.processed_files,
                'files_per_second': processor.progress_reporter.stats.files_per_second
            }
            
            self.stats_history.append(stats)
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            logger.info("", component="MONITOR", extra={"component": "MONITOR"})
            logger.info(f"ğŸ“Š Ø°Ø§ÙƒØ±Ø©: {stats['memory_mb']:.1f}MB | Ù…Ø¹Ø§Ù„Ø¬: {stats['cpu_percent']:.1f}% | Ø³Ø±Ø¹Ø©: {stats['files_per_second']:.2f} Ù…Ù„Ù/Ø«Ø§Ù†ÙŠØ©")
            
            await asyncio.sleep(self.update_interval)
    
    def stop_monitoring(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        self.is_monitoring = False
        
    def get_stats_summary(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        if not self.stats_history:
            return {}
        
        memory_values = [s['memory_mb'] for s in self.stats_history]
        cpu_values = [s['cpu_percent'] for s in self.stats_history]
        speed_values = [s['files_per_second'] for s in self.stats_history if s['files_per_second'] > 0]
        
        return {
            'average_memory_mb': statistics.mean(memory_values),
            'peak_memory_mb': max(memory_values),
            'average_cpu_percent': statistics.mean(cpu_values),
            'peak_cpu_percent': max(cpu_values),
            'average_speed': statistics.mean(speed_values) if speed_values else 0,
            'peak_speed': max(speed_values) if speed_values else 0
        }

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
async def test_fast_folder_processor():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹"""
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹")
    print("=" * 60)
    
    test_folder = "d:/ezz/compair/edu-compare-wizard/backend/uploads/landingai_results"
    
    if Path(test_folder).exists():
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
        print("ğŸ“ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯...")
        result = await quick_process_folder(test_folder, max_workers=8)
        
        print(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {result['processing_summary']['completed_files']} Ù…Ù„Ù")
        print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {result['processing_summary']['total_processing_time']:.2f}s")
        print(f"ğŸš€ Ø§Ù„Ø³Ø±Ø¹Ø©: {result['processing_summary']['files_per_second']:.2f} Ù…Ù„Ù/Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ“Š Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {result['processing_summary']['success_rate']:.1f}%")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡
        print(f"\nğŸ Ø§Ø®ØªØ¨Ø§Ø± Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡...")
        benchmark_result = await benchmark_processing_speed(test_folder, [2, 4, 8])
        print(f"ğŸ† Ø£ÙØ¶Ù„ Ø¥Ø¹Ø¯Ø§Ø¯: {benchmark_result['recommendations']['optimal_workers']}")
    
    else:
        print(f"âŒ Ø§Ù„Ù…Ø¬Ù„Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {test_folder}")

if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_fast_folder_processor())
